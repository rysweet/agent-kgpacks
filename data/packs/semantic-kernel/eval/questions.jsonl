{"id": "sk_001", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the primary role of the Kernel object in Semantic Kernel?", "ground_truth": "The Kernel object serves as a dependency injection container that manages AI services, plugins, and their interactions within the Semantic Kernel framework.", "source": "kernel_object"}
{"id": "sk_002", "domain": "semantic_kernel", "difficulty": "easy", "question": "Name three programming languages supported by Microsoft Semantic Kernel.", "ground_truth": "Semantic Kernel supports C#, Python, and Java as its primary programming languages.", "source": "introduction"}
{"id": "sk_003", "domain": "semantic_kernel", "difficulty": "easy", "question": "What are the two main types of plugins in Semantic Kernel?", "ground_truth": "The two main types of plugins are native code plugins (written in the host language) and API-based plugins (OpenAPI and MCP server).", "source": "plugins"}
{"id": "sk_004", "domain": "semantic_kernel", "difficulty": "easy", "question": "Which Azure service is commonly integrated with Semantic Kernel for language models?", "ground_truth": "Azure OpenAI is the commonly integrated Azure service that provides language models and chat completion capabilities for Semantic Kernel.", "source": "azure_openai_integration"}
{"id": "sk_005", "domain": "semantic_kernel", "difficulty": "easy", "question": "What has replaced the deprecated Stepwise and Handlebars planners in Semantic Kernel?", "ground_truth": "Auto function calling has replaced the deprecated Stepwise and Handlebars planners, providing automatic function invocation based on AI decisions.", "source": "planners"}
{"id": "sk_006", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is a prompt template in Semantic Kernel?", "ground_truth": "A prompt template is a parameterized text structure that allows dynamic insertion of variables and content to create flexible prompts for AI services.", "source": "prompt_templates"}
{"id": "sk_007", "domain": "semantic_kernel", "difficulty": "easy", "question": "Name two vector store connectors mentioned in the Semantic Kernel documentation.", "ground_truth": "Azure AI Search and Chroma are two vector store connectors supported by Semantic Kernel for semantic search and embeddings management.", "source": "vector_store_connectors"}
{"id": "sk_008", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the purpose of the ChatCompletionAgent in Semantic Kernel?", "ground_truth": "The ChatCompletionAgent is an agent implementation that uses chat completion models to interact with plugins and handle conversational AI orchestration.", "source": "chat_completion_agent"}
{"id": "sk_009", "domain": "semantic_kernel", "difficulty": "easy", "question": "What does function calling enable in Semantic Kernel?", "ground_truth": "Function calling enables AI models to automatically invoke semantic and native functions based on user requests and context, bridging AI decisions with executable code.", "source": "function_calling"}
{"id": "sk_010", "domain": "semantic_kernel", "difficulty": "easy", "question": "What are the two runtime environments supported by the Semantic Kernel Process Framework?", "ground_truth": "The Process Framework supports local runtime (for development and testing) and cloud runtime (for scalable production deployments).", "source": "process_framework"}
{"id": "sk_011", "domain": "semantic_kernel", "difficulty": "easy", "question": "Which AI service capability is used for generating text responses in Semantic Kernel?", "ground_truth": "Text generation and chat completion services are the primary AI service capabilities used for generating text responses in Semantic Kernel.", "source": "ai_services"}
{"id": "sk_012", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is Pinecone in the context of Semantic Kernel?", "ground_truth": "Pinecone is a vector database service that Semantic Kernel can connect to via its vector store connectors for semantic search and similarity-based retrieval.", "source": "vector_store_connectors"}
{"id": "sk_013", "domain": "semantic_kernel", "difficulty": "easy", "question": "How many orchestration patterns are mentioned for agents in Semantic Kernel?", "ground_truth": "Semantic Kernel supports four main agent orchestration patterns: sequential, concurrent, handoff, and group chat.", "source": "agent_orchestration"}
{"id": "sk_014", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the role of middleware in enterprise-ready Semantic Kernel applications?", "ground_truth": "Middleware provides cross-cutting functionality for request/response processing, enabling features like logging, filtering, and telemetry in enterprise applications.", "source": "enterprise_readiness"}
{"id": "sk_015", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is text search functionality in Semantic Kernel used for?", "ground_truth": "Text search functionality in Semantic Kernel enables searching and retrieving relevant documents or content based on semantic similarity and keyword matching.", "source": "text_search"}
{"id": "sk_016", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the primary purpose of telemetry in Semantic Kernel?", "ground_truth": "Telemetry enables monitoring, logging, and observability of Semantic Kernel operations, helping track performance and troubleshoot issues in production environments.", "source": "enterprise_readiness"}
{"id": "sk_017", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is an MCP server plugin in Semantic Kernel?", "ground_truth": "An MCP server plugin is a plugin that connects to external services through the Model Context Protocol, extending Semantic Kernel's capabilities beyond native code.", "source": "plugins"}
{"id": "sk_018", "domain": "semantic_kernel", "difficulty": "easy", "question": "What are filters used for in enterprise Semantic Kernel deployments?", "ground_truth": "Filters in Semantic Kernel are used to enforce security policies, validate inputs/outputs, and control access to plugins and AI services in enterprise environments.", "source": "enterprise_readiness"}
{"id": "sk_019", "domain": "semantic_kernel", "difficulty": "easy", "question": "What does the Agent Framework in Semantic Kernel provide?", "ground_truth": "The Agent Framework provides abstractions and patterns for building intelligent agents that can reason, plan, and execute tasks using plugins and AI services.", "source": "agent_framework"}
{"id": "sk_020", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the primary benefit of using dependency injection with the Kernel object?", "ground_truth": "Dependency injection via the Kernel object enables loose coupling between components, improves testability, and simplifies configuration management of AI services and plugins.", "source": "kernel_object"}
{"id": "sk_021", "domain": "semantic_kernel", "difficulty": "medium", "question": "What is the primary purpose of the Kernel object in Semantic Kernel, and how does it function within the framework?", "ground_truth": "The Kernel object serves as a dependency injection container that orchestrates AI services, plugins, and functions. It manages the lifecycle and configuration of all Semantic Kernel components, enabling seamless integration between native code plugins, AI services, and orchestration logic.", "source": "kernel_object"}
{"id": "sk_022", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do native code plugins differ from OpenAPI plugins in Semantic Kernel, and when would you use each?", "ground_truth": "Native code plugins are written directly in C#, Python, or Java and tightly integrated with the Kernel, offering better performance and type safety. OpenAPI plugins allow integration with external APIs through OpenAPI specifications. Use native plugins for custom logic and performance-critical operations; use OpenAPI plugins for external service integration.", "source": "plugins"}
{"id": "sk_023", "domain": "semantic_kernel", "difficulty": "medium", "question": "What replaced the deprecated Stepwise and Handlebars planners in Semantic Kernel, and what is the advantage of this approach?", "ground_truth": "Auto function calling replaced deprecated planners by leveraging the native function-calling capabilities of modern LLMs like GPT-4. This approach is simpler, more reliable, and reduces overhead by delegating planning decisions directly to the AI model rather than using separate planning logic.", "source": "planners"}
{"id": "sk_024", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe the relationship between prompts, prompt templates, and function calling in Semantic Kernel.", "ground_truth": "Prompt templates define reusable prompts with variable placeholders that are rendered at runtime. Functions are created from prompts and can be invoked with specific parameters. Function calling allows the AI service to automatically invoke these functions based on the prompt context and model capabilities, creating a dynamic execution flow.", "source": "prompts_and_functions"}
{"id": "sk_025", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are the key differences between chat completion and text generation AI services in Semantic Kernel?", "ground_truth": "Chat completion services maintain conversation history and handle multi-turn dialogue, returning structured responses with role information (user, assistant, system). Text generation services produce open-ended text output without conversation context. Chat completion is ideal for interactive agents; text generation is better for content creation and analysis tasks.", "source": "ai_services"}
{"id": "sk_026", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain how vector store connectors work in Semantic Kernel and name three supported vector databases.", "ground_truth": "Vector store connectors abstract the interaction with vector databases for semantic search and retrieval. They convert text to embeddings and perform similarity searches. Three supported connectors are Azure AI Search, Chroma, and Pinecone, enabling integration with various vector storage solutions for RAG applications.", "source": "vector_store_connectors"}
{"id": "sk_027", "domain": "semantic_kernel", "difficulty": "medium", "question": "How does the Semantic Kernel text search feature differ from traditional keyword-based search?", "ground_truth": "Semantic text search uses embeddings and vector similarity to find contextually relevant results rather than matching exact keywords. It understands meaning and intent, allowing it to find documents related to a query's semantic meaning even if they don't contain the exact search terms. This provides more intelligent and contextually aware search results.", "source": "text_search"}
{"id": "sk_028", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are the main agent orchestration patterns available in Semantic Kernel, and what is a practical use case for each?", "ground_truth": "Sequential orchestration executes agents one after another (suitable for step-by-step processes), concurrent orchestration runs agents in parallel (useful for independent tasks), handoff orchestration transfers control between agents (good for specialized role-based workflows), and group chat enables multi-agent discussion (ideal for collaborative problem-solving).", "source": "agent_orchestration"}
{"id": "sk_029", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe the role of ChatCompletionAgent in Semantic Kernel and how it differs from other agent types.", "ground_truth": "ChatCompletionAgent is designed for conversational AI scenarios, maintaining chat history and leveraging chat completion models for intelligent responses. It handles multi-turn interactions and integrates naturally with prompt templates and function calling. Unlike other agents, it prioritizes conversation coherence and context preservation across multiple exchanges.", "source": "chat_completion_agent"}
{"id": "sk_030", "domain": "semantic_kernel", "difficulty": "medium", "question": "What is the Process Framework in Semantic Kernel, and how does it support both local and cloud runtime execution?", "ground_truth": "The Process Framework provides a structured way to define and execute complex multi-step workflows with state management and error handling. It supports local runtime execution for development and testing, while also enabling cloud deployment for production scenarios. It abstracts the execution environment, allowing the same process definition to run locally or in cloud services.", "source": "process_framework"}
{"id": "sk_031", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do filters and middleware in Semantic Kernel contribute to enterprise readiness?", "ground_truth": "Filters and middleware enable cross-cutting concerns like security, logging, and request validation without modifying core logic. They intercept kernel operations, allowing organizations to implement authentication, audit trails, rate limiting, and data sanitization consistently across all plugins and functions, ensuring compliance and security standards.", "source": "enterprise_readiness"}
{"id": "sk_032", "domain": "semantic_kernel", "difficulty": "medium", "question": "What role does telemetry play in Semantic Kernel's enterprise capabilities, and what can it monitor?", "ground_truth": "Telemetry provides observability into Semantic Kernel operations for monitoring, debugging, and optimization in production environments. It can track function execution times, token usage, errors, API calls, and cost metrics. This enables organizations to identify bottlenecks, manage AI service costs, and ensure system reliability.", "source": "telemetry"}
{"id": "sk_033", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain how Semantic Kernel integrates with Azure OpenAI services and what configuration is required.", "ground_truth": "Semantic Kernel integrates with Azure OpenAI through the Kernel's AI service registration, requiring endpoint URL, API key, and deployment name configuration. The integration leverages Azure OpenAI's chat completion and embedding models. This allows applications to use enterprise-grade AI services with security, compliance, and regional deployment benefits.", "source": "azure_openai_integration"}
{"id": "sk_034", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do MCP server plugins extend Semantic Kernel's capabilities, and what is a typical integration scenario?", "ground_truth": "MCP (Model Context Protocol) server plugins enable Semantic Kernel to communicate with external services and tools following the MCP specification. A typical scenario is integrating specialized tools or services (e.g., weather APIs, database queries, external APIs) as plugins without modifying the Kernel's core code. This allows dynamic capability extension.", "source": "mcp_server_plugins"}
{"id": "sk_035", "domain": "semantic_kernel", "difficulty": "medium", "question": "What considerations should be made when designing prompt templates for function calling in Semantic Kernel?", "ground_truth": "Effective prompt templates should clearly describe function parameters, expected outputs, and usage examples. They should include contextual constraints and edge cases to guide the AI model's function selection. Templates should be tested for clarity, ambiguity reduction, and ensuring the model reliably invokes the intended functions with appropriate parameters.", "source": "prompt_engineering"}
{"id": "sk_036", "domain": "semantic_kernel", "difficulty": "medium", "question": "How would you configure a Semantic Kernel application to support multi-turn conversations with state preservation across requests?", "ground_truth": "Use ChatCompletionAgent with persistent chat history storage (in-memory, database, or cache). Configure the Kernel with chat completion service and populate the agent's chat history before each invocation. Implement state management to store and retrieve conversation context. This ensures the AI maintains coherence and remembers previous exchanges across sessions.", "source": "chat_completion_agent"}
{"id": "sk_037", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are the advantages and limitations of using auto function calling compared to explicit plan-based approaches?", "ground_truth": "Auto function calling leverages native LLM capabilities for simplicity and reduced latency, with better model understanding of context and dependencies. However, it depends on model quality and may be less transparent in decision-making compared to explicit plans. It also requires careful prompt engineering to avoid hallucinations or incorrect function invocations.", "source": "planners"}
{"id": "sk_038", "domain": "semantic_kernel", "difficulty": "medium", "question": "How would you implement a RAG (Retrieval-Augmented Generation) solution using Semantic Kernel's vector store and text search features?", "ground_truth": "Configure a vector store connector (e.g., Azure AI Search or Pinecone) in the Kernel, create embeddings for your document corpus, and implement text search to retrieve relevant documents based on semantic similarity. Use the retrieved documents as context in prompt templates before calling the chat completion service, augmenting the AI model's responses with domain-specific knowledge.", "source": "vector_store_connectors"}
{"id": "sk_039", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe a scenario where agent collaboration through group chat would be beneficial and how it would be orchestrated in Semantic Kernel.", "ground_truth": "Group chat is beneficial for complex problem-solving like code review (developer, security, and performance agents) or customer support escalation (first-tier, specialist, and manager agents). It's orchestrated by creating multiple ChatCompletionAgents with distinct roles, passing messages between them iteratively, and using moderation logic to determine conversation flow, consensus, or escalation paths.", "source": "agent_collaboration"}
{"id": "sk_040", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do dependency injection and the Kernel's service container pattern support extensibility and testability in Semantic Kernel applications?", "ground_truth": "The Kernel acts as an IoC container allowing registration and resolution of AI services, plugins, and dependencies. This enables easy substitution of implementations (e.g., swapping Azure OpenAI for local models), facilitates unit testing by injecting mock services, and promotes loose coupling between components. Services can be registered with different lifetimes (singleton, transient) for flexibility.", "source": "kernel_object"}
{"id": "sk_041", "domain": "semantic_kernel", "difficulty": "hard", "question": "When migrating from the deprecated Stepwise planner to auto function calling in Semantic Kernel, what are the key architectural differences in how the kernel handles function invocation and result propagation?", "ground_truth": "Auto function calling integrates directly into the chat completion service's native function calling capability, eliminating the need for a separate planning loop. This removes the overhead of explicit plan generation and execution steps, streaming results back through the AI service rather than managing function calls through discrete planner iterations, resulting in lower latency and reduced token consumption.", "source": "planners_deprecation"}
{"id": "sk_042", "domain": "semantic_kernel", "difficulty": "hard", "question": "How does the Kernel object's dependency injection container handle circular dependencies between plugins, and what patterns should developers follow to avoid runtime resolution failures in complex enterprise scenarios?", "ground_truth": "The Kernel uses a singleton-based DI container that resolves dependencies at registration time. Circular dependencies should be avoided by designing plugins with clear hierarchical dependencies; use factory functions or lazy initialization patterns to defer dependency resolution, and leverage the Kernel's service collection to register dependencies in topological order.", "source": "kernel_dependency_injection"}
{"id": "sk_043", "domain": "semantic_kernel", "difficulty": "hard", "question": "When integrating OpenAPI plugins with MCP server plugins in the same Kernel instance, what are the security and performance considerations for managing authentication contexts across different plugin types?", "ground_truth": "OpenAPI plugins require separate credential management per API endpoint, while MCP server plugins share a single transport connection. Developers must isolate authentication contexts using middleware filters to prevent credential leakage, implement per-plugin credential rotation strategies, and monitor connection pooling for MCP servers to avoid resource exhaustion in concurrent scenarios.", "source": "plugins_openapi_mcp_integration"}
{"id": "sk_044", "domain": "semantic_kernel", "difficulty": "hard", "question": "Explain the technical tradeoffs between using ChatCompletionAgent versus manual function calling with KernelFunction invocation in a high-concurrency scenario with 1000+ simultaneous requests.", "ground_truth": "ChatCompletionAgent provides simpler orchestration with automatic prompt management but incurs overhead from extra API calls per agentic loop iteration. Manual KernelFunction invocation offers finer control and potential batching optimizations but requires explicit function orchestration logic. For high concurrency, manual invocation with connection pooling and request batching typically yields better throughput, while ChatCompletionAgent is better for complex reasoning tasks with acceptable latency overhead.", "source": "agent_framework_performance"}
{"id": "sk_045", "domain": "semantic_kernel", "difficulty": "hard", "question": "How does the vector store connector abstraction in Semantic Kernel handle schema mapping inconsistencies when switching between Azure AI Search and Pinecone backends, and what data loss scenarios could occur?", "ground_truth": "Vector store connectors provide a unified interface but have backend-specific limitations: Azure AI Search enforces stricter schema validation with fixed field types, while Pinecone uses flexible metadata structures. Data loss can occur when migrating if Azure AI Search's field length limits are exceeded or if metadata structures violate its type system. Developers must implement schema validation layers and test migrations with full datasets to identify incompatibilities.", "source": "vector_store_connectors"}
{"id": "sk_046", "domain": "semantic_kernel", "difficulty": "hard", "question": "In the Process Framework, what are the implications of choosing cloud runtime versus local runtime for long-running agent workflows that involve group chat with handoff patterns, particularly regarding state persistence and failure recovery?", "ground_truth": "Cloud runtime provides built-in state persistence, automatic checkpoint recovery, and distributed handoff coordination across multiple agents, but incurs latency and cost overhead. Local runtime offers lower latency and no external dependencies but requires manual state management and loses all context on process termination. For production group chat workflows, cloud runtime is essential to survive agent failures and maintain conversation context across handoffs.", "source": "process_framework_runtime"}
{"id": "sk_047", "domain": "semantic_kernel", "difficulty": "hard", "question": "When implementing enterprise telemetry and middleware filters in Semantic Kernel, how would you detect and prevent prompt injection attacks that target OpenAPI plugin endpoints without degrading legitimate function performance?", "ground_truth": "Implement middleware filters that sanitize inputs before plugin invocation by validating parameter types, length limits, and pattern matching against known injection payloads. Use semantic analysis through a separate small LLM to detect suspicious patterns in user inputs. Apply rate limiting and anomaly detection via telemetry to identify coordinated injection attempts. Balance security with performance by caching sanitization results and applying filters only to user-controlled parameters passed to external APIs.", "source": "enterprise_security_middleware"}
{"id": "sk_048", "domain": "semantic_kernel", "difficulty": "hard", "question": "Describe the semantic differences between sequential agent orchestration and concurrent agent orchestration in Semantic Kernel, and explain how consistency guarantees differ when agents share access to vector store connectors.", "ground_truth": "Sequential orchestration processes agents serially with guaranteed ordering and consistency\u2014only one agent modifies shared state at a time. Concurrent orchestration runs agents in parallel, improving throughput but requiring explicit locking or conflict resolution when agents access the same vector store. Vector store connectors lack built-in transaction support, so concurrent writes can cause data races. Developers must implement application-level consistency mechanisms such as versioning, optimistic locking, or immutable snapshots to ensure data integrity.", "source": "agent_orchestration_consistency"}
{"id": "sk_049", "domain": "semantic_kernel", "difficulty": "hard", "question": "How does prompt template variable resolution in Semantic Kernel handle nested variable references and what performance degradation should be expected when using deeply nested templating with complex filter expressions?", "ground_truth": "Prompt templates support single-level variable substitution through KernelArguments; nested references require manual multi-pass resolution or custom filter implementations. Each template render pass scans the entire prompt text, so deeply nested structures cause O(n) performance degradation per nesting level. For complex scenarios, developers should precompile templates, cache resolution results, and avoid runtime filter evaluation within templates to maintain sub-100ms latency for interactive applications.", "source": "prompt_templating_performance"}
{"id": "sk_050", "domain": "semantic_kernel", "difficulty": "hard", "question": "In a multi-tenant enterprise environment using Semantic Kernel, how would you isolate plugin execution contexts and prevent information leakage between tenants when all tenants share a single Kernel instance with Azure OpenAI integration?", "ground_truth": "Implement tenant isolation through middleware filters that tag all Kernel invocations with tenant context before execution and validate tenant context matches authenticated user identity on responses. Use separate Azure OpenAI deployments or isolated connection strings per tenant when possible. For shared instances, implement context-aware caching in vector stores using tenant prefixes, and apply row-level security filtering on all data retrieval. Implement audit logging for all cross-tenant access attempts to detect breaches.", "source": "enterprise_multi_tenancy"}
