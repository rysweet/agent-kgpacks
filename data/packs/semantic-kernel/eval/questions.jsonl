{"id": "sk_001", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the primary role of the Kernel object in Semantic Kernel?", "ground_truth": "The Kernel object serves as a dependency injection container that manages and coordinates all components of Semantic Kernel, including plugins, AI services, and function execution.", "source": "kernel_object"}
{"id": "sk_002", "domain": "semantic_kernel", "difficulty": "easy", "question": "Name three programming languages supported by Microsoft Semantic Kernel.", "ground_truth": "Microsoft Semantic Kernel supports C#, Python, and Java.", "source": "introduction_to_semantic_kernel"}
{"id": "sk_003", "domain": "semantic_kernel", "difficulty": "easy", "question": "What are the two main types of plugins that can be added to Semantic Kernel?", "ground_truth": "The two main types of plugins are native code plugins and OpenAPI/external service plugins, with MCP server support also available.", "source": "plugins"}
{"id": "sk_004", "domain": "semantic_kernel", "difficulty": "easy", "question": "Which planners were deprecated in Semantic Kernel and replaced with auto function calling?", "ground_truth": "The Stepwise and Handlebars planners were deprecated and replaced with automatic function calling for improved orchestration.", "source": "planners"}
{"id": "sk_005", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is a prompt template in Semantic Kernel?", "ground_truth": "A prompt template is a predefined text structure with variables and formatting that allows dynamic generation of prompts for AI services.", "source": "prompt_templates"}
{"id": "sk_006", "domain": "semantic_kernel", "difficulty": "easy", "question": "Name two AI services that can be integrated with Semantic Kernel.", "ground_truth": "Chat completion and text generation are two core AI services that can be integrated with Semantic Kernel.", "source": "ai_services"}
{"id": "sk_007", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the purpose of vector store connectors in Semantic Kernel?", "ground_truth": "Vector store connectors enable integration with external vector databases like Azure AI Search, Chroma, and Pinecone for semantic search and similarity operations.", "source": "vector_stores"}
{"id": "sk_008", "domain": "semantic_kernel", "difficulty": "easy", "question": "Which Azure service is commonly integrated with Semantic Kernel for AI capabilities?", "ground_truth": "Azure OpenAI is the primary Azure service integrated with Semantic Kernel for providing language model and chat completion capabilities.", "source": "azure_openai_integration"}
{"id": "sk_009", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is function calling in Semantic Kernel?", "ground_truth": "Function calling is the mechanism that allows AI services to automatically invoke registered functions and plugins based on user requests and context.", "source": "function_calling"}
{"id": "sk_010", "domain": "semantic_kernel", "difficulty": "easy", "question": "Name three vector database connectors supported by Semantic Kernel.", "ground_truth": "Azure AI Search, Chroma, and Pinecone are three vector database connectors supported by Semantic Kernel.", "source": "vector_stores"}
{"id": "sk_011", "domain": "semantic_kernel", "difficulty": "easy", "question": "What does the Process Framework in Semantic Kernel provide?", "ground_truth": "The Process Framework provides support for both local and cloud runtime execution of AI orchestration workflows.", "source": "process_framework"}
{"id": "sk_012", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the ChatCompletionAgent used for in Semantic Kernel?", "ground_truth": "The ChatCompletionAgent is used to create autonomous agents that can engage in conversations and perform tasks using chat completion AI services.", "source": "chat_completion_agent"}
{"id": "sk_013", "domain": "semantic_kernel", "difficulty": "easy", "question": "List three agent orchestration patterns supported by Semantic Kernel.", "ground_truth": "Sequential, concurrent, handoff, and group chat are agent orchestration patterns supported by Semantic Kernel for coordinating multiple agents.", "source": "agent_orchestration"}
{"id": "sk_014", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is telemetry used for in enterprise-ready Semantic Kernel deployments?", "ground_truth": "Telemetry is used to monitor, track, and collect diagnostic information about Semantic Kernel operations for observability and performance analysis.", "source": "enterprise_readiness"}
{"id": "sk_015", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the purpose of middleware in Semantic Kernel?", "ground_truth": "Middleware provides extensibility points for intercepting and processing requests and responses in the Semantic Kernel pipeline.", "source": "enterprise_readiness"}
{"id": "sk_016", "domain": "semantic_kernel", "difficulty": "easy", "question": "What feature allows filtering of results in enterprise Semantic Kernel implementations?", "ground_truth": "Filters provide a mechanism to control and limit which results are processed or returned in Semantic Kernel operations.", "source": "enterprise_readiness"}
{"id": "sk_017", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is text search in Semantic Kernel used for?", "ground_truth": "Text search in Semantic Kernel enables searching and retrieving relevant information from documents or data sources based on semantic meaning.", "source": "text_search"}
{"id": "sk_018", "domain": "semantic_kernel", "difficulty": "easy", "question": "How do native code plugins differ from OpenAPI plugins in Semantic Kernel?", "ground_truth": "Native code plugins are written directly in the same language as your application, while OpenAPI plugins call external services through API specifications.", "source": "plugins"}
{"id": "sk_019", "domain": "semantic_kernel", "difficulty": "easy", "question": "What are MCP servers in the context of Semantic Kernel plugins?", "ground_truth": "MCP servers are external service endpoints that can be integrated as plugins in Semantic Kernel for accessing additional functionality.", "source": "plugins"}
{"id": "sk_020", "domain": "semantic_kernel", "difficulty": "easy", "question": "What is the Agent Framework in Semantic Kernel designed to enable?", "ground_truth": "The Agent Framework is designed to enable the creation and orchestration of autonomous AI agents that can collaborate and solve complex tasks.", "source": "agent_framework"}
{"id": "sk_021", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain the role of the Kernel object in Semantic Kernel and how it functions as a dependency injection container.", "ground_truth": "The Kernel object serves as the central dependency injection container in Semantic Kernel, managing AI services, plugins, and other dependencies. It acts as the orchestrator that coordinates interactions between plugins, AI services, and the application, allowing you to register and retrieve services needed for AI operations.", "source": "Kernel object and DI container"}
{"id": "sk_022", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are the three primary types of plugins available in Semantic Kernel, and how do they differ in their integration approach?", "ground_truth": "The three plugin types are: native code plugins (direct C# methods), OpenAPI plugins (external API specifications), and MCP server plugins (Model Context Protocol integration). They differ in how they expose functionality\u2014native code is tightly integrated, OpenAPI uses external API definitions, and MCP servers provide protocol-based interoperability.", "source": "Plugins in Semantic Kernel"}
{"id": "sk_023", "domain": "semantic_kernel", "difficulty": "medium", "question": "How have planners evolved in Semantic Kernel, and what modern approach has replaced the deprecated Stepwise and Handlebars planners?", "ground_truth": "Semantic Kernel has deprecated the Stepwise and Handlebars planners in favor of auto function calling, which leverages native LLM function-calling capabilities. This shift provides better performance and reliability by allowing the AI model to directly invoke functions rather than relying on manual planning logic.", "source": "Planners and auto function calling"}
{"id": "sk_024", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe the relationship between function calling and AI services in Semantic Kernel, and why this integration is important.", "ground_truth": "Function calling is a capability of AI services (particularly chat completion models) that enables automatic invocation of plugin functions based on model responses. This integration is critical because it allows AI models to trigger specific actions, access data, and orchestrate complex workflows without explicit human intervention between steps.", "source": "Function calling and AI services"}
{"id": "sk_025", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are the key AI services that can be integrated with Semantic Kernel, and how do chat completion and text generation services differ?", "ground_truth": "Key AI services include chat completion and text generation. Chat completion services (like Azure OpenAI Chat) maintain conversation context and support multi-turn interactions, while text generation services focus on producing text from prompts without conversation state. Both can be integrated via the Kernel for different use cases.", "source": "AI services integration"}
{"id": "sk_026", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do prompt templates in Semantic Kernel enable dynamic prompt creation, and what variables or expressions can they contain?", "ground_truth": "Prompt templates use variable substitution and handlebars-style syntax to dynamically generate prompts. They can contain variables (enclosed in curly braces), conditional logic, and helper functions, allowing prompts to adapt based on runtime context, user input, and application state without hardcoding values.", "source": "Prompt templates and engineering"}
{"id": "sk_027", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain the preview features of Vector Store connectors in Semantic Kernel and name at least two supported implementations.", "ground_truth": "Vector Store connectors in Semantic Kernel (preview) enable semantic search over embeddings. Supported implementations include Azure AI Search, Chroma, and Pinecone, each providing different deployment options\u2014Azure AI Search for cloud-native solutions, Chroma for local/open-source deployments, and Pinecone for fully managed vector database services.", "source": "Vector store connectors"}
{"id": "sk_028", "domain": "semantic_kernel", "difficulty": "medium", "question": "How does Text Search in Semantic Kernel (Preview) complement vector store functionality, and what problem does it address?", "ground_truth": "Text Search in Semantic Kernel provides traditional keyword-based search capabilities alongside vector semantic search. It addresses the need for hybrid search strategies, allowing applications to combine keyword matching with semantic similarity to improve retrieval accuracy and handle both exact match and conceptual queries.", "source": "Text search functionality"}
{"id": "sk_029", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe the different agent orchestration patterns available in Semantic Kernel and their appropriate use cases.", "ground_truth": "Semantic Kernel supports sequential orchestration (agents execute in order), concurrent execution (multiple agents run in parallel), handoff patterns (agents delegate to each other), and group chat (multiple agents collaborate in discussion). Sequential suits workflows with dependencies, concurrent works for independent tasks, handoff for specialized expertise routing, and group chat for collaborative problem-solving.", "source": "Agent orchestration patterns"}
{"id": "sk_030", "domain": "semantic_kernel", "difficulty": "medium", "question": "What is the ChatCompletionAgent in Semantic Kernel, and how does it differ from other agent types in the framework?", "ground_truth": "The ChatCompletionAgent is an agent type that leverages chat completion AI services to maintain conversation context and make decisions about function invocation. It differs from other agent types by focusing on conversational interactions with memory, making it ideal for multi-turn dialogue where the agent needs to understand conversation history and maintain coherence.", "source": "ChatCompletionAgent"}
{"id": "sk_031", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain how the Process Framework in Semantic Kernel handles both local and cloud runtime execution.", "ground_truth": "The Process Framework provides abstraction for defining AI workflows that can execute in both local (single-machine) and cloud (distributed) environments. It separates the process definition from the runtime execution, allowing the same orchestration logic to run on local machines for development or on cloud infrastructure for production scalability.", "source": "Process framework runtimes"}
{"id": "sk_032", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do filters and middleware in Semantic Kernel contribute to enterprise readiness, and what capabilities do they provide?", "ground_truth": "Filters and middleware enable cross-cutting concerns crucial for enterprise applications: request/response validation, logging, caching, rate limiting, and security enforcement. They allow consistent policy application across all Kernel operations without modifying individual function implementations, improving maintainability and compliance.", "source": "Filters and middleware"}
{"id": "sk_033", "domain": "semantic_kernel", "difficulty": "medium", "question": "What role does telemetry play in Semantic Kernel's enterprise readiness, and what types of telemetry are typically collected?", "ground_truth": "Telemetry enables monitoring and observability for production deployments, collecting data on function invocations, AI service calls, latency metrics, error rates, and token usage. This data is critical for performance optimization, debugging, cost management, and compliance in enterprise environments.", "source": "Telemetry and observability"}
{"id": "sk_034", "domain": "semantic_kernel", "difficulty": "medium", "question": "How does Semantic Kernel integrate with Azure OpenAI services, and what are the configuration requirements?", "ground_truth": "Semantic Kernel integrates with Azure OpenAI through the AzureOpenAIChatCompletionService class, requiring configuration of the Azure endpoint URL, deployment name, and authentication credentials (API key or managed identity). The service is registered in the Kernel's dependency container and used for chat completion and function calling operations.", "source": "Azure OpenAI integration"}
{"id": "sk_035", "domain": "semantic_kernel", "difficulty": "medium", "question": "Describe how native code plugins are implemented in Semantic Kernel and what attributes or patterns are used to expose functions.", "ground_truth": "Native code plugins are implemented as classes with methods decorated with attributes (like [KernelFunction]) to expose them as invocable functions. These methods can accept context parameters and return results; the framework automatically handles serialization, parameter binding, and integration with the Kernel's function calling mechanism.", "source": "Native code plugins"}
{"id": "sk_036", "domain": "semantic_kernel", "difficulty": "medium", "question": "What is the purpose of agent handoff in Semantic Kernel, and how does it improve multi-agent system design?", "ground_truth": "Agent handoff allows one agent to delegate tasks to another agent based on expertise or context, creating specialized agent collaboration. This improves system design by enabling separation of concerns, allowing each agent to focus on specific domains, improving response quality, and reducing unnecessary processing when tasks can be routed to specialized handlers.", "source": "Agent handoff patterns"}
{"id": "sk_037", "domain": "semantic_kernel", "difficulty": "medium", "question": "How do OpenAPI plugins in Semantic Kernel enable integration with external APIs, and what information must be provided to configure them?", "ground_truth": "OpenAPI plugins allow Semantic Kernel to invoke external REST APIs by parsing OpenAPI specifications. Configuration requires the OpenAPI document (URL or local file), optional authentication credentials, and plugin parameters. The framework automatically converts OpenAPI definitions into callable functions that can be used by agents and functions.", "source": "OpenAPI plugins"}
{"id": "sk_038", "domain": "semantic_kernel", "difficulty": "medium", "question": "Explain how group chat in Semantic Kernel (experimental) enables agent collaboration, and what challenges it addresses.", "ground_truth": "Group chat allows multiple agents to participate in a collaborative discussion, with each agent contributing perspectives and decisions based on the conversation. It addresses challenges of complex problem-solving requiring multiple viewpoints, consensus-building, and handling conflicting recommendations through structured agent-to-agent communication.", "source": "Group chat and collaboration"}
{"id": "sk_039", "domain": "semantic_kernel", "difficulty": "medium", "question": "How does the Semantic Kernel framework support concurrent agent execution, and what synchronization or coordination mechanisms are available?", "ground_truth": "Semantic Kernel supports concurrent agent execution through task-based asynchronous patterns in C#, Python, and Java. The framework provides coordination through event handling, shared kernel context, and result aggregation mechanisms, allowing multiple agents to run in parallel while maintaining consistency and enabling cross-agent communication.", "source": "Concurrent agent execution"}
{"id": "sk_040", "domain": "semantic_kernel", "difficulty": "medium", "question": "What are MCP server plugins in Semantic Kernel, and how do they differ from native and OpenAPI plugins in terms of protocol and communication?", "ground_truth": "MCP server plugins implement the Model Context Protocol for standardized communication between Semantic Kernel and external services. Unlike native plugins (in-process code) and OpenAPI plugins (REST-based), MCP uses a structured protocol for bidirectional communication, enabling more robust error handling, streaming support, and complex interactions with remote services.", "source": "MCP server plugins"}
{"id": "sk_041", "domain": "semantic_kernel", "difficulty": "hard", "question": "When implementing a custom plugin that integrates with multiple vector stores (Azure AI Search, Chroma, and Pinecone simultaneously), what are the key architectural considerations for managing connection pooling and tenant isolation in an enterprise multi-tenant environment?", "ground_truth": "Enterprise implementations require using the Kernel's dependency injection container to register vector store connectors as scoped services with proper connection pooling configuration. Each connector maintains its own connection state; tenant isolation is achieved through connector-level configuration (API keys, indices/collections) rather than Semantic Kernel itself, necessitating request-scoped tenant context propagation through middleware.", "source": "vector_store_connectors_enterprise"}
{"id": "sk_042", "domain": "semantic_kernel", "difficulty": "hard", "question": "Explain the deprecated planner deprecation pathway and why Stepwise and Handlebars planners were replaced with auto function calling. What are the performance and reliability trade-offs between the old and new approaches?", "ground_truth": "Legacy planners (Stepwise/Handlebars) relied on explicit plan generation through prompt templates, which introduced latency and parsing brittleness. Auto function calling leverages LLM-native tool calling (OpenAI function calling format), reducing round-trips, improving reliability, and enabling true concurrent execution. The trade-off is less explicit control over plan structure but superior performance and fewer hallucination-induced failures.", "source": "planners_deprecation"}
{"id": "sk_043", "domain": "semantic_kernel", "difficulty": "hard", "question": "When designing an agent that uses concurrent orchestration with group chat capabilities, what deadlock and race condition scenarios could occur, and how does the Process Framework's local vs. cloud runtime differ in handling these conditions?", "ground_truth": "Concurrent agent orchestration risks include message ordering violations, state inconsistency across agents, and deadlocks when agents wait on each other's responses. Local runtime uses in-process threading with potential race conditions in shared state; cloud runtime (distributed) mitigates this via message queues but introduces eventual consistency challenges. Process Framework abstracts both via formal state machines and checkpoint/resume semantics.", "source": "agent_orchestration_concurrency"}
{"id": "sk_044", "domain": "semantic_kernel", "difficulty": "hard", "question": "Design a solution where a Semantic Kernel application must dynamically select between Azure OpenAI, local LLM via MCP server plugin, and fallback text generation service based on cost, latency, and availability SLAs. What middleware and AI service configuration patterns would you implement?", "ground_truth": "Implement a custom middleware that wraps IChatCompletionService with a circuit breaker pattern and metrics collection. Use the Kernel's AI service registration to support multiple providers; implement a strategy selector middleware that evaluates SLA thresholds (latency percentiles, error rates, cost per token) and routes requests accordingly. Log telemetry for each provider to inform SLA decisions.", "source": "ai_services_selection_strategy"}
{"id": "sk_045", "domain": "semantic_kernel", "difficulty": "hard", "question": "In a production enterprise deployment, how would you implement audit logging and compliance filters for sensitive data handling across plugins, prompt templates, and vector store operations without introducing unacceptable latency overhead?", "ground_truth": "Use Semantic Kernel's filter middleware to intercept function invocations and store operations at the Kernel level, applying regex or PII detection asynchronously. Store audit logs to a separate event stream (not blocking the request path). For sensitive prompt data, implement prompt template encryption and key management through configuration providers. Use sampling strategies for high-volume operations to reduce I/O impact.", "source": "enterprise_readiness_compliance"}
{"id": "sk_046", "domain": "semantic_kernel", "difficulty": "hard", "question": "What are the implications of using native plugins versus OpenAPI-based plugins in a Semantic Kernel application regarding type safety, versioning, and cross-language compatibility? Provide a decision framework.", "ground_truth": "Native plugins (C#/Python/Java) offer compile-time type safety and zero serialization overhead but are language-locked and tightly coupled to SDK versions. OpenAPI plugins provide language-agnostic integration and loose coupling but incur serialization costs and runtime type discovery. Decision framework: use native plugins for performance-critical paths and internal orchestration; use OpenAPI plugins for third-party integrations, cross-platform services, and maintainability.", "source": "plugins_architecture_tradeoffs"}
{"id": "sk_047", "domain": "semantic_kernel", "difficulty": "hard", "question": "When implementing text search with Semantic Kernel across heterogeneous data sources (structured database, unstructured documents, vector embeddings), how would you prevent query injection attacks and ensure semantic consistency across different search backends?", "ground_truth": "Implement parameterized queries for structured backends (SQL) and use OpenAPI plugin abstractions for unstructured search to avoid raw query construction. For semantic consistency, normalize query embeddings using a single embedding model registered in the Kernel's AI services. Apply input validation middleware that sanitizes search terms before passing to any backend. Use per-backend authorization filters.", "source": "text_search_security"}
{"id": "sk_048", "domain": "semantic_kernel", "difficulty": "hard", "question": "Discuss the Kernel object's role as a dependency injection container. How would you configure complex scenarios where plugins depend on conditional services, and how does this affect function calling resolution and performance?", "ground_truth": "The Kernel object wraps Microsoft.Extensions.DependencyInjection; it supports scoped/singleton/transient lifetimes for service registration. Complex scenarios use factory delegates and conditional registration via service collection extensions. Function calling resolution occurs at invocation time through service lookup; conditional registration adds indirection (factory invocation) but minimal latency. Proper lifetime management prevents memory leaks in agent loops.", "source": "kernel_dependency_injection"}
{"id": "sk_049", "domain": "semantic_kernel", "difficulty": "hard", "question": "In the Process Framework, what are the key differences between local runtime (in-process) and cloud runtime execution models? How would you handle state persistence, checkpoint recovery, and long-running agent processes that exceed cloud function timeout limits?", "ground_truth": "Local runtime executes in-process with direct memory state; cloud runtime distributes execution across serverless functions with explicit state serialization. For long-running processes, use Process Framework's checkpoint/resume semantics to persist state to durable storage (blob/database) at defined milestones. Cloud runtime requires idempotent step handlers and compensating transaction logic for failure recovery; implement heartbeat mechanisms to prevent timeout-induced orphaned processes.", "source": "process_framework_runtime"}
{"id": "sk_050", "domain": "semantic_kernel", "difficulty": "hard", "question": "Design a multi-turn conversation system using ChatCompletionAgent and agent handoff orchestration where agents must maintain context consistency, avoid infinite loops, and gracefully degrade when upstream services fail. What patterns would you implement?", "ground_truth": "Implement conversation history management through a scoped service registered in the Kernel's DI container that persists chat messages. Use handoff orchestration with explicit agent selection logic (not LLM-driven) to prevent circular handoffs. Implement a conversation turn counter and max-turn limit filter to detect infinite loops. For graceful degradation, wrap AI service calls in a circuit breaker and implement fallback agents with reduced capability or escalation to human support.", "source": "agent_handoff_resilience"}
