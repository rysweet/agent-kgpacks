[
  {
    "id": "oe_001",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is OpenTelemetry and what are the three main types of telemetry data it helps generate and collect?",
    "ground_truth": "OpenTelemetry is the CNCF observability framework that generates, collects, and exports telemetry data. The three main types are traces (distributed tracing), metrics (measurements), and logs.",
    "source": "OpenTelemetry_definition"
  },
  {
    "id": "oe_002",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "Name the three main providers in the OpenTelemetry SDK and what each one is responsible for.",
    "ground_truth": "The three main providers are: TracerProvider (manages span creation and tracing), MeterProvider (manages metric collection), and LoggerProvider (manages log signal generation and processing).",
    "source": "SDK_providers"
  },
  {
    "id": "oe_003",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is OTLP and what are the two transport protocols it supports?",
    "ground_truth": "OTLP (OpenTelemetry Protocol) is the protocol used to transmit telemetry data from SDKs to collectors or backends. It supports two transport protocols: gRPC and HTTP with Protocol Buffers (protobuf).",
    "source": "OTLP_protocol"
  },
  {
    "id": "oe_004",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What are the four main components in an OpenTelemetry Collector pipeline?",
    "ground_truth": "The four main components are: receivers (accept telemetry data in various formats), processors (modify or filter data), exporters (send data to backends), and connectors (connect different signal types within the pipeline).",
    "source": "collector_pipeline"
  },
  {
    "id": "oe_005",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the purpose of semantic conventions in OpenTelemetry?",
    "ground_truth": "Semantic conventions define standardized attribute names and values for common scenarios (HTTP, database, messaging, RPC) to ensure consistency across different instrumentation libraries and backend systems.",
    "source": "semantic_conventions"
  },
  {
    "id": "oe_006",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is context propagation and name two common context propagation standards supported by OpenTelemetry.",
    "ground_truth": "Context propagation is the mechanism for passing trace context across service boundaries. OpenTelemetry supports W3C TraceContext (standard trace and span IDs) and Baggage (arbitrary key-value pairs for cross-service data sharing).",
    "source": "context_propagation"
  },
  {
    "id": "oe_007",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is auto-instrumentation in OpenTelemetry and what approach does it typically use?",
    "ground_truth": "Auto-instrumentation allows monitoring without code changes (zero-code instrumentation). It typically uses monkey patching to automatically inject instrumentation into libraries and frameworks at runtime.",
    "source": "auto_instrumentation"
  },
  {
    "id": "oe_008",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "Name three supported languages for OpenTelemetry SDK instrumentation.",
    "ground_truth": "OpenTelemetry supports instrumentation in multiple languages including Python, JavaScript, Go, and Java (among others).",
    "source": "SDK_languages"
  },
  {
    "id": "oe_009",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is a span in the context of OpenTelemetry distributed tracing?",
    "ground_truth": "A span represents a single unit of work or operation in a distributed system, containing information such as operation name, start/end time, attributes, and status. Multiple spans form a trace.",
    "source": "distributed_tracing_concepts"
  },
  {
    "id": "oe_010",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the difference between a trace and a span?",
    "ground_truth": "A trace is a complete end-to-end request flow across services, composed of multiple related spans. A span is a single operation or unit of work within that trace.",
    "source": "distributed_tracing_concepts"
  },
  {
    "id": "oe_011",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is sampling in OpenTelemetry and why is it important?",
    "ground_truth": "Sampling is a technique to reduce the volume of telemetry data collected by selecting only a percentage of traces for export. It is important to manage storage costs and performance impact while maintaining observability.",
    "source": "sampling"
  },
  {
    "id": "oe_012",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "Name three backend systems that OpenTelemetry exporters can send data to.",
    "ground_truth": "OpenTelemetry exporters support multiple backends including Jaeger (distributed tracing), Prometheus (metrics), Zipkin (distributed tracing), and various vendor-specific backends.",
    "source": "exporter_backends"
  },
  {
    "id": "oe_013",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the purpose of batching in OpenTelemetry Collector configuration?",
    "ground_truth": "Batching groups multiple telemetry records together before export to reduce network overhead, improve throughput, and lower resource consumption by sending fewer, larger requests.",
    "source": "collector_best_practices"
  },
  {
    "id": "oe_014",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the role of extensions in an OpenTelemetry Collector?",
    "ground_truth": "Extensions provide optional components that enhance Collector functionality, such as health checks, performance profiling, or integration with external systems, without directly processing telemetry data.",
    "source": "collector_extensions"
  },
  {
    "id": "oe_015",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the purpose of processors in the OpenTelemetry Collector pipeline?",
    "ground_truth": "Processors modify, filter, or enrich telemetry data as it flows through the pipeline. Examples include batch processors, sampling processors, and attribute processors.",
    "source": "collector_processors"
  },
  {
    "id": "oe_016",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is memory limiting in an OpenTelemetry Collector and why is it important?",
    "ground_truth": "Memory limiting is a configuration practice to set maximum memory usage for the Collector, preventing it from consuming excessive resources and causing system instability or crashes.",
    "source": "collector_memory_limits"
  },
  {
    "id": "oe_017",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What does the OpenTelemetry Kubernetes operator provide?",
    "ground_truth": "The OpenTelemetry Kubernetes operator enables auto-instrumentation for containerized applications in Kubernetes by automatically injecting instrumentation without code modification.",
    "source": "kubernetes_operator"
  },
  {
    "id": "oe_018",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "Give an example of HTTP semantic convention attributes that OpenTelemetry defines.",
    "ground_truth": "HTTP semantic conventions include attributes such as http.method (GET, POST), http.status_code (200, 404), http.url (the full request URL), and http.target (the request path).",
    "source": "semantic_conventions_http"
  },
  {
    "id": "oe_019",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is filtering in the context of OpenTelemetry Collector configuration?",
    "ground_truth": "Filtering is a processor configuration that selectively discards or includes telemetry data based on criteria such as attribute values, reducing data volume and costs.",
    "source": "collector_filtering"
  },
  {
    "id": "oe_020",
    "domain": "opentelemetry_expert",
    "difficulty": "easy",
    "question": "What is the relationship between W3C TraceContext and context propagation in OpenTelemetry?",
    "ground_truth": "W3C TraceContext is a standard format for propagating trace and span IDs across service boundaries, enabling distributed tracing by ensuring trace context is consistent across multiple services.",
    "source": "w3c_tracecontext"
  },
  {
    "id": "oe_021",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "How does the TracerProvider in the OpenTelemetry SDK determine which spans to actually export to the backend, and what is the role of the Sampler component in this decision?",
    "ground_truth": "The Sampler is invoked during span creation and makes a sampling decision (RECORD_AND_SAMPLE, RECORD_ONLY, DROP) based on trace ID and attributes, allowing you to reduce data volume by only exporting a percentage of traces while maintaining representative data.",
    "source": "SDK_instrumentation_sampling"
  },
  {
    "id": "oe_022",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "In the OpenTelemetry Collector pipeline, explain the difference between a Processor and a Connector, and describe a use case where you would need a Connector instead of just using multiple exporters.",
    "ground_truth": "Processors modify telemetry data within a single signal pipeline (traces, metrics, logs), while Connectors consume one signal type and emit another (e.g., converting metrics to traces). Use Connectors when you need to correlate or derive data across different signal types.",
    "source": "collector_pipeline_connectors"
  },
  {
    "id": "oe_023",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "You are instrumenting a Python microservice with OpenTelemetry. Explain the relationship between context propagation headers (W3C TraceContext) and the Context API, and why both are necessary for distributed tracing.",
    "ground_truth": "W3C TraceContext headers carry trace ID and span ID across process boundaries in HTTP requests, while the Context API manages the active span and baggage within a process. Together they enable correlation: headers extract remote context, Context API makes it active locally.",
    "source": "context_propagation_W3C"
  },
  {
    "id": "oe_024",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "When configuring the OTLP exporter in an OpenTelemetry SDK, what are the trade-offs between using gRPC and HTTP/protobuf transports, and in what scenarios would you choose one over the other?",
    "ground_truth": "gRPC uses HTTP/2 multiplexing and is more efficient for high-volume data, while HTTP/protobuf is simpler and works through proxies/firewalls more reliably. Choose gRPC for performance-critical paths and HTTP when network infrastructure is restrictive or mixed compatibility is needed.",
    "source": "OTLP_protocol_transport"
  },
  {
    "id": "oe_025",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Describe how auto-instrumentation via monkey patching works in OpenTelemetry, and explain why it requires careful ordering during application startup compared to manual SDK initialization.",
    "ground_truth": "Auto-instrumentation patches library functions before your code runs (via agents or initialization hooks), automatically creating spans without code changes. It must be loaded early (before instrumented libraries) to intercept calls; late loading may miss initialization spans or fail to patch already-imported modules.",
    "source": "auto_instrumentation_monkey_patching"
  },
  {
    "id": "oe_026",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "A Go service exports metrics to both Prometheus and a vendor backend via the OpenTelemetry Collector. Explain how the MeterProvider aggregation settings might differ between exporting to Prometheus (a pull model) versus push-based OTLP exporters.",
    "ground_truth": "Prometheus pull model requires cumulative aggregation and precise cardinality control (to avoid high-cardinality metrics), while OTLP push can handle delta or cumulative aggregation flexibly. MeterProvider aggregation temporality should match the exporter's expectations to avoid data loss or misinterpretation.",
    "source": "SDK_instrumentation_MeterProvider_aggregation"
  },
  {
    "id": "oe_027",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "In the OpenTelemetry semantic conventions, what information should be captured in the 'http.request.header' attribute for an HTTP span, and what security considerations should you take into account?",
    "ground_truth": "The semantic convention captures HTTP request headers as key-value pairs, but sensitive headers (Authorization, Set-Cookie, API keys) should be excluded to prevent credential leakage in logs/traces. Use filtering or processor policies to redact such attributes before export.",
    "source": "semantic_conventions_HTTP_attributes"
  },
  {
    "id": "oe_028",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "When implementing distributed tracing for a system with synchronous and asynchronous service calls, explain how context propagation differs between the two and what challenges arise in async contexts.",
    "ground_truth": "Synchronous calls propagate context via call stack and thread-local storage, while async calls (callbacks, futures, event loops) lose context at suspension points. OpenTelemetry requires explicit context restoration before async task execution (via baggage propagators or context managers like Python's contextvars).",
    "source": "context_propagation_async"
  },
  {
    "id": "oe_029",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Configure a JavaScript/Node.js OpenTelemetry SDK with auto-instrumentation for Express and PostgreSQL. What components must be initialized, and what is the correct order of operations?",
    "ground_truth": "Initialize NodeTracerProvider first, register auto-instrumentation plugins (ExpressInstrumentation, PostgresInstrumentation) before app code loads, add a batch span processor, configure the exporter, and finally register the provider globally. Incorrect ordering causes auto-instrumentation to miss spans.",
    "source": "SDK_instrumentation_JavaScript_setup"
  },
  {
    "id": "oe_030",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "The OpenTelemetry Collector's batch processor introduces latency and memory buffering. Explain the configuration parameters (batch size, timeout, queue size) and how to tune them for a high-throughput system without exceeding memory limits.",
    "ground_truth": "Batch size (spans per batch) and timeout control when batches are sent; queue size limits memory. For high throughput, increase batch size and queue size, but monitor memory usage. Timeout prevents excessive latency if traffic is bursty. Memory limit should account for span size and system capacity.",
    "source": "collector_configuration_batching"
  },
  {
    "id": "oe_031",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "You are exporting traces to Jaeger via the OpenTelemetry Collector. Explain the semantic conventions required for Jaeger to correctly display service name, operation name, and span status in its UI.",
    "ground_truth": "Semantic convention attributes 'service.name' identifies the service, 'rpc.method' or 'http.method' + 'http.target' define operation name, and 'otel.status_code' with 'otel.status_description' map to span status. Jaeger parses these to populate UI fields; missing conventions degrade visualization quality.",
    "source": "semantic_conventions_Jaeger_integration"
  },
  {
    "id": "oe_032",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Describe how the OpenTelemetry Collector's LoggerProvider differs from traditional log exporters, and explain when you would use OTLP logs versus standalone log aggregation (ELK, Loki).",
    "ground_truth": "LoggerProvider in OpenTelemetry SDK generates structured logs as telemetry with full context correlation (trace ID, span ID), while standalone log aggregators are decoupled. Use OTLP logs when you need trace-log correlation and unified observability backend; use standalone for log-only use cases or when trace integration is not required.",
    "source": "SDK_instrumentation_LoggerProvider"
  },
  {
    "id": "oe_033",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "A Java application uses the OpenTelemetry Java agent with auto-instrumentation. Explain the role of the Instrumentation SPI, why certain libraries require custom instrumentation, and how to verify instrumentation is active.",
    "ground_truth": "The Instrumentation SPI allows the agent to inject instrumentation into library classes at load time. Custom instrumentation is needed for proprietary or dynamically-loaded code not covered by built-in agents. Verify with agent debug logs, span output checks, or -Dotel.javaagent.debug=true flag.",
    "source": "auto_instrumentation_Java_agent"
  },
  {
    "id": "oe_034",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "In semantic conventions for database spans, what attributes must be captured for a SQL query to a PostgreSQL database, and how do they differ from NoSQL (MongoDB) conventions?",
    "ground_truth": "SQL spans require 'db.system', 'db.name', 'db.user', 'db.statement' (query), and 'db.sql.table'; NoSQL spans require 'db.system', 'db.name', 'db.operation' (e.g., find, insert), and 'db.mongodb.collection'. SQL includes statement text; NoSQL focuses on operation type due to schema flexibility.",
    "source": "semantic_conventions_database_attributes"
  },
  {
    "id": "oe_035",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Explain how OpenTelemetry's baggage mechanism works, the difference between baggage and span attributes, and a practical use case where baggage is preferable for propagating request context.",
    "ground_truth": "Baggage is context metadata propagated across process boundaries (via headers) and available to all spans, while span attributes are specific to individual spans. Use baggage for cross-cutting data (user ID, request ID, environment) that should apply to all downstream services; use span attributes for span-specific details.",
    "source": "context_propagation_baggage"
  },
  {
    "id": "oe_036",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Configure an OpenTelemetry Collector receiver for Prometheus metrics (pull model) and explain the configuration required for the Collector to scrape a target application without the application needing OTLP export.",
    "ground_truth": "Use the Prometheus receiver in the Collector with scrape_configs specifying targets and intervals. The Collector pulls metrics from the target's /metrics endpoint, converts them to OTLP format internally, and exports via configured exporters. This enables OTLP export from non-OTLP sources.",
    "source": "collector_pipeline_receivers_Prometheus"
  },
  {
    "id": "oe_037",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "When deploying the OpenTelemetry Collector as a sidecar in Kubernetes, what resource limits and configuration best practices should you implement to prevent the Collector from becoming a bottleneck?",
    "ground_truth": "Set CPU/memory requests and limits based on expected throughput, use batch processors to reduce overhead, implement queue length limits, and configure tail sampling to reduce data volume. Monitor Collector metrics (queued spans, dropped spans, memory usage) and scale horizontally if needed.",
    "source": "collector_configuration_Kubernetes_best_practices"
  },
  {
    "id": "oe_038",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "A service calls a third-party API that does not use OpenTelemetry. How would you instrument this call to maintain distributed trace continuity, and what challenges might you face with trace context propagation?",
    "ground_truth": "Manually create a span for the API call and inject W3C TraceContext headers into the outgoing request. If the third-party service does not return correlation headers, you cannot extract its internal spans; the trace will show only the outbound call as a leaf span without downstream activity.",
    "source": "distributed_tracing_third_party_integration"
  },
  {
    "id": "oe_039",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "Explain the difference between head-based sampling (at span creation) and tail-based sampling (at export), the trade-offs of each, and when you would use tail-based sampling in the OpenTelemetry Collector.",
    "ground_truth": "Head-based sampling decides early (low overhead) but may drop entire traces before seeing errors; tail-based sampling (via Collector's sampling processor) sees full trace context and can intelligently retain error traces. Use tail-based for error capture and performance debugging when Collector overhead is acceptable.",
    "source": "distributed_tracing_sampling_strategies"
  },
  {
    "id": "oe_040",
    "domain": "opentelemetry_expert",
    "difficulty": "medium",
    "question": "You need to export metrics from OpenTelemetry to both Prometheus (via pull) and a cloud vendor backend (via push) simultaneously. Design the Collector pipeline configuration and explain how the data flows through receivers, processors, and exporters.",
    "ground_truth": "Configure a Prometheus receiver (scrapes OTLP metrics or native Prometheus source), apply processors for filtering/transformation, split into two pipelines: one to Prometheus remote-write exporter and one to vendor OTLP exporter. Each pipeline independently exports, allowing different formats and protocols.",
    "source": "collector_pipeline_multi_exporter_design"
  },
  {
    "id": "oe_041",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "When implementing context propagation across service boundaries using W3C TraceContext and baggage simultaneously, what are the security implications of propagating sensitive data in baggage headers, and how should you configure the SDK to prevent unintended leakage?",
    "ground_truth": "Baggage is propagated in plaintext headers and is visible to all downstream services; sensitive data should never be placed in baggage. Configure baggage filtering at the SDK level using BaggagePropagator or at the collector using processors (e.g., attributes processor) to strip or mask sensitive keys before external propagation. Use encryption at transport layer (TLS) and implement strict baggage allowlists.",
    "source": "context_propagation_security"
  },
  {
    "id": "oe_042",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "In a high-throughput Java application using OpenTelemetry SDK, explain how to balance memory consumption and trace fidelity when configuring the BatchSpanProcessor. What are the performance trade-offs of adjusting maxQueueSize, maxExportBatchSize, and scheduleDelayMillis?",
    "ground_truth": "BatchSpanProcessor buffers spans in a queue (maxQueueSize) before exporting in batches (maxExportBatchSize). Larger maxQueueSize increases memory but reduces export frequency; larger maxExportBatchSize reduces CPU overhead but increases latency. scheduleDelayMillis controls flush timing\u2014longer delays batch more spans (better throughput) but increase memory and latency. For high-throughput: increase batch size and queue size with memory monitoring; for low-latency: reduce delays and batch sizes.",
    "source": "sdk_instrumentation_java_batching"
  },
  {
    "id": "oe_043",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "When configuring an OpenTelemetry Collector in a Kubernetes environment with memory constraints, describe a multi-stage processor pipeline that prevents out-of-memory crashes due to sudden traffic spikes while maintaining acceptable data loss rates.",
    "ground_truth": "Use: (1) attributes processor to drop high-cardinality or unnecessary attributes early, (2) sampling processor (probabilistic or tail-based) to reduce volume before batching, (3) batch processor with memory_limiter to enforce hard limits and trigger tail-sampling decisions, (4) tail_sampling processor for intelligent filtering of trace data. Configure memory_limiter with limit_mib and spike_limit_mib to prevent OOM; pair with batch processor to export before hitting limits.",
    "source": "collector_pipeline_memory_management"
  },
  {
    "id": "oe_044",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "Explain the differences between implementing auto-instrumentation via Kubernetes Operator versus language-specific Java agent injection, including trade-offs in deployment complexity, compatibility, and observability gaps.",
    "ground_truth": "Kubernetes Operator (via webhook mutation) injects agents across services but requires cluster-level permissions and cannot handle non-containerized workloads. Java agent (-javaagent) provides direct bytecode manipulation with better library coverage but requires build-time configuration or JVM startup modifications. Operators are cluster-agnostic; agents may have compatibility issues with newer frameworks. Operator requires less code change but more infrastructure; agents risk startup latency and increased JVM memory footprint.",
    "source": "auto_instrumentation_deployment_strategies"
  },
  {
    "id": "oe_045",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "Design a semantic convention mapping strategy for a legacy application that uses non-standard database attribute names (e.g., 'query_text' instead of 'db.statement', 'host_ip' instead of 'server.address'). How would you implement this in the OpenTelemetry SDK instrumentation layer?",
    "ground_truth": "Implement a custom SpanProcessor or use the attributes processor in the collector to transform legacy attributes to OTel semantic conventions. In SDK: create a custom SpanProcessor that maps attributes on span end. In collector: use attributes processor with actions to rename keys (from 'query_text' to 'db.statement'). Prefer SDK-level mapping for consistency; use collector mapping for centralized policy enforcement. Document mappings to ensure all services apply identical transformations.",
    "source": "semantic_conventions_custom_mapping"
  },
  {
    "id": "oe_046",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "In a polyglot microservices environment (Python, Go, Node.js, Java), what are the critical differences in how each language's OpenTelemetry SDK handles async context propagation, and what pitfalls should you avoid when tracing across async boundaries?",
    "ground_truth": "Python relies on contextvars (async-safe but requires explicit setup with AsyncContextManager). Go uses context.Context propagation (built-in, most reliable). Node.js uses async_hooks or AsyncLocalStorage (v13+), but requires careful instrumentation of async operations. Java uses ThreadLocal + virtual threads support. Pitfalls: Python missing contextvars setup loses trace context in async calls; Go passing wrong context copies lose correlation; Node.js missing Promise instrumentation breaks trace continuity; Java thread pool misconfigurations drop context. Use language-specific auto-instrumentation to avoid these issues.",
    "source": "sdk_async_context_propagation"
  },
  {
    "id": "oe_047",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "You deploy an OpenTelemetry Collector with both gRPC and HTTP/protobuf receivers behind a load balancer. Under what conditions would you experience trace fragmentation or out-of-order span delivery, and how would you diagnose this issue?",
    "ground_truth": "Trace fragmentation occurs when spans from the same trace hit different collector instances without shared state, or when batch processors export before all spans arrive due to load balancing. HTTP stateless nature and gRPC connection pooling can cause uneven distribution. Diagnose via: collector logs (check batch processor delays), metrics (otelcol_receiver_*_spans_received per instance), and examining trace IDs in backend (missing root spans, orphaned children). Mitigation: use consistent hashing load balancing or a single collector gateway; increase batch timeouts.",
    "source": "collector_pipeline_reliability"
  },
  {
    "id": "oe_048",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "When implementing head-based sampling at the SDK level versus tail-based sampling at the collector, explain the data loss implications for tail-latency tracing and how you would design a hybrid strategy that minimizes both data loss and overhead.",
    "ground_truth": "Head-based sampling (SDK) decides early using only root span attributes, causing loss of important slow traces not sampled initially. Tail-based sampling (collector) sees complete traces but requires buffering all data (higher memory). Hybrid strategy: use head-based probabilistic sampling (10-20%) at SDK to reduce collector load, then apply tail-sampling at collector for specific criteria (latency > Xms, errors, specific services). This maintains slow/error trace visibility while keeping overhead acceptable. Document sampling rates to avoid confusion in metrics.",
    "source": "sampling_distributed_strategy"
  },
  {
    "id": "oe_049",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "In a GDPR-compliant environment, design a data scrubbing pipeline using OpenTelemetry components that removes personally identifiable information (PII) from traces while preserving observability value. What are the risks of implementing this at the SDK versus collector level?",
    "ground_truth": "Collector-level scrubbing (attributes processor with regex-based redaction, span name filtering) is preferred for centralized enforcement and audit trails. SDK-level scrubbing risks inconsistency across services and is harder to update. Use collector processor chain: (1) attributes processor to redact PII patterns (email, SSN, credit card regex), (2) span processor to drop sensitive spans. Risks of SDK: individual services may have different rules; PII may leak before reaching collector if using exporters. Risks of collector: unencrypted PII crosses networks before scrubbing (mitigate with TLS). Log all redaction actions for compliance auditing.",
    "source": "data_privacy_pii_redaction"
  },
  {
    "id": "oe_050",
    "domain": "opentelemetry_expert",
    "difficulty": "hard",
    "question": "Explain the cardinality explosion problem when using high-cardinality dimensions (e.g., user_id, request_id) in OTLP metrics exported to Prometheus, and propose a collector-level solution using OpenTelemetry semantic conventions and processors.",
    "ground_truth": "High-cardinality attributes create a unique metric time series for each value, exhausting Prometheus storage and causing performance degradation. Solution: (1) Remove user_id/request_id from metrics at instrumentation level (track in traces instead); (2) Use attributes processor in collector to strip high-cardinality keys before export; (3) Use span-to-metrics connector to aggregate only on low-cardinality attributes (service, method, status); (4) Apply semantic conventions that naturally limit cardinality (http.status_code vs. error messages). Configure Prometheus scrape_configs with metric_relabel_configs as backup filtering.",
    "source": "metrics_cardinality_management"
  }
]
