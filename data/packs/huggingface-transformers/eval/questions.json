[
  {
    "id": "ht_001",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the primary purpose of the Hugging Face Transformers library?",
    "ground_truth": "The Transformers library provides pre-trained models and tools for natural language processing (NLP), computer vision, and audio tasks, allowing users to easily download, fine-tune, and deploy state-of-the-art transformer models.",
    "source": "library_overview"
  },
  {
    "id": "ht_002",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "How do you load a pre-trained model from Hugging Face Model Hub?",
    "ground_truth": "Use the `from_pretrained()` method from the appropriate model class (e.g., `AutoModel.from_pretrained('model-name')`) or use specialized classes like `AutoModelForSequenceClassification` to load a model with a specific task head.",
    "source": "model_loading"
  },
  {
    "id": "ht_003",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the AutoTokenizer class used for?",
    "ground_truth": "AutoTokenizer automatically selects and loads the correct tokenizer for a given model based on the model name or configuration, eliminating the need to manually specify which tokenizer class to use.",
    "source": "tokenizer_auto_selection"
  },
  {
    "id": "ht_004",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does the tokenizer.encode() method return?",
    "ground_truth": "The encode() method converts text into a list of token IDs (integers) that can be fed into a transformer model, typically including special tokens like [CLS] and [SEP] for BERT-like models.",
    "source": "tokenization_basics"
  },
  {
    "id": "ht_005",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the purpose of the Trainer class in Hugging Face Transformers?",
    "ground_truth": "The Trainer class simplifies model fine-tuning by handling training loops, evaluation, loss computation, optimization, and checkpointing, requiring only a model, dataset, and TrainingArguments configuration.",
    "source": "trainer_class"
  },
  {
    "id": "ht_006",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is TrainingArguments used for?",
    "ground_truth": "TrainingArguments is a configuration class that specifies hyperparameters for training such as learning rate, batch size, number of epochs, output directory, logging frequency, and evaluation strategy.",
    "source": "training_configuration"
  },
  {
    "id": "ht_007",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does the pipeline() function do?",
    "ground_truth": "The pipeline() function provides a high-level API that handles model loading, tokenization, and inference for common NLP tasks (like text classification, NER, question-answering) in a single function call.",
    "source": "pipeline_api"
  },
  {
    "id": "ht_008",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "How do you specify a custom task with the pipeline() function?",
    "ground_truth": "Pass the task name as the first argument to pipeline(), such as `pipeline('text-classification')`, `pipeline('token-classification')`, or `pipeline('question-answering')`.",
    "source": "pipeline_tasks"
  },
  {
    "id": "ht_009",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the Model Hub and where is it located?",
    "ground_truth": "The Model Hub is a repository hosted at huggingface.co that contains thousands of pre-trained models contributed by the community and Hugging Face, along with model cards documenting their use and limitations.",
    "source": "model_hub"
  },
  {
    "id": "ht_010",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does a model card contain?",
    "ground_truth": "A model card contains metadata about a model including its architecture, training data, intended use cases, performance metrics, limitations, and recommendations for responsible use.",
    "source": "model_card"
  },
  {
    "id": "ht_011",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the difference between attention_mask and token_type_ids in tokenizer output?",
    "ground_truth": "attention_mask indicates which tokens should be attended to (1) and which are padding (0), while token_type_ids distinguish between different segments of text, commonly used in BERT for separating sentence A from sentence B.",
    "source": "tokenizer_output_fields"
  },
  {
    "id": "ht_012",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does the pad_token_id represent in a tokenizer?",
    "ground_truth": "The pad_token_id is the token ID assigned to padding tokens, which are added to shorter sequences to make all sequences in a batch the same length for batch processing.",
    "source": "special_tokens"
  },
  {
    "id": "ht_013",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "How do you push a fine-tuned model to the Hugging Face Hub?",
    "ground_truth": "Use the `push_to_hub()` method on the model and tokenizer objects, or configure the Trainer with `push_to_hub=True` in TrainingArguments, after logging in with `huggingface-cli login`.",
    "source": "model_sharing"
  },
  {
    "id": "ht_014",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is AutoConfig used for?",
    "ground_truth": "AutoConfig automatically loads the configuration file for a model from the Model Hub, which contains architecture-specific settings like hidden size, number of attention heads, and vocabulary size.",
    "source": "config_loading"
  },
  {
    "id": "ht_015",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What are some common model architecture classes in Transformers?",
    "ground_truth": "Common architecture classes include BertModel, GPT2Model, T5ForConditionalGeneration, and RobertaModel, with task-specific variants like AutoModelForSequenceClassification for classification and AutoModelForTokenClassification for NER.",
    "source": "model_architectures"
  },
  {
    "id": "ht_016",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does the output of a forward pass through a transformer model typically contain?",
    "ground_truth": "The forward pass typically returns a tuple containing logits (raw model outputs) and optionally hidden states and attention weights, depending on the output configuration and task.",
    "source": "model_output"
  },
  {
    "id": "ht_017",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the purpose of the datasets library in relation to Transformers?",
    "ground_truth": "The datasets library provides tools to load, preprocess, and manage datasets for training transformer models, with built-in support for common NLP benchmarks and efficient data handling.",
    "source": "datasets_integration"
  },
  {
    "id": "ht_018",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "How do you set a model to evaluation mode?",
    "ground_truth": "Call the `eval()` method on the model object (e.g., `model.eval()`) to disable dropout and batch normalization, which is important for consistent inference and evaluation.",
    "source": "model_modes"
  },
  {
    "id": "ht_019",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What is the purpose of using torch.no_grad() during inference?",
    "ground_truth": "torch.no_grad() disables gradient calculation during inference, reducing memory consumption and speeding up computations since gradients are not needed when you are only making predictions.",
    "source": "inference_optimization"
  },
  {
    "id": "ht_020",
    "domain": "huggingface_transformers",
    "difficulty": "easy",
    "question": "What does the max_length parameter in tokenizer() control?",
    "ground_truth": "The max_length parameter specifies the maximum number of tokens that the tokenizer will produce, truncating longer sequences to this length if exceeded.",
    "source": "tokenizer_parameters"
  },
  {
    "id": "ht_021",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the difference between using `AutoModel` and `AutoModelForSequenceClassification` when loading a pretrained transformer model, and when would you choose one over the other?",
    "ground_truth": "AutoModel loads the base transformer architecture without task-specific heads, returning raw hidden states. AutoModelForSequenceClassification adds a classification head on top. Use AutoModel for custom downstream tasks or feature extraction; use task-specific variants for standard NLP tasks like classification, QA, or NER.",
    "source": "model_loading"
  },
  {
    "id": "ht_022",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain the purpose of the `max_position_embeddings` configuration parameter and what happens when your input sequence exceeds this limit.",
    "ground_truth": "max_position_embeddings defines the maximum sequence length the model was trained on. If input exceeds this, you'll encounter errors or need to use position interpolation techniques. This is model-specific (e.g., BERT uses 512, newer models may use 2048+) and cannot be simply increased without retraining.",
    "source": "configuration_limits"
  },
  {
    "id": "ht_023",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the role of the `attention_mask` in transformer models, and how does it interact with padding tokens?",
    "ground_truth": "attention_mask is a binary tensor (1 for real tokens, 0 for padding) that prevents the model from attending to padded positions. It ensures that self-attention operations ignore padding tokens, preventing them from contributing to meaningful token representations and improving computational efficiency.",
    "source": "tokenization_masking"
  },
  {
    "id": "ht_024",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Describe the trade-offs between `torch.no_grad()` context and setting `model.eval()` when performing inference with a transformer model.",
    "ground_truth": "`torch.no_grad()` disables gradient computation to save memory and speed up inference but doesn't affect dropout or batch normalization. `model.eval()` disables dropout and switches batch norm to evaluation mode, but still computes gradients. Use both together for optimal inference: `model.eval()` with `torch.no_grad()` for proper evaluation without gradient overhead.",
    "source": "inference_modes"
  },
  {
    "id": "ht_025",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is quantization in the context of Hugging Face Transformers, and what are the main approaches to quantize a model?",
    "ground_truth": "Quantization reduces model size and latency by converting weights from float32 to lower-bit representations. Main approaches include: post-training quantization (PTQ) applied after training, dynamic quantization, and quantization-aware training (QAT). Hugging Face supports integration with libraries like GPTQ and bitsandbytes for 8-bit and 4-bit quantization.",
    "source": "quantization"
  },
  {
    "id": "ht_026",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain how the `AutoTokenizer.from_pretrained()` method selects the correct tokenizer architecture for a given model, and what happens if tokenizer and model are mismatched.",
    "ground_truth": "AutoTokenizer uses the model's config to identify the tokenizer class stored in the model repo's `tokenizer_config.json`. Mismatched tokenizer and model cause semantic drift: tokens may not align with what the model expects, degrading performance or producing errors since each architecture (BERT, GPT-2, RoBERTa) uses different tokenization schemes.",
    "source": "tokenizer_selection"
  },
  {
    "id": "ht_027",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the purpose of `hidden_dropout_prob` and `attention_probs_dropout_prob` configuration parameters, and why might they be different values?",
    "ground_truth": "`hidden_dropout_prob` applies dropout to hidden layer outputs while `attention_probs_dropout_prob` applies dropout to attention weights. They can differ because attention mechanisms may benefit from different regularization strengths than feed-forward layers. These are training-time parameters; they have no effect during inference in eval mode.",
    "source": "dropout_configuration"
  },
  {
    "id": "ht_028",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Describe the difference between `output_hidden_states=True` and `output_attentions=True` in the model forward pass, and explain potential memory implications.",
    "ground_truth": "`output_hidden_states=True` returns hidden states from all layers; `output_attentions=True` returns attention weights from all heads. Both significantly increase memory usage and output size, especially for large models with many layers. Use selectively for analysis or feature extraction; avoid in production inference unless necessary.",
    "source": "model_outputs"
  },
  {
    "id": "ht_029",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the Trainer class in Hugging Face Transformers, and what are key advantages of using it over manual training loops?",
    "ground_truth": "Trainer is a high-level API that handles training loops, evaluation, logging, checkpointing, and distributed training. Key advantages include automatic mixed precision, gradient accumulation, learning rate scheduling, evaluation on validation sets, and multi-GPU/TPU support with minimal code, reducing boilerplate and common mistakes.",
    "source": "trainer_api"
  },
  {
    "id": "ht_030",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain what `token_type_ids` are, which models require them, and what happens if you omit them when they are expected.",
    "ground_truth": "`token_type_ids` (segment IDs) distinguish between two input sequences, typically 0 for the first sequence and 1 for the second. Models like BERT use them for NSP (Next Sentence Prediction) and multi-sentence classification. Omitting them when expected may cause performance degradation or errors, as the model won't distinguish input segments.",
    "source": "token_type_ids"
  },
  {
    "id": "ht_031",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is LoRA (Low-Rank Adaptation) and how does the `peft` library integrate with Hugging Face Transformers for efficient fine-tuning?",
    "ground_truth": "LoRA freezes pre-trained weights and trains low-rank matrices to approximate weight updates, reducing trainable parameters by 90%+. The PEFT library provides integrations with Transformers via `get_peft_model()` to wrap models, enabling memory-efficient fine-tuning on consumer GPUs while maintaining near-full-model performance.",
    "source": "peft_lora"
  },
  {
    "id": "ht_032",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Describe the role of `pad_token_id` in the model configuration and explain why it must match the tokenizer's pad token.",
    "ground_truth": "`pad_token_id` tells the model which token ID represents padding during generation and loss computation. It must match the tokenizer's `pad_token_id` to ensure consistent preprocessing and post-processing. Mismatches cause incorrect padding handling, loss calculation errors, or generation failures.",
    "source": "special_tokens"
  },
  {
    "id": "ht_033",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the difference between `model.generate()` with `greedy_search` vs `beam_search`, and when would you use each?",
    "ground_truth": "Greedy search selects the highest-probability token at each step (fast, low quality). Beam search maintains k-best hypotheses and explores multiple paths (slower, better quality). Use greedy for speed-critical applications; use beam search for quality-critical tasks like translation. Beam search has configurable `num_beams` parameter.",
    "source": "generation_methods"
  },
  {
    "id": "ht_034",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain the concept of `use_cache=True` in generation and why it is important for inference latency.",
    "ground_truth": "`use_cache=True` reuses computed key-value caches across generation steps, avoiding recomputation of attention for previously generated tokens. This reduces latency by ~2x for long sequences. It trades memory for speed; disable if memory is extremely limited, but it's essential for efficient autoregressive generation.",
    "source": "kv_caching"
  },
  {
    "id": "ht_035",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the purpose of the `BertConfig` (or model-specific config) class, and how does it relate to the model's computational graph?",
    "ground_truth": "Config classes store architecture hyperparameters (hidden size, num layers, attention heads, vocab size, etc.) that define the model's structure. They are serialized to JSON and used during model initialization to build the exact computational graph. Modifying config changes model capacity and requires retraining; configs cannot be arbitrarily changed on a pre-trained model.",
    "source": "model_config"
  },
  {
    "id": "ht_036",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Describe the purpose of `pipeline()` in Hugging Face Transformers and what preprocessing/postprocessing it handles automatically.",
    "ground_truth": "`pipeline()` is a high-level API that combines tokenization, model inference, and postprocessing into a single function call. It automatically handles: tokenization, batching, moving tensors to the correct device, decoding outputs, and task-specific post-processing (e.g., softmax for classification). It's ideal for quick prototyping but less flexible than raw model usage.",
    "source": "pipeline_api"
  },
  {
    "id": "ht_037",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain how gradient checkpointing works in Transformers and what memory-speed trade-off it introduces during training.",
    "ground_truth": "Gradient checkpointing recomputes activations during backprop instead of storing them, reducing memory usage by ~50% at the cost of slower training (~20-30% slower). It's enabled via `gradient_checkpointing=True` in config. Useful for training large models or large batch sizes on memory-limited GPUs.",
    "source": "gradient_checkpointing"
  },
  {
    "id": "ht_038",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "What is the difference between `model.push_to_hub()` and manually uploading to Hugging Face Hub, and what metadata is automatically included?",
    "ground_truth": "`model.push_to_hub()` uses the HF SDK to automatically upload model weights, config, tokenizer, and README. It handles authentication via HF token and creates git-based versioning. Manual upload requires separate steps and may miss metadata. The method also generates a model card and enables version control.",
    "source": "model_hub"
  },
  {
    "id": "ht_039",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Explain what happens when you call `model.to('cuda')` vs `model.half()` and the combined effect of using both together.",
    "ground_truth": "`model.to('cuda')` moves parameters to GPU device. `model.half()` converts float32 weights to float16 (reduces memory by ~50%). Using both together moves half-precision weights to GPU, achieving maximum memory efficiency. Requires mixed precision training to avoid numerical instability; use with automatic mixed precision (AMP).",
    "source": "device_dtype"
  },
  {
    "id": "ht_040",
    "domain": "huggingface_transformers",
    "difficulty": "medium",
    "question": "Describe how transfer learning works in Transformers by explaining what layers are typically frozen vs fine-tuned and why.",
    "ground_truth": "Transfer learning freezes early transformer layers (capturing general linguistic knowledge) and fine-tunes later layers + task-specific heads. This reduces training time and data requirements while leveraging pre-trained representations. Full fine-tuning of all layers is also common but requires more data and compute. Layer freezing strategy depends on downstream task similarity to pre-training.",
    "source": "transfer_learning"
  },
  {
    "id": "ht_041",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "Explain the performance implications of using `torch.jit.trace()` versus `torch.jit.script()` when optimizing a Hugging Face transformer model for inference, and when would you choose each approach?",
    "ground_truth": "torch.jit.trace() records tensor operations during execution but fails on dynamic control flow, making it faster but inflexible; torch.jit.script() compiles Python code directly, supporting control flow but requiring careful annotation. Use trace() for models with fixed architectures and dynamic shapes, script() for models with conditional logic or complex Python dependencies.",
    "source": "model_compilation_optimization"
  },
  {
    "id": "ht_042",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "What are the security risks when using `transformers.AutoModel.from_pretrained()` with a `trust_remote_code=True` parameter, and how should you mitigate them in production?",
    "ground_truth": "Setting `trust_remote_code=True` executes arbitrary Python code from the model repository, enabling code injection attacks. Mitigations include: sandboxing with containers, code review before setting flag, using hash verification, restricting to trusted organizations, and scanning dependencies with tools like Bandit before production deployment.",
    "source": "remote_code_execution_security"
  },
  {
    "id": "ht_043",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "How does Flash Attention integration in Hugging Face transformers reduce memory consumption and improve throughput, and what are the hardware and model constraints for using it?",
    "ground_truth": "Flash Attention uses a tiled algorithm to reduce memory I/O between GPU compute and high-bandwidth memory, lowering peak memory and increasing speed. It requires NVIDIA GPUs with SM >= 80 (A100, H100), works best with batch size > 1, and may degrade performance on smaller models or CPU inference.",
    "source": "flash_attention_optimization"
  },
  {
    "id": "ht_044",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "Describe the difference between LoRA, QLoRA, and prefix tuning in Hugging Face PEFT, including their memory footprints and when each is appropriate for fine-tuning large models.",
    "ground_truth": "LoRA adds low-rank adapters to weight matrices (~0.1% parameters); QLoRA quantizes base model to 4-bit and adds LoRA (~2-4GB for 7B models); prefix tuning prepends learnable tokens to embeddings (higher inference cost). Use LoRA for balanced efficiency, QLoRA for extreme memory constraints, prefix tuning for prompt-based tasks requiring minimal parameter overhead.",
    "source": "peft_adapter_methods"
  },
  {
    "id": "ht_045",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "What is the impact of gradient checkpointing on training dynamics and convergence, and how does it interact with mixed precision training in transformers?",
    "ground_truth": "Gradient checkpointing recomputes activations during backward pass, reducing memory by ~50% but increasing compute by ~25-33%. With mixed precision (FP16), checkpointing can amplify numerical instability due to recomputation in lower precision; use loss scaling and monitor gradient norms closely to ensure convergence isn't compromised.",
    "source": "gradient_checkpointing_memory"
  },
  {
    "id": "ht_046",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "Explain how the `token_type_ids` parameter works in BERT-like models and what issues arise when fine-tuning on sequences longer than the model's pre-training length, and how to handle them.",
    "ground_truth": "`token_type_ids` mark which tokens belong to sentence A (0) or B (1), crucial for tasks like QA and NLI. Sequences longer than max_position_embeddings fail due to missing position embeddings; solutions include: rotary embeddings, ALiBi position bias, or retraining position embeddings on longer sequences via interpolation.",
    "source": "sequence_length_extension"
  },
  {
    "id": "ht_047",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "How does the `generate()` method's beam search implementation handle repetition penalties and length normalization, and what are the numerical stability concerns with large beam widths?",
    "ground_truth": "Beam search applies repetition penalty by reducing logit scores of previously generated tokens, and length normalization by dividing log-probabilities by sequence length to favor longer sequences. Numerical instability arises with large beams due to accumulated log-probability underflow; mitigate by capping beam width (typically 4-8) and using top-k/top-p filtering to reduce candidate space.",
    "source": "beam_search_generation"
  },
  {
    "id": "ht_048",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "What is the difference between `hidden_size` and `embedding_dim` in transformer configurations, and how do bottleneck projection layers affect model capacity and training stability?",
    "ground_truth": "`hidden_size` is the main transformer dimension; `embedding_dim` is often equal but can differ in some architectures. Bottleneck projections (e.g., ALBERT) reduce parameters by sharing weights across layers but limit model capacity and can cause training instability due to gradient bottlenecks; careful learning rate tuning is essential.",
    "source": "model_architecture_configuration"
  },
  {
    "id": "ht_049",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "Explain how `attention_mask` vs `attention_bias` differ in preventing information leakage during multi-task or multi-document batch processing, and potential performance pitfalls.",
    "ground_truth": "`attention_mask` (zeros out softmax logits via multiplication) is standard but masks padding globally. `attention_bias` (adds large negative values before softmax) offers more granular control for custom masking patterns. Pitfall: incorrect broadcasting of masks across batch/sequence/head dimensions causes silent correctness failures; always verify mask shapes with gradient inspection.",
    "source": "attention_masking_strategies"
  },
  {
    "id": "ht_050",
    "domain": "huggingface_transformers",
    "difficulty": "hard",
    "question": "How does the `use_cache=True` parameter in decoder-only models optimize inference latency and memory, and what are the cache invalidation issues when using batch_size > 1 with variable-length sequences?",
    "ground_truth": "`use_cache=True` stores past key-value tensors to skip recomputation, reducing latency from O(n\u00b2) to O(n) for autoregressive generation. With variable-length sequences and batching, cache shape mismatches occur; solutions include padding to fixed length, recompiling per sequence length, or using separate cache per batch element, all with memory-latency tradeoffs.",
    "source": "kv_cache_inference_optimization"
  }
]
