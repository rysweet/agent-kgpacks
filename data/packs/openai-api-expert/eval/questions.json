[
  {
    "id": "oa_001",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the primary authentication method required to use the OpenAI API?",
    "ground_truth": "The OpenAI API uses API keys for authentication. You must include your API key in the Authorization header as a Bearer token when making requests to the API.",
    "source": "authentication"
  },
  {
    "id": "oa_002",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "Name the most commonly used OpenAI model for general-purpose text generation as of 2024.",
    "ground_truth": "GPT-4 and GPT-4 Turbo are the most advanced general-purpose models, while GPT-3.5 Turbo remains popular for cost-effective applications.",
    "source": "models"
  },
  {
    "id": "oa_003",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What parameter controls the randomness or creativity of API responses in text generation?",
    "ground_truth": "The 'temperature' parameter controls randomness, ranging from 0 to 2. Lower values (closer to 0) produce more deterministic outputs, while higher values produce more random and creative responses.",
    "source": "parameters"
  },
  {
    "id": "oa_004",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What does the 'max_tokens' parameter do in an OpenAI API request?",
    "ground_truth": "The 'max_tokens' parameter specifies the maximum number of tokens the API will generate in its response. It controls the length of the output and helps manage costs.",
    "source": "parameters"
  },
  {
    "id": "oa_005",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the purpose of the 'system' role in the chat completion API?",
    "ground_truth": "The 'system' role sets the behavior and instructions for the assistant. It provides context and guidelines that shape how the model responds throughout the conversation.",
    "source": "chat_completions"
  },
  {
    "id": "oa_006",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "Which HTTP method is used to make requests to the OpenAI chat completions endpoint?",
    "ground_truth": "POST is used to make requests to the OpenAI chat completions endpoint (/v1/chat/completions).",
    "source": "api_endpoints"
  },
  {
    "id": "oa_007",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What does the 'top_p' parameter control in text generation?",
    "ground_truth": "The 'top_p' parameter implements nucleus sampling, controlling diversity by setting a probability threshold. The model only considers tokens with cumulative probability up to top_p.",
    "source": "parameters"
  },
  {
    "id": "oa_008",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "Define what a 'token' is in the context of the OpenAI API.",
    "ground_truth": "A token is a unit of text that the API processes. Roughly, 1 token equals 4 characters or 0.75 words. Tokens are used to calculate API costs and enforce rate limits.",
    "source": "tokens"
  },
  {
    "id": "oa_009",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the 'model' parameter used for in an OpenAI API request?",
    "ground_truth": "The 'model' parameter specifies which OpenAI model to use for processing the request, such as 'gpt-4' or 'gpt-3.5-turbo'.",
    "source": "models"
  },
  {
    "id": "oa_010",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "How should you store your OpenAI API key securely?",
    "ground_truth": "Store your API key in environment variables, configuration files outside of version control, or secret management systems. Never hardcode it directly in your source code or commit it to repositories.",
    "source": "security"
  },
  {
    "id": "oa_011",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the 'messages' parameter in a chat completion request?",
    "ground_truth": "The 'messages' parameter is an array of message objects containing the conversation history. Each message has a 'role' (system, user, or assistant) and 'content' (the message text).",
    "source": "chat_completions"
  },
  {
    "id": "oa_012",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the base URL for the OpenAI API endpoints?",
    "ground_truth": "The base URL for OpenAI API endpoints is 'https://api.openai.com/v1'.",
    "source": "api_endpoints"
  },
  {
    "id": "oa_013",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What does the 'user' parameter in an API request do?",
    "ground_truth": "The 'user' parameter is an optional identifier for the end-user making the request. It helps OpenAI monitor for abuse and is useful for tracking user-specific usage.",
    "source": "parameters"
  },
  {
    "id": "oa_014",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What type of content can the Vision API process?",
    "ground_truth": "The Vision API can process images in JPEG, PNG, GIF, and WebP formats, allowing the model to analyze and describe visual content.",
    "source": "vision_api"
  },
  {
    "id": "oa_015",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the purpose of 'presence_penalty' in text generation?",
    "ground_truth": "The 'presence_penalty' parameter penalizes tokens that have already appeared in the output, encouraging the model to introduce new topics and reduce repetition.",
    "source": "parameters"
  },
  {
    "id": "oa_016",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "How are you billed when using the OpenAI API?",
    "ground_truth": "You are billed per token used, with input tokens and output tokens charged at different rates depending on the model. Costs are calculated based on actual usage.",
    "source": "billing"
  },
  {
    "id": "oa_017",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What does the 'finish_reason' field in an API response indicate?",
    "ground_truth": "The 'finish_reason' field indicates why the model stopped generating text. Common values are 'stop' (natural completion), 'length' (max_tokens reached), or 'content_filter' (safety filter triggered).",
    "source": "response_format"
  },
  {
    "id": "oa_018",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "Name one use case where you would use the Embeddings API.",
    "ground_truth": "The Embeddings API is used for semantic search, recommendation systems, clustering, or converting text into numerical vectors for similarity comparisons.",
    "source": "embeddings_api"
  },
  {
    "id": "oa_019",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "What is the 'frequency_penalty' parameter used for?",
    "ground_truth": "The 'frequency_penalty' parameter reduces the probability of tokens appearing multiple times in the output, encouraging lexical diversity.",
    "source": "parameters"
  },
  {
    "id": "oa_020",
    "domain": "openai_api_expert",
    "difficulty": "easy",
    "question": "How do you report an API issue or get support from OpenAI?",
    "ground_truth": "You can report issues through the OpenAI Help Center, community forums, or by contacting support through your OpenAI account dashboard.",
    "source": "support"
  },
  {
    "id": "oa_021",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "When using the OpenAI API with function calling, what is the purpose of the 'tools' parameter and how does it differ from the deprecated 'functions' parameter?",
    "ground_truth": "The 'tools' parameter is the current standard for providing function definitions to the API, with a more flexible structure that supports function calling through a tool interface. The deprecated 'functions' parameter was the earlier implementation; 'tools' offers better extensibility and is the recommended approach for new implementations.",
    "source": "function_calling"
  },
  {
    "id": "oa_022",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How does token counting affect billing and what is the relationship between prompt tokens, completion tokens, and total tokens in API pricing?",
    "ground_truth": "Prompt tokens are input tokens charged at a lower rate, completion tokens are output tokens charged at a higher rate, and total tokens is their sum. Different models have different per-token pricing; understanding token usage is critical for cost optimization and budget forecasting.",
    "source": "billing_and_tokens"
  },
  {
    "id": "oa_023",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "Explain the trade-offs between using temperature and top_p parameters for controlling output randomness in the OpenAI API.",
    "ground_truth": "Temperature (0-2) controls randomness directly\u2014lower values produce deterministic outputs while higher values increase variability. Top_p (nucleus sampling, 0-1) controls diversity by filtering to the smallest set of tokens with cumulative probability p. Using both together can limit model flexibility, so typically one is used at a time.",
    "source": "sampling_parameters"
  },
  {
    "id": "oa_024",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What are the key differences between the Chat Completions API and the Completions API, and when should each be used?",
    "ground_truth": "Chat Completions API uses a message-based format with roles (system, user, assistant) and is optimized for conversational interactions; it's the recommended endpoint for most use cases. The Completions API uses raw text prompts and is legacy; Chat Completions provides better performance, easier multi-turn conversations, and is the current standard.",
    "source": "api_endpoints"
  },
  {
    "id": "oa_025",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How does the 'max_tokens' parameter interact with the model's context window, and what happens if you set it to exceed remaining available tokens?",
    "ground_truth": "Max_tokens limits the completion length independent of context window size; if set too high relative to remaining tokens (context window minus prompt tokens), the API returns fewer tokens than requested. The API will never exceed the model's total context window, and attempting to do so results in truncation or an error.",
    "source": "context_management"
  },
  {
    "id": "oa_026",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What is the purpose of system prompts in the Chat Completions API, and how does their token cost compare to user messages?",
    "ground_truth": "System prompts set the behavior and personality of the assistant and are included in the messages array with role='system'. System prompt tokens are counted and billed identically to user and assistant message tokens; they are not free and contribute to both token count and cost.",
    "source": "system_prompts"
  },
  {
    "id": "oa_027",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "Explain the concept of 'logit_bias' in the OpenAI API and describe a practical use case for it.",
    "ground_truth": "Logit_bias allows you to adjust the probability of specific token IDs being selected in the output; values range from -100 to 100. A practical use case is enforcing output format compliance\u2014for example, biasing against certain punctuation to ensure consistent JSON formatting or preventing specific unwanted tokens.",
    "source": "output_control"
  },
  {
    "id": "oa_028",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What is the difference between streaming and non-streaming responses in the OpenAI API, and when would you choose streaming for production applications?",
    "ground_truth": "Non-streaming returns the complete response after processing; streaming returns tokens incrementally via server-sent events. Streaming is preferred for production when lower latency perception is important, user experience responsiveness matters (e.g., chat interfaces), or when users want to see partial results quickly.",
    "source": "streaming"
  },
  {
    "id": "oa_029",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How do you implement retry logic with exponential backoff for OpenAI API calls, and what HTTP status codes should trigger retries?",
    "ground_truth": "Implement exponential backoff by catching rate limit (429) and temporary server errors (500, 502, 503) and retrying with increasing delays. Use status codes 429 (rate limit), 500 (server error), 502 (bad gateway), and 503 (service unavailable) as triggers; avoid retrying on 401 (auth), 404 (not found), or 400 (validation) errors.",
    "source": "error_handling"
  },
  {
    "id": "oa_030",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What are embeddings in the OpenAI API, and what are the common use cases for using the embeddings endpoint?",
    "ground_truth": "Embeddings are vector representations of text generated by models like text-embedding-3-small. Common use cases include semantic search, clustering, recommendation systems, anomaly detection, and retrieving contextually similar documents for Retrieval-Augmented Generation (RAG) applications.",
    "source": "embeddings"
  },
  {
    "id": "oa_031",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "Describe how to implement prompt caching in the OpenAI API and what performance benefits it provides.",
    "ground_truth": "Prompt caching stores frequently-reused prompt components (system prompts, long context) and reuses them across requests by including cache_control parameters. Benefits include 90% cost reduction on cached tokens and faster response times by avoiding redundant processing of identical prompt sections.",
    "source": "prompt_caching"
  },
  {
    "id": "oa_032",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How does the presence_penalty and frequency_penalty configuration affect model output, and what scenarios justify using both?",
    "ground_truth": "Presence_penalty (0-2) discourages repeating any tokens that appeared in the prompt; frequency_penalty (0-2) discourages repeating tokens proportionally to their frequency in the prompt. Use both for creative writing to increase diversity, use presence_penalty alone for reducing token repetition, and frequency_penalty for encouraging variation while allowing necessary repetitions.",
    "source": "repetition_control"
  },
  {
    "id": "oa_033",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What is the purpose of the 'stop' parameter in the OpenAI API, and how does it compare to using max_tokens for controlling response length?",
    "ground_truth": "The 'stop' parameter terminates generation when specified sequences are encountered (up to 4 sequences), enabling semantic stopping. It differs from max_tokens which is a hard limit; 'stop' provides semantic control and is useful for structured outputs (e.g., stopping at newlines), while max_tokens provides hard length bounds.",
    "source": "generation_control"
  },
  {
    "id": "oa_034",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "Explain the concept of context window and how to optimize prompt structure when approaching token limits with GPT models.",
    "ground_truth": "Context window is the total token capacity of a model (e.g., 128K for gpt-4-turbo). When approaching limits, optimize by summarizing older conversation history, removing irrelevant context, prioritizing recent messages, and using prompt caching for reusable content. Consider whether a shorter context window model suffices for cost savings.",
    "source": "context_optimization"
  },
  {
    "id": "oa_035",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How should you structure a multi-turn conversation in the Chat Completions API to maintain context and avoid token waste?",
    "ground_truth": "Include all prior conversation messages in the messages array to maintain context; the API uses full history for coherence. Avoid waste by periodically summarizing old exchanges into a system prompt, using conversation pruning strategies, or implementing sliding window approaches where only recent messages are retained after a certain length.",
    "source": "conversation_management"
  },
  {
    "id": "oa_036",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What are vision capabilities in the OpenAI API, and what image formats and size constraints must you consider when submitting images?",
    "ground_truth": "Vision capabilities allow the model to analyze images via the Chat Completions API using base64 encoding or URLs. Supported formats are JPEG, PNG, GIF, and WebP; images should be under 20MB, and detailed analysis requires high resolution while thumbnails work for basic identification to optimize token usage.",
    "source": "vision_api"
  },
  {
    "id": "oa_037",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How do you handle API authentication securely in production applications, and what are the risks of storing API keys?",
    "ground_truth": "Store API keys in environment variables or secure vaults (not in code or version control), use role-based API keys if available, and rotate keys regularly. Risks include unauthorized API usage, billing fraud, and data exposure; always use HTTPS and never commit keys to repositories.",
    "source": "authentication_and_security"
  },
  {
    "id": "oa_038",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What is the difference between model availability across organizations, and how do you select the appropriate model version for your use case?",
    "ground_truth": "Different models have different capabilities, context windows, and pricing; gpt-4 variants offer superior reasoning, gpt-3.5-turbo is faster and cheaper, and specialized models like gpt-4-vision handle images. Selection depends on task complexity, latency requirements, and budget\u2014balance accuracy needs against cost and speed constraints.",
    "source": "model_selection"
  },
  {
    "id": "oa_039",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "How do you implement Retrieval-Augmented Generation (RAG) using the OpenAI API, and what are the key architectural considerations?",
    "ground_truth": "RAG involves retrieving relevant documents via embeddings search, then passing them with the user query to the Chat Completions API. Key considerations include embedding freshness, retrieval ranking quality, managing context window constraints when appending retrieved documents, deduplication, and verifying retrieved content accuracy to prevent hallucination.",
    "source": "rag_implementation"
  },
  {
    "id": "oa_040",
    "domain": "openai_api_expert",
    "difficulty": "medium",
    "question": "What are the differences between fine-tuning and prompt engineering approaches for customizing model behavior, and when should you choose each?",
    "ground_truth": "Fine-tuning trains a model on custom data (expensive, slower) and works best for learning specific styles, formats, or domain knowledge with significant data. Prompt engineering is free, immediate, and works well for instruction-following and behavioral adjustments; use prompt engineering first, resort to fine-tuning when extensive custom patterns are needed.",
    "source": "customization_strategies"
  },
  {
    "id": "oa_041",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "When using the OpenAI API with function calling, what are the performance and token implications of including verbose descriptions versus minimal parameter schemas, and how does this affect latency in streaming contexts?",
    "ground_truth": "Verbose descriptions increase prompt tokens consumed per request, raising costs and latency; minimal schemas reduce token overhead but may degrade model reasoning quality. In streaming, larger function schemas delay first token arrival. The optimal balance requires testing with your specific use case, as token savings must outweigh potential accuracy loss.",
    "source": "function_calling_optimization"
  },
  {
    "id": "oa_042",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "Explain the security implications of storing API keys in environment variables versus using Azure OpenAI managed identities, and what vulnerabilities each approach introduces in production deployments.",
    "ground_truth": "Environment variables are readable by any process with sufficient privileges, enabling lateral movement if a service is compromised. Azure managed identities eliminate key storage entirely by using RBAC and short-lived tokens, but require Azure infrastructure. Environment variables suit development; managed identities are required for production security compliance.",
    "source": "authentication_security"
  },
  {
    "id": "oa_043",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "How does the OpenAI API handle rate limiting across batch versus real-time requests, and what strategies minimize token bucket depletion when mixing both request types?",
    "ground_truth": "Batch requests use separate, higher rate limits and do not consume real-time quota, making them ideal for non-urgent volume. Real-time requests share a token bucket per organization. Mix by prioritizing batch for non-latency-sensitive work, implementing exponential backoff with jitter for real-time, and using separate API keys if rate limit isolation is critical.",
    "source": "rate_limiting_strategy"
  },
  {
    "id": "oa_044",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "What are the technical differences between fine-tuning on GPT-3.5-turbo versus using prompt engineering with in-context examples, and when does each approach yield better cost-to-accuracy ratios?",
    "ground_truth": "Fine-tuning creates a model checkpoint optimized for your data distribution, reducing context length needed and improving performance on specialized tasks; it incurs upfront training cost but lowers per-inference tokens. In-context examples consume tokens per request but avoid training latency and are flexible. Fine-tuning is cost-effective for high-volume, specialized tasks; examples suit low-volume or rapidly changing use cases.",
    "source": "fine_tuning_vs_prompting"
  },
  {
    "id": "oa_045",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "When implementing retrieval-augmented generation (RAG) with the OpenAI API, what are the trade-offs between embedding vector dimensionality, retrieval chunk size, and context window usage in the final prompt?",
    "ground_truth": "Higher embedding dimensions improve retrieval precision but increase storage and similarity computation cost. Larger chunks reduce retrieval calls but may include irrelevant context, wasting tokens in the LLM call. Optimal chunk size balances retrieval relevance with total token consumption; embedding dimensions depend on semantic complexity and infrastructure constraints.",
    "source": "rag_optimization"
  },
  {
    "id": "oa_046",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "Describe the execution model and error handling guarantees for OpenAI Batch API jobs. What happens to in-flight requests if a job is cancelled, and how should you implement idempotency to recover from failures?",
    "ground_truth": "Batch API processes requests asynchronously and returns results in JSONL files. Cancelled jobs may have already started processing requests; no strong guarantee that in-flight requests are rolled back. Implement idempotency by including a unique request ID and storing results keyed by request ID, then replay failed requests. OpenAI does not guarantee at-most-once semantics.",
    "source": "batch_api_reliability"
  },
  {
    "id": "oa_047",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "How do temperature and top_p sampling interact when both are set, and what pathological outputs can result from misconfigured values in production systems?",
    "ground_truth": "Temperature scales logits before top_p filtering; when both are used, they compound. High temperature + low top_p creates erratic, low-diversity outputs. Low temperature + high top_p wastes computation. OpenAI recommends tuning one parameter: either set temperature to 0 and adjust top_p, or keep top_p at 1 and adjust temperature. Misconfigured values cause inconsistent quality and waste tokens.",
    "source": "sampling_parameters"
  },
  {
    "id": "oa_048",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "Explain how to use the OpenAI API's moderation endpoint in a production pipeline without creating a bottleneck, and what are the false positive/negative trade-offs when tuning sensitivity?",
    "ground_truth": "Call the moderation endpoint asynchronously in parallel with other processing; cache results for identical inputs. Sensitivity cannot be tuned; categories return binary flags. False positives block legitimate content; false negatives allow harmful content. Mitigate by combining moderation with post-response filtering and user reporting. Do not rely solely on the API for content policy enforcement.",
    "source": "moderation_pipeline"
  },
  {
    "id": "oa_049",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "What are the architectural implications of using streaming responses with GPT-4 in a multi-tenant SaaS application, and how do you prevent resource exhaustion from slow or abandoned streams?",
    "ground_truth": "Streaming opens long-lived HTTP connections; in multi-tenant systems, slow clients can exhaust connection pools. Implement timeouts on stream reads, limit concurrent streams per user/org, and monitor stream abandonment rates. Use load balancing with connection draining and consider server-sent events (SSE) instead of raw HTTP streams for better connection lifecycle management.",
    "source": "streaming_architecture"
  },
  {
    "id": "oa_050",
    "domain": "openai_api_expert",
    "difficulty": "hard",
    "question": "How should you structure prompts for GPT-4's extended context window to avoid degraded performance on queries about content in the middle or end of long documents, and what is the optimal retrieval strategy?",
    "ground_truth": "GPT-4 exhibits 'lost in the middle' phenomenon where critical information in the middle of long contexts is attended to less. Mitigate by placing the most important instructions at the start and end, using summarization for bulk content, or implementing hierarchical chunking with retrieval. For long documents, RAG with embedding-based ranking outperforms dumping the entire document, even with extended windows.",
    "source": "context_window_optimization"
  }
]
