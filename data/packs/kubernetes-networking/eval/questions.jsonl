{"id": "kn_001", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is a CNI plugin and what is its primary responsibility in Kubernetes?", "ground_truth": "A CNI (Container Network Interface) plugin is responsible for allocating IP addresses to pods and managing their network connectivity. It implements the CNI specification to enable pod-to-pod communication across the cluster.", "source": "CNI_plugins"}
{"id": "kn_002", "domain": "kubernetes_networking", "difficulty": "easy", "question": "Name three popular CNI plugins used in Kubernetes clusters.", "ground_truth": "Three popular CNI plugins are Cilium (uses eBPF), Calico (uses iptables or eBPF), and Flannel (overlay-based). Other examples include Weave and Antrea.", "source": "CNI_plugins"}
{"id": "kn_003", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is the primary difference between a ClusterIP and a NodePort service?", "ground_truth": "ClusterIP exposes the service only internally within the cluster, while NodePort exposes the service on a port on every node, making it accessible from outside the cluster.", "source": "Services"}
{"id": "kn_004", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What type of service should you use to expose a Kubernetes service on an external load balancer?", "ground_truth": "You should use a LoadBalancer service type, which automatically provisions an external load balancer (if supported by your cloud provider) and routes traffic to the service.", "source": "Services"}
{"id": "kn_005", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What does CoreDNS do in a Kubernetes cluster?", "ground_truth": "CoreDNS is the DNS server in Kubernetes that resolves service names to IP addresses, enabling pods to discover and communicate with services using DNS names like service-name.namespace.svc.cluster.local.", "source": "DNS"}
{"id": "kn_006", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is an ingress controller and provide one example?", "ground_truth": "An ingress controller is a Kubernetes component that implements ingress rules by routing HTTP/HTTPS traffic from outside the cluster to services inside the cluster. Examples include NGINX Ingress Controller, Traefik, and HAProxy.", "source": "Ingress_controllers"}
{"id": "kn_007", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is the ExternalName service type used for?", "ground_truth": "ExternalName service type is used to map a Kubernetes service to an external DNS name, allowing pods to reach external services through a Kubernetes service abstraction.", "source": "Services"}
{"id": "kn_008", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What problem do EndpointSlices solve compared to the older Endpoints object?", "ground_truth": "EndpointSlices improve scalability by dividing endpoints into smaller slices rather than storing all endpoints in a single object, reducing API server load and improving performance in large clusters.", "source": "EndpointSlices"}
{"id": "kn_009", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is a NetworkPolicy and what types of traffic rules can it define?", "ground_truth": "A NetworkPolicy is a Kubernetes resource that defines how pods can communicate with each other and external endpoints. It can define both ingress rules (incoming traffic) and egress rules (outgoing traffic).", "source": "Network_policies"}
{"id": "kn_010", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is Cilium's main advantage as a CNI plugin?", "ground_truth": "Cilium uses eBPF (extended Berkeley Packet Filter) technology, which provides efficient kernel-level networking, advanced security policies, and observability features with low overhead.", "source": "CNI_plugins"}
{"id": "kn_011", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is kube-proxy and what is its primary function?", "ground_truth": "kube-proxy is a Kubernetes component that runs on every node and maintains network rules to enable communication to services. It can use iptables, IPVS, or eBPF (via eBPF-based CNI plugins) to implement these rules.", "source": "kube_proxy"}
{"id": "kn_012", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is the Gateway API and how does it differ from traditional Ingress?", "ground_truth": "The Gateway API is a more flexible and role-oriented successor to Ingress that supports multiple protocols (HTTP, HTTPS, TCP, UDP) and provides better separation of concerns between cluster operators and application developers.", "source": "Gateway_API"}
{"id": "kn_013", "domain": "kubernetes_networking", "difficulty": "easy", "question": "Name two routing rule types supported by the Gateway API.", "ground_truth": "HTTPRoute handles HTTP/HTTPS traffic routing, GRPCRoute handles gRPC traffic routing, and TLSRoute handles TLS traffic routing. Any two of these are valid examples.", "source": "Gateway_API"}
{"id": "kn_014", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is a service mesh and provide one example?", "ground_truth": "A service mesh is a dedicated infrastructure layer that handles service-to-service communication in microservices architectures. Examples include Istio (with ambient mode), Linkerd, and Cilium service mesh.", "source": "Service_mesh"}
{"id": "kn_015", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is topology-aware routing in Kubernetes?", "ground_truth": "Topology-aware routing directs traffic to endpoints that are in the same topology zone (region, zone, node) as the client, reducing latency and cross-zone bandwidth costs.", "source": "Topology_aware_routing"}
{"id": "kn_016", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is dual-stack networking in Kubernetes?", "ground_truth": "Dual-stack networking enables a Kubernetes cluster to simultaneously support both IPv4 and IPv6 addressing, allowing pods and services to communicate using either protocol.", "source": "Dual_stack_networking"}
{"id": "kn_017", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What is Istio ambient mode and what problem does it address?", "ground_truth": "Istio ambient mode is a deployment model that removes the need for sidecar proxies by using eBPF to manage networking, reducing resource overhead and simplifying pod lifecycle management.", "source": "Service_mesh"}
{"id": "kn_018", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What does a GatewayClass represent in the Gateway API?", "ground_truth": "A GatewayClass defines a template or cluster-scoped configuration for gateways, allowing cluster administrators to specify which gateway controller implementations are available in the cluster.", "source": "Gateway_API"}
{"id": "kn_019", "domain": "kubernetes_networking", "difficulty": "easy", "question": "What are the two directions of traffic that NetworkPolicy rules can control?", "ground_truth": "NetworkPolicy rules can control ingress (incoming) traffic to pods and egress (outgoing) traffic from pods, with both supporting L3 (IP) and L4 (port/protocol) level filtering.", "source": "Network_policies"}
{"id": "kn_020", "domain": "kubernetes_networking", "difficulty": "easy", "question": "How does eBPF-based kube-proxy replacement improve upon traditional iptables-based kube-proxy?", "ground_truth": "eBPF-based kube-proxy replacement runs in the kernel with lower latency and better performance than iptables, reduces memory usage, and supports more advanced features like connection tracking and observability without requiring iptables rules.", "source": "kube_proxy"}
{"id": "kn_021", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Explain the difference between how Cilium with eBPF and Calico handle packet filtering at the kernel level. What are the performance implications of each approach?", "ground_truth": "Cilium uses eBPF to implement packet filtering and forwarding directly in the kernel, bypassing iptables for better performance and lower latency. Calico relies on Linux iptables and netfilter for network policy enforcement, which has higher overhead. Cilium's eBPF approach provides lower CPU usage and faster packet processing, while Calico offers broader OS compatibility and simpler troubleshooting.", "source": "CNI_plugins_eBPF"}
{"id": "kn_022", "domain": "kubernetes_networking", "difficulty": "medium", "question": "When would you use a ClusterIP Service versus a NodePort Service? Describe a scenario where each is appropriate.", "ground_truth": "ClusterIP Services are used for internal cluster communication and are only accessible within the cluster; use this for backend microservices that don't need external access. NodePort Services expose applications on a port across all nodes and are suitable for development/testing or when you lack a LoadBalancer implementation; use when external clients need direct access without a load balancer.", "source": "Services_types"}
{"id": "kn_023", "domain": "kubernetes_networking", "difficulty": "medium", "question": "How does the EndpointSlices resource improve upon the older Endpoints API in terms of scalability? What is the key limitation it addresses?", "ground_truth": "EndpointSlices split endpoints across multiple resources (default 100 per slice) instead of storing all in a single Endpoints object, reducing object size and API server load. This addresses the scalability bottleneck where large services with thousands of pods would create monolithic Endpoints objects that were expensive to transmit and process.", "source": "EndpointSlices"}
{"id": "kn_024", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Explain how CoreDNS handles Kubernetes service discovery. What DNS records are created for a Service named 'web' in the 'production' namespace?", "ground_truth": "CoreDNS watches Kubernetes APIs and creates DNS records on-the-fly for services and pods. For a Service 'web' in 'production' namespace, CoreDNS creates: 'web.production.svc.cluster.local' (resolves to ClusterIP), 'web.production.svc' (short form), and 'web.production' (minimal form). Each resolves to the Service's ClusterIP, enabling service discovery within the cluster.", "source": "DNS_CoreDNS"}
{"id": "kn_025", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Compare NGINX, HAProxy, and Traefik ingress controllers. What are the key differences in their configuration models and use cases?", "ground_truth": "NGINX Ingress uses ConfigMaps for configuration and is widely adopted with mature ecosystem; best for standard HTTP/HTTPS routing. HAProxy offers superior load-balancing algorithms and raw performance for high-throughput scenarios. Traefik provides dynamic configuration, auto-discovery, and native Kubernetes integration with support for multiple protocols (HTTP, TCP, UDP); ideal for cloud-native and GitOps workflows.", "source": "ingress_controllers"}
{"id": "kn_026", "domain": "kubernetes_networking", "difficulty": "medium", "question": "What is the Gateway API and how does it improve upon traditional Ingress resources? Name three route types it supports.", "ground_truth": "Gateway API is a more expressive, role-oriented successor to Ingress that separates infrastructure (Gateway/GatewayClass) from application routing (HTTPRoute, GRPCRoute, etc.). It supports HTTPRoute for HTTP/HTTPS, GRPCRoute for gRPC services, and TLSRoute for TLS termination, providing better multi-tenant support and clearer separation of concerns between platform and application teams.", "source": "Gateway_API"}
{"id": "kn_027", "domain": "kubernetes_networking", "difficulty": "medium", "question": "How do network policies enforce namespace isolation? What happens to traffic between pods in different namespaces if no explicit network policy allows it?", "ground_truth": "Network policies can use 'namespaceSelector' to restrict ingress/egress between namespaces. By default (without policies), pods in different namespaces can communicate freely. When network policies are enabled and deny all by default (via an empty selector policy), inter-namespace traffic is blocked until explicitly allowed by a policy with matching namespaceSelector rules.", "source": "network_policies_isolation"}
{"id": "kn_028", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Describe the difference between L3 and L4 filtering in Kubernetes network policies. Provide an example of each.", "ground_truth": "L3 (IP/CIDR) filtering controls traffic based on source/destination IP addresses; example: 'allow pods from CIDR 10.0.0.0/24'. L4 (port/protocol) filtering controls based on port numbers and protocols (TCP, UDP); example: 'allow TCP port 8080 from specific pods'. Most Kubernetes network policies combine both: matching pods/namespaces (L3) and restricting ports/protocols (L4).", "source": "network_policies_filtering"}
{"id": "kn_029", "domain": "kubernetes_networking", "difficulty": "medium", "question": "What is Istio ambient mode and how does it differ from sidecar-based service mesh architecture?", "ground_truth": "Istio ambient mode eliminates per-pod sidecar proxies by moving mesh functionality to shared infrastructure proxies (ztunnels for L4, waypoints for L7). This reduces resource overhead (no per-pod containers), simplifies operations, and enables zero-trust security without modifying applications. Sidecar mode embeds proxies in every pod, consuming more resources but offering pod-level control.", "source": "service_mesh_istio_ambient"}
{"id": "kn_030", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Compare Linkerd and Cilium as service mesh solutions. What are their respective strengths?", "ground_truth": "Linkerd is lightweight (minimal overhead), Rust-based, and focused on simplicity with L4 and basic L7 features; ideal for small-to-medium clusters. Cilium provides deeper observability via eBPF, L7 policy enforcement, and NetworkPolicy enforcement in a single platform; better for advanced requirements and large deployments. Linkerd prioritizes ease-of-use; Cilium prioritizes capabilities.", "source": "service_mesh_comparison"}
{"id": "kn_031", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Explain topology-aware routing in Kubernetes. How does it optimize traffic patterns and what metric does it use to make decisions?", "ground_truth": "Topology-aware routing directs traffic to endpoints in the same topology zone (node, zone, region) to reduce cross-zone costs and latency. It uses the 'topology.kubernetes.io/zone' label on nodes to group endpoints and prefers local endpoints. When local endpoints are unavailable, traffic spills over to other zones; this is controlled via 'minReadySeconds' and EndpointSlice topology hints.", "source": "topology_aware_routing"}
{"id": "kn_032", "domain": "kubernetes_networking", "difficulty": "medium", "question": "What is dual-stack networking in Kubernetes? How are IPv4 and IPv6 addresses allocated to Pods and Services?", "ground_truth": "Dual-stack networking allows pods and services to use both IPv4 and IPv6 simultaneously. Pods receive both IPv4 and IPv6 addresses from the CNI plugin during network setup. Services can have dual-stack IPs (both IPv4 and IPv6 ClusterIPs) controlled by 'ipFamilies' and 'ipFamilyPolicy' fields; 'PreferDualStack' allocates both, 'SingleStack' allocates only the primary family.", "source": "dual_stack_networking"}
{"id": "kn_033", "domain": "kubernetes_networking", "difficulty": "medium", "question": "How does kube-proxy replacement with eBPF improve upon traditional kube-proxy? What limitations does it address?", "ground_truth": "eBPF-based kube-proxy replacement (used by Cilium) implements Service load balancing directly in the kernel, bypassing userspace kube-proxy overhead. This reduces latency, CPU usage, and memory consumption while improving scalability. Traditional kube-proxy suffers from connection tracking overhead and iptables rule scaling issues; eBPF eliminates these bottlenecks.", "source": "kube_proxy_eBPF_replacement"}
{"id": "kn_034", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Describe how you would configure a network policy to block all egress traffic from a namespace except to DNS (port 53) and external APIs. What resources are needed?", "ground_truth": "Create a default-deny egress NetworkPolicy, then add specific allow rules with 'to' selectors for DNS (port 53/UDP to kube-dns) and external APIs (via 'cidrSelector' for allowed external IPs). Example: match pods in namespace, set 'policyTypes: [Egress]', add rules allowing port 53 to CoreDNS pod and port 443/80 to external CIDR. Requires both default-deny and explicit allow policies.", "source": "network_policies_egress_rules"}
{"id": "kn_035", "domain": "kubernetes_networking", "difficulty": "medium", "question": "When would you use a LoadBalancer Service type instead of an Ingress? What are the trade-offs?", "ground_truth": "LoadBalancer Services are appropriate for non-HTTP protocols (TCP, UDP), multiple ports, or scenarios where you need direct L4 load balancing without HTTP-level routing. Ingress is better for HTTP/HTTPS with path/hostname-based routing and multiple backends. LoadBalancer creates a cloud provider load balancer (higher cost), while Ingress uses a shared ingress controller; LoadBalancer is simpler but less resource-efficient.", "source": "Services_LoadBalancer"}
{"id": "kn_036", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Explain the concept of GatewayClass in the Gateway API. How does it relate to GatewayClass conformance profiles?", "ground_truth": "GatewayClass is a cluster-scoped resource that defines a set of Gateways with common configuration and controller behavior, similar to StorageClass. Conformance profiles (Core, Extended) define which Gateway API features a GatewayClass must support; Core includes basic HTTP routing, Extended adds advanced features like weighted routing and header manipulation. Controllers declare their supported features via GatewayClass.", "source": "Gateway_API_GatewayClass"}
{"id": "kn_037", "domain": "kubernetes_networking", "difficulty": "medium", "question": "How does Flannel differ from Cilium and Calico in terms of features and use cases? When would you choose Flannel?", "ground_truth": "Flannel is a lightweight, simple overlay network CNI that focuses on basic pod-to-pod connectivity without advanced features like network policies or service mesh. Use Flannel for simple clusters, edge environments, or minimal resource footprint requirements. Unlike Cilium (eBPF, service mesh) and Calico (network policies, BGP), Flannel trades capabilities for simplicity and low overhead.", "source": "CNI_plugins_Flannel"}
{"id": "kn_038", "domain": "kubernetes_networking", "difficulty": "medium", "question": "What is the purpose of ExternalName Services in Kubernetes? Provide a real-world use case.", "ground_truth": "ExternalName Services create a DNS CNAME record pointing to an external service, allowing cluster pods to reference external services using Kubernetes DNS without exposing them via ClusterIP or NodePort. Use case: migrating a legacy database to an external managed service (e.g., RDS); applications reference 'database.default.svc.cluster.local' which maps to 'db.example.com', enabling gradual migration without code changes.", "source": "Services_ExternalName"}
{"id": "kn_039", "domain": "kubernetes_networking", "difficulty": "medium", "question": "How do ingress controllers determine which backend pod to route traffic to? Explain the role of EndpointSlices in this process.", "ground_truth": "Ingress controllers watch Ingress, Service, and EndpointSlice resources. When traffic arrives matching an Ingress rule, the controller queries the Service's EndpointSlices to get the list of healthy pod IPs, then load-balances traffic across them. EndpointSlices provide efficient, scalable endpoint discovery; the controller dynamically updates routes as pods are added/removed.", "source": "ingress_controllers_EndpointSlices"}
{"id": "kn_040", "domain": "kubernetes_networking", "difficulty": "medium", "question": "Describe a scenario where you would need to combine network policies with a service mesh. What does each layer enforce?", "ground_truth": "Combine when you need both zero-trust networking and application-level traffic management. Network policies enforce L3/L4 rules at the kernel level (who can talk to whom based on IPs/ports), while service mesh (Istio, Cilium) enforces L7 rules (request routing, retries, authentication). Example: network policy blocks all ingress, service mesh routes allowed traffic with rate limiting and authentication.", "source": "network_policies_service_mesh_integration"}
{"id": "kn_041", "domain": "kubernetes_networking", "difficulty": "hard", "question": "When deploying Cilium with eBPF in kube-proxy replacement mode, what are the critical prerequisites for the kernel version, and what specific networking features become unavailable if you attempt to run it on an older kernel without native eBPF support?", "ground_truth": "Cilium requires Linux kernel 5.8+ for full eBPF kube-proxy replacement with XDP capabilities. On older kernels, you cannot use socket-level load balancing, connection tracking optimizations, and policy enforcement via eBPF programs; you must fall back to legacy iptables-based kube-proxy, losing the performance gains and detailed visibility that eBPF provides.", "source": "Cilium_eBPF_kube-proxy_replacement"}
{"id": "kn_042", "domain": "kubernetes_networking", "difficulty": "hard", "question": "Explain the critical difference between EndpointSlices and traditional Endpoints in terms of scalability, and describe a scenario where this difference causes performance degradation if not properly managed.", "ground_truth": "EndpointSlices split endpoints across multiple resources (max 100 per slice) to avoid single large etcd objects, preventing apiserver bottlenecks. Traditional Endpoints store all endpoints in one object; at 5000+ pods per service, a single Endpoints object becomes massive, causing slow watch updates and increased etcd load. Controllers must batch-watch many small EndpointSlices instead of one large object.", "source": "EndpointSlices_scalability"}
{"id": "kn_043", "domain": "kubernetes_networking", "difficulty": "hard", "question": "When implementing dual-stack networking (IPv4/IPv6) in Kubernetes with a CNI plugin like Calico, what are the primary failure modes if the cluster's service CIDR ranges are not properly configured for both address families, and how does this affect DNS resolution?", "ground_truth": "If IPv6 service CIDR is missing or misconfigured, CoreDNS will only register A records (IPv4) for services and fail to create AAAA records (IPv6). This causes IPv6-only or dual-stack clients to experience connection timeouts when resolving services. Additionally, ExternalName services and cross-cluster communication may fail if the secondary CIDR allocation is exhausted or overlaps with existing routes.", "source": "dual-stack_networking_configuration"}
{"id": "kn_044", "domain": "kubernetes_networking", "difficulty": "hard", "question": "In Istio ambient mode, explain how the ztunnel component handles encrypted mTLS traffic between workloads, and what specific performance or security trade-offs arise compared to sidecar-based service mesh implementations.", "ground_truth": "Ztunnel uses kernel-level interception via eBPF and manages encryption at the node boundary without sidecar proxies, reducing per-pod overhead. Trade-offs include: reduced granularity for per-pod policies (enforcement at node level), potential increased latency from centralized tunnel processing, and dependency on eBPF-capable kernels. Benefits include lower memory/CPU footprint and transparent mTLS without application changes.", "source": "Istio_ambient_mode_architecture"}
{"id": "kn_045", "domain": "kubernetes_networking", "difficulty": "hard", "question": "Describe how Network Policies with both ingress and egress rules interact in Kubernetes, and explain the default-deny behavior if a pod has only an ingress rule but no egress rule defined.", "ground_truth": "Network Policies use AND logic: if a pod has any policy, both ingress AND egress default to deny unless explicitly allowed. If only an ingress rule exists, egress traffic is blocked by default, causing outbound connections (DNS, external APIs) to fail. An egress rule allowing traffic to port 53 (CoreDNS) and desired destinations must be explicitly added; otherwise, the pod is effectively isolated for outbound traffic.", "source": "network_policies_default_deny"}
{"id": "kn_046", "domain": "kubernetes_networking", "difficulty": "hard", "question": "When using topology-aware routing with topologyKeys in a Service, what happens if a zone becomes unavailable and no endpoints exist in the preferred topology, and how does this differ from the behavior of strict topology enforcement versus permissive fallback?", "ground_truth": "In permissive mode (default), traffic fails over to the next topology zone if preferred zones have no endpoints, preserving availability. In strict mode, the service returns no endpoints, causing connection failures. If the topologyKeys hint is zone-aware and a zone fails, clients in that zone must be manually rebalanced or the service must support multiple topology hints; strict topology creates outages requiring manual intervention.", "source": "topology-aware_routing_failover"}
{"id": "kn_047", "domain": "kubernetes_networking", "difficulty": "hard", "question": "Explain the interaction between Ingress controller rate limiting, service mesh circuit breaking, and kube-proxy connection limits in a multi-layer architecture. What specific issues arise if these are not coordinated?", "ground_truth": "Rate limiting at Ingress (NGINX/HAProxy) happens before the request reaches the service mesh; if circuit breaking is too aggressive downstream, legitimate requests may be rejected silently while Ingress still allows them, causing client timeouts. Uncoordinated kube-proxy connection limits (via conntrack settings) can exhaust source ports before circuit breakers trigger, resulting in asymmetric failures where some clients succeed while others fail based on port exhaustion, not actual service health.", "source": "multi-layer_rate_limiting_coordination"}
{"id": "kn_048", "domain": "kubernetes_networking", "difficulty": "hard", "question": "In the Gateway API, describe how GatewayClass conformance profiles affect the behavior of HTTPRoute, GRPCRoute, and TLSRoute, and what happens when you attempt to use a feature unsupported by your chosen conformance level.", "ground_truth": "GatewayClass defines conformance levels (Core, Extended, Implementation-specific) that specify which Route types and filters are guaranteed to work. Unsupported features may be silently ignored or cause the route to fail with status conditions indicating non-conformance. For example, a Core profile may not support path rewrites, causing GRPCRoute requests to fail if path rewriting is required; the Route status will indicate 'Programmed: False' with a conformance reason.", "source": "Gateway_API_conformance_profiles"}
{"id": "kn_049", "domain": "kubernetes_networking", "difficulty": "hard", "question": "When Flannel is used as the CNI plugin in an environment with frequent pod churn (rapid creation/deletion), what specific routing table bloat and performance issues can occur, and how does this differ from Cilium's handling of the same scenario?", "ground_truth": "Flannel's vxlan backend creates persistent routes for each node; with rapid pod churn, routes accumulate in the kernel routing table, increasing lookup latency. Cilium uses eBPF maps instead of kernel routes, which are automatically cleaned up when entries expire, preventing bloat. At scale (1000s of pods/node), Flannel's routing table can exceed thousands of entries, while Cilium maintains fixed eBPF map sizes with LRU eviction.", "source": "Flannel_routing_table_bloat"}
{"id": "kn_050", "domain": "kubernetes_networking", "difficulty": "hard", "question": "Describe a scenario where a LoadBalancer service's external IP is assigned but traffic fails to reach the backing pods, and explain how you would systematically debug this across the CNI, network policies, and service mesh layers.", "ground_truth": "Likely causes: (1) Network policy blocking ingress on the service port (check namespace selectors and L4 rules), (2) CNI overlay network routing broken (verify node-to-node connectivity with tcpdump), (3) service mesh sidecar rejecting traffic without mTLS (check Istio PeerAuthentication), (4) LoadBalancer endpoint address type mismatch (IPv4 vs IPv6). Debug by: checking NetworkPolicy logs, testing direct node SSH, verifying EndpointSlices population, inspecting kube-proxy rules (iptables-save), and examining service mesh proxy logs (istioctl proxy-config).", "source": "LoadBalancer_debugging_systematic"}
