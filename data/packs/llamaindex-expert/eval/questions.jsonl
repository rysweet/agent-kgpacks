{"id": "le_001", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What are the two fundamental data primitives in LlamaIndex?", "ground_truth": "Documents and Nodes are the two fundamental data primitives in LlamaIndex. Documents represent raw data sources, while Nodes are the processed, atomic units of data used for indexing and retrieval.", "source": "data_primitives"}
{"id": "le_002", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is the primary purpose of VectorStoreIndex in LlamaIndex?", "ground_truth": "VectorStoreIndex is designed to create embeddings of document nodes and store them in a vector database, enabling similarity-based retrieval for RAG applications.", "source": "indexing_strategies"}
{"id": "le_003", "domain": "llamaindex_expert", "difficulty": "easy", "question": "Name three other index types available in LlamaIndex besides VectorStoreIndex.", "ground_truth": "Three other index types are SummaryIndex, TreeIndex, and KeywordTableIndex, each optimized for different retrieval and query patterns.", "source": "indexing_strategies"}
{"id": "le_004", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What role do embeddings play in LlamaIndex RAG applications?", "ground_truth": "Embeddings convert text into numerical vectors that enable semantic similarity matching, allowing the system to retrieve contextually relevant documents based on query meaning rather than keyword matching.", "source": "embeddings"}
{"id": "le_005", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is a query engine in LlamaIndex?", "ground_truth": "A query engine is a component that processes user queries against an index and retrieves relevant information, typically combining retrieval and response synthesis to produce answers.", "source": "query_engines"}
{"id": "le_006", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is the main function of a node parser in LlamaIndex?", "ground_truth": "A node parser transforms raw documents into structured nodes by breaking them down into manageable chunks while preserving metadata and relationships between pieces of text.", "source": "node_parsers"}
{"id": "le_007", "domain": "llamaindex_expert", "difficulty": "easy", "question": "How do text splitters contribute to LlamaIndex document processing?", "ground_truth": "Text splitters divide large documents into smaller, semantically meaningful chunks that fit within token limits and improve retrieval efficiency for RAG systems.", "source": "text_splitters"}
{"id": "le_008", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is the purpose of a response synthesizer in LlamaIndex?", "ground_truth": "A response synthesizer combines retrieved context with an LLM to generate coherent, contextually relevant answers to user queries in RAG applications.", "source": "response_synthesizers"}
{"id": "le_009", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is a retriever in LlamaIndex?", "ground_truth": "A retriever is a component that fetches relevant nodes or documents from an index based on a query, serving as the information retrieval stage in RAG pipelines.", "source": "retrievers"}
{"id": "le_010", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What are Workflows in LlamaIndex?", "ground_truth": "Workflows are event-driven, step-based execution frameworks that enable building complex multi-step AI applications with orchestrated task sequences and state management.", "source": "workflows"}
{"id": "le_011", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is AgentWorkflow used for?", "ground_truth": "AgentWorkflow is used for multi-agent orchestration and end-to-end automation, enabling task handoffs and coordination between multiple AI agents to solve complex problems.", "source": "agent_workflows"}
{"id": "le_012", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What does LlamaParse provide to LlamaIndex users?", "ground_truth": "LlamaParse is a document parsing service that handles complex PDFs and unstructured documents, converting them into AI-ready structured data for better RAG and agent performance.", "source": "llamaparse"}
{"id": "le_013", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is Agentic Document Workflows (ADW) in LlamaIndex?", "ground_truth": "Agentic Document Workflows combine intelligent agents with document processing to automate complex document-centric tasks like extraction, classification, and analysis at scale.", "source": "agentic_document_workflows"}
{"id": "le_014", "domain": "llamaindex_expert", "difficulty": "easy", "question": "How many integration packages does LlamaIndex offer?", "ground_truth": "LlamaIndex offers 300+ integration packages, enabling seamless connection with various data sources, vector databases, LLMs, and other third-party services.", "source": "integrations"}
{"id": "le_015", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is LlamaCloud and how does it relate to LlamaIndex?", "ground_truth": "LlamaCloud is a hosted platform that provides managed services for LlamaIndex components, including LlamaParse and other production-ready tools for scaling RAG and agent applications.", "source": "llamacloud"}
{"id": "le_016", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What production optimization techniques does LlamaIndex support?", "ground_truth": "LlamaIndex supports chunking strategies optimization, retrieval tuning, and observability monitoring to improve RAG system performance and reliability in production environments.", "source": "production_optimization"}
{"id": "le_017", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What are evaluation modules in LlamaIndex?", "ground_truth": "Evaluation modules provide tools and frameworks to assess RAG and agent system performance, measuring retrieval quality, response accuracy, and overall application effectiveness.", "source": "evaluation_modules"}
{"id": "le_018", "domain": "llamaindex_expert", "difficulty": "easy", "question": "How does LlamaIndex define RAG?", "ground_truth": "RAG (Retrieval-Augmented Generation) is a technique that retrieves relevant documents from a knowledge base and uses them as context to generate more accurate and grounded LLM responses.", "source": "rag_introduction"}
{"id": "le_019", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is the main advantage of using TreeIndex in LlamaIndex?", "ground_truth": "TreeIndex organizes nodes in a hierarchical tree structure, enabling efficient traversal and reasoning over document hierarchies for complex retrieval scenarios.", "source": "indexing_strategies"}
{"id": "le_020", "domain": "llamaindex_expert", "difficulty": "easy", "question": "What is the KeywordTableIndex designed to do?", "ground_truth": "KeywordTableIndex creates a keyword-based mapping to nodes, enabling fast keyword and boolean query matching for applications requiring keyword-driven retrieval.", "source": "indexing_strategies"}
{"id": "le_021", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What is the primary difference between VectorStoreIndex and SummaryIndex in terms of retrieval strategy and when would you choose one over the other?", "ground_truth": "VectorStoreIndex uses semantic similarity search on embeddings for dense retrieval, while SummaryIndex stores the entire document/node and uses LLM-based retrieval via summarization. Choose VectorStoreIndex for keyword/semantic relevance, and SummaryIndex when you need full document context or have unstructured data without clear semantic boundaries.", "source": "indexing_strategies"}
{"id": "le_022", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How do node parsers and text splitters work together in LlamaIndex's data pipeline, and what problems do different chunking strategies solve?", "ground_truth": "Node parsers orchestrate the splitting process while text splitters handle the actual segmentation logic. Different strategies (fixed-size, semantic, hierarchical) solve different problems: fixed-size is simple and predictable, semantic chunking preserves meaning and improves retrieval quality, and hierarchical chunking maintains document structure for better context.", "source": "node_parsers_text_splitters"}
{"id": "le_023", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What is the role of a response synthesizer in a RAG pipeline, and how does it differ from simply returning retrieved documents to the LLM?", "ground_truth": "A response synthesizer post-processes retrieved nodes and generates a cohesive answer by combining retrieval results with LLM inference. It differs from raw document return by refining, summarizing, and synthesizing information across multiple retrieved nodes into a single coherent response tailored to the query.", "source": "response_synthesizers"}
{"id": "le_024", "domain": "llamaindex_expert", "difficulty": "medium", "question": "Explain the concept of task handoffs in AgentWorkflow and how they enable multi-agent orchestration.", "ground_truth": "Task handoffs allow agents to pass responsibility for subtasks to other specialized agents based on task type or complexity. This enables multi-agent orchestration by decomposing complex problems into domain-specific workflows where each agent has clear ownership and can delegate work transparently.", "source": "agent_workflows"}
{"id": "le_025", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What are the key differences between Workflows (event-driven) and AgentWorkflow (multi-agent orchestration) in LlamaIndex?", "ground_truth": "Workflows use event-driven step-based execution with flexible, reactive task flow triggered by events. AgentWorkflow is purpose-built for multi-agent systems with structured task handoffs and agent specialization. Workflows offer more granular control; AgentWorkflow provides higher-level orchestration abstractions.", "source": "workflows_comparison"}
{"id": "le_026", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How does LlamaParse improve upon traditional document parsing for complex PDFs, and what is its relationship to LlamaCloud?", "ground_truth": "LlamaParse uses advanced OCR and layout understanding for complex PDFs with tables, forms, and mixed content, producing AI-ready structured data. LlamaCloud is the managed service platform that hosts and scales LlamaParse, providing 10k free monthly credits and enterprise-grade document processing at scale.", "source": "llamaparse_llamacloud"}
{"id": "le_027", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What is the primary purpose of embedding models in LlamaIndex's retrieval pipeline, and how do embedding dimensions affect search quality and performance?", "ground_truth": "Embedding models convert text to dense vectors for semantic similarity search in VectorStoreIndex. Higher dimensions capture more semantic nuance and improve retrieval quality but increase computational cost and memory usage; lower dimensions are faster but may lose semantic precision. The optimal dimension depends on data complexity and latency requirements.", "source": "embeddings"}
{"id": "le_028", "domain": "llamaindex_expert", "difficulty": "medium", "question": "In what scenarios would you use KeywordTableIndex instead of VectorStoreIndex, and what are its trade-offs?", "ground_truth": "KeywordTableIndex uses keyword extraction for retrieval without embeddings, making it suitable for exact keyword matching, low-latency scenarios, and when embedding models are unavailable. Trade-off: simpler and faster but lacks semantic understanding, so it's better for structured queries than semantic relevance tasks.", "source": "indexing_strategies"}
{"id": "le_029", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How do retrieval tuning strategies in LlamaIndex improve RAG performance, and what metrics should you monitor?", "ground_truth": "Retrieval tuning optimizes chunk size, embedding model, similarity thresholds, and top-k settings. Monitor precision (relevance of retrieved docs), recall (proportion of relevant docs retrieved), and end-to-end metrics like answer correctness and latency to balance quality and performance.", "source": "production_optimization"}
{"id": "le_030", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What role do evaluation modules play in a production LlamaIndex system, and how do they support continuous improvement?", "ground_truth": "Evaluation modules measure RAG quality across metrics like retrieval accuracy, answer relevance, and factuality. They support continuous improvement by identifying failure modes, validating chunking strategies, comparing embedding models, and providing feedback loops for iterative optimization.", "source": "evaluation_modules"}
{"id": "le_031", "domain": "llamaindex_expert", "difficulty": "medium", "question": "Describe the relationship between Documents, Nodes, and Indexes in LlamaIndex's data model, and why this abstraction matters.", "ground_truth": "Documents are raw data sources that are parsed into Nodes (atomic data primitives with metadata). Indexes organize and index Nodes for efficient retrieval. This abstraction allows flexible transformations: the same document can generate multiple node types, and indexes can work with any node representation for different query patterns.", "source": "data_primitives"}
{"id": "le_032", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How do Agentic Document Workflows (ADW) extend beyond basic RAG, and what specific use cases do they enable?", "ground_truth": "ADW combines agents, workflows, and document parsing to automate multi-step document analysis tasks like classification, extraction, summarization, and cross-document reasoning. Use cases include document triage, contract analysis, research synthesis, and compliance checking where simple retrieval is insufficient.", "source": "agentic_document_workflows"}
{"id": "le_033", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What is the significance of the 300+ integration packages in LlamaIndex, and how do they extend the framework's capabilities?", "ground_truth": "Integration packages extend LlamaIndex to work with external services (vector databases, LLM providers, observability tools, data sources). They abstract away vendor-specific details, allowing developers to swap implementations without rewriting core logic, enabling flexibility and vendor independence.", "source": "integrations"}
{"id": "le_034", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How does observability in production LlamaIndex systems help diagnose RAG failures, and what should you instrument?", "ground_truth": "Observability captures execution traces, retrieval logs, LLM calls, and latency metrics across the RAG pipeline. Instrument document ingestion, retrieval (queries/results), embedding quality, LLM responses, and response synthesis to identify bottlenecks like poor retrieval, irrelevant documents, or LLM hallucination.", "source": "production_optimization"}
{"id": "le_035", "domain": "llamaindex_expert", "difficulty": "medium", "question": "Explain how TreeIndex organizes nodes hierarchically and when its retrieve strategy is superior to flat vector search.", "ground_truth": "TreeIndex builds a hierarchical tree structure where parent nodes summarize child nodes, enabling top-down traversal. It's superior for large datasets where you want to search coarse-to-fine (summary \u2192 detail), reducing embedding overhead and improving retrieval efficiency over massive flat indexes.", "source": "indexing_strategies"}
{"id": "le_036", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How do query engines in LlamaIndex abstract the complexity of the retrieval and synthesis pipeline?", "ground_truth": "Query engines provide a unified interface that orchestrates retrieval, reranking, and synthesis behind the scenes. Users call a single query() method which internally handles node retrieval, LLM invocation, and response generation, hiding pipeline complexity and supporting different index types transparently.", "source": "query_engines"}
{"id": "le_037", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What chunking strategies are most effective for different document types (e.g., research papers vs. legal contracts), and why?", "ground_truth": "Research papers benefit from semantic chunking (preserving paragraph structure), legal contracts need fixed-size or clause-based chunking for precision, and tables require special handling (extraction or image-based). The strategy depends on document semantics and downstream task requirements (e.g., QA vs. compliance checking).", "source": "production_optimization"}
{"id": "le_038", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How does the Agentic Retrieval Guide's approach differ from naive RAG, and what advanced techniques does it introduce?", "ground_truth": "Agentic Retrieval adds agent decision-making to RAG: agents decide when to retrieve, what queries to run, how to synthesize results, and whether more context is needed. Advanced techniques include iterative refinement, multi-query synthesis, and dynamic retrieval based on intermediate results rather than static retrieval-then-answer.", "source": "agentic_retrieval"}
{"id": "le_039", "domain": "llamaindex_expert", "difficulty": "medium", "question": "What configuration decisions are critical when setting up a LlamaIndex system for production, and how do they trade off quality, latency, and cost?", "ground_truth": "Critical decisions include embedding model (quality vs. latency), chunk size (coverage vs. precision), index type (latency vs. accuracy), vector database (scalability vs. cost), and LLM model (quality vs. cost). Each decision creates trade-offs; optimal configuration depends on SLA requirements and data characteristics.", "source": "production_optimization"}
{"id": "le_040", "domain": "llamaindex_expert", "difficulty": "medium", "question": "How do retrievers in LlamaIndex enable pluggable retrieval strategies, and what advantages does this design provide?", "ground_truth": "Retrievers abstract the retrieval mechanism (vector, keyword, hybrid, LLM-based) behind a common interface, allowing swapping implementations without changing query code. This enables experimentation, A/B testing, and multi-retriever fusion strategies while maintaining consistent application logic.", "source": "retrievers"}
{"id": "le_041", "domain": "llamaindex_expert", "difficulty": "hard", "question": "When implementing a multi-agent orchestration system using AgentWorkflow, what are the key architectural considerations for handling task handoffs between agents, and how does the event-driven execution model prevent race conditions in concurrent agent operations?", "ground_truth": "AgentWorkflow uses event-driven, step-based execution where tasks are explicitly handed off between agents through defined state transitions. Race conditions are prevented by the sequential processing of events in the workflow engine, which maintains a deterministic execution order. Task handoffs are managed through explicit event emission and handler registration, ensuring only one agent processes a task at any given time.", "source": "AgentWorkflow multi-agent orchestration"}
{"id": "le_042", "domain": "llamaindex_expert", "difficulty": "hard", "question": "In Agentic Document Workflows (ADW), explain how the combination of LlamaParse and intelligent node chunking strategies impacts retrieval performance and why naive RAG chunking approaches fail for complex, multi-modal documents.", "ground_truth": "LlamaParse uses AI-powered OCR to extract structured data from complex PDFs while preserving layout and multi-modal elements (tables, images, forms). ADW then applies intelligent chunking strategies that respect semantic boundaries and document structure, rather than naive character/token-based splitting. This prevents splitting table rows or breaking image-text relationships, which would cause retrieval failures and hallucinations in naive RAG systems that don't preserve context.", "source": "Agentic Document Workflows, LlamaParse"}
{"id": "le_043", "domain": "llamaindex_expert", "difficulty": "hard", "question": "When designing a retrieval system using both VectorStoreIndex and KeywordTableIndex together, what are the performance tradeoffs and when should you prefer a hybrid retrieval strategy over a single index type?", "ground_truth": "VectorStoreIndex provides semantic similarity retrieval with high recall but requires embeddings and approximate nearest neighbor search. KeywordTableIndex enables exact keyword matching with minimal computational overhead but lacks semantic understanding. A hybrid strategy is preferred for production systems: use KeywordTableIndex for precise lookups and document filtering, then use VectorStoreIndex for semantic re-ranking. This reduces embedding costs while maintaining recall, particularly effective when keywords are highly predictable or regulatory compliance requires exact-match auditability.", "source": "Indexing strategies, query engines"}
{"id": "le_044", "domain": "llamaindex_expert", "difficulty": "hard", "question": "Explain how node parsers and text splitters interact with the Nodes data primitive, and what happens to metadata and relationships when documents are split across multiple nodes using different splitting strategies.", "ground_truth": "Node parsers transform Documents into Nodes, where text splitters determine chunk boundaries. Text splitters can use character-based, token-based, or semantic-based strategies. When documents are split, metadata is inherited by all child nodes by default, but relationships between nodes (parent-child, sibling) are explicitly tracked through node metadata. Different splitting strategies affect retrieval quality: semantic splitters preserve meaning but are computationally expensive, while token-based splitters are fast but may fragment concepts. Metadata propagation must be carefully managed to maintain context in downstream retrieval and response synthesis.", "source": "Documents and Nodes, node parsers, text splitters"}
{"id": "le_045", "domain": "llamaindex_expert", "difficulty": "hard", "question": "In production RAG systems using LlamaCloud, what observability metrics should be monitored to detect retrieval degradation, and how would you implement A/B testing for different retrieval tuning configurations without impacting end users?", "ground_truth": "Key observability metrics include: retrieval precision/recall (comparing retrieved documents to ground truth), latency per retrieval step, embedding model performance drift, and response relevance scores from evaluation modules. For safe A/B testing, implement a shadow traffic pattern where alternative configurations process queries in parallel without affecting served responses, logging performance metrics separately. Use LlamaCloud's integration capabilities to route a percentage of traffic to different retriever configurations with feature flags, enabling rollback without downtime. Track statistical significance over time to determine winner configuration.", "source": "Production optimization, observability, LlamaCloud"}
{"id": "le_046", "domain": "llamaindex_expert", "difficulty": "hard", "question": "When using TreeIndex for hierarchical document organization, how does the tree traversal strategy during querying affect both latency and result quality, and what are the failure modes when tree depth exceeds optimal thresholds?", "ground_truth": "TreeIndex organizes documents hierarchically, with query execution traversing from root to leaf nodes. Latency increases logarithmically with depth under optimal conditions, but excessive depth causes exponential query time degradation due to repeated LLM calls at each level deciding which branch to traverse. Quality degrades when tree depth is too deep because intermediate summarization at each level loses granular information, reducing retrieval precision. Optimal depth is typically 3-5 levels; beyond that, failures manifest as queries missing relevant documents that were pruned by inaccurate intermediate summaries, or timeout errors from too many LLM invocations.", "source": "Indexing strategies, TreeIndex"}
{"id": "le_047", "domain": "llamaindex_expert", "difficulty": "hard", "question": "How does the response synthesizer component in LlamaIndex handle conflicting information across multiple retrieved documents, and what strategies can mitigate hallucinations when source documents contain contradictory content?", "ground_truth": "The response synthesizer receives multiple retrieved documents and uses either compact summarization (condensing documents first) or refine strategy (iteratively building response across documents). When documents conflict, the synthesizer has no built-in conflict resolution\u2014it depends on the LLM's behavior and prompt engineering. Mitigation strategies include: explicitly instructing the LLM to cite sources and flag contradictions, implementing source verification steps in the query engine, using retrieval-augmented evaluation to identify conflicting documents before synthesis, and configuring the synthesizer to return multiple competing answers with confidence scores rather than forcing consensus.", "source": "Response synthesizers, query engines"}
{"id": "le_048", "domain": "llamaindex_expert", "difficulty": "hard", "question": "Describe a scenario where the 300+ integration packages in LlamaIndex would require custom implementation, and explain how to extend the framework's vector store or embedding abstraction for a proprietary system.", "ground_truth": "Custom implementation is needed when using proprietary databases (custom vector stores with non-standard APIs), specialized embedding models (domain-specific or fine-tuned models), or when integration packages lack critical features (e.g., vector store doesn't support metadata filtering or hybrid search). To extend, implement the BaseVectorStore abstract class defining methods: add(), delete(), query() with filtering, and metadata_filters(). For embeddings, subclass BaseEmbedding implementing embed() and get_text_embedding_batch(). Register custom implementations via LlamaIndex's dependency injection system. This allows seamless integration with existing RAG pipelines while maintaining type safety and interface compatibility.", "source": "Integration packages, extensibility"}
{"id": "le_049", "domain": "llamaindex_expert", "difficulty": "hard", "question": "In an Agentic Retrieval system, explain how to design the agent's decision logic to avoid infinite retrieval loops when the initial query returns insufficient results, and what safeguards prevent resource exhaustion.", "ground_truth": "Agents can enter infinite loops if the retrieval quality is poor and the agent keeps re-querying with similar parameters. Prevention mechanisms include: (1) explicit attempt counters with maximum limits (typically 3-5 retries), (2) query reformulation strategies that diversify search terms rather than repeating identical queries, (3) confidence thresholds on relevance scores that trigger different strategies rather than more retrieval, (4) circuit breakers that escalate to fallback responses after failed retries. Resource safeguards include timeouts on individual retrieval operations, token limits on agent reasoning steps, and rate limiting on vector store queries. Monitoring should track retrieval attempt patterns to identify and block pathological behaviors.", "source": "Agentic Retrieval Guide, Agent Workflows"}
{"id": "le_050", "domain": "llamaindex_expert", "difficulty": "hard", "question": "When implementing production RAG systems with LlamaIndex evaluation modules, what are the critical differences between offline evaluation (pre-deployment) and online evaluation (production monitoring), and how would you establish ground truth for domain-specific query-document relevance?", "ground_truth": "Offline evaluation uses static test datasets with manual relevance judgments or synthetic queries to measure baseline performance before deployment. Metrics include precision@k, NDCG, and MRR computed against labeled ground truth. Online evaluation monitors production queries in real-time using implicit signals (user dwell time, result clicks, feedback submissions) and explicit metrics (user ratings). Ground truth establishment for domain-specific relevance requires: subject matter expert annotation of representative query-document pairs (typically 200-500 examples), inter-annotator agreement measurement (Cohen's kappa > 0.7), and periodic re-annotation as domain evolves. Hybrid approaches use online feedback to continuously update offline test sets, creating a feedback loop that keeps evaluation aligned with production behavior.", "source": "Evaluation modules, production optimization"}
