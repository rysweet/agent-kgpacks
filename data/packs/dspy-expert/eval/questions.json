[
  {
    "id": "de_001",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is DSPy and what is its primary purpose?",
    "ground_truth": "DSPy is a framework for optimizing language model prompts and weights. It abstracts language models as operations that can be composed, optimized, and validated systematically rather than hand-crafted.",
    "source": "framework_overview"
  },
  {
    "id": "de_002",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does ChainOfThought module do in DSPy?",
    "ground_truth": "ChainOfThought is a DSPy module that instructs language models to produce intermediate reasoning steps before generating the final answer, improving output quality for complex tasks.",
    "source": "chainofthought_module"
  },
  {
    "id": "de_003",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is a Signature in DSPy?",
    "ground_truth": "A Signature in DSPy is a declarative specification of input-output behavior for a task, defining what fields are expected as input and what the model should produce as output.",
    "source": "signature_concept"
  },
  {
    "id": "de_004",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "Name three built-in optimizers available in DSPy.",
    "ground_truth": "Three built-in optimizers in DSPy are BootstrapFewShot, MIPro, and Copro. These optimize prompt examples and configurations to improve model performance.",
    "source": "optimizers"
  },
  {
    "id": "de_005",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is the purpose of the Retrieve module in DSPy?",
    "ground_truth": "The Retrieve module enables retrieval-augmented generation by fetching relevant documents or passages from a corpus to augment language model inputs with contextual information.",
    "source": "retrieve_module"
  },
  {
    "id": "de_006",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does the Predict primitive do in DSPy?",
    "ground_truth": "The Predict primitive is a basic DSPy operation that wraps a language model call with a given signature, handling input formatting, model invocation, and output parsing.",
    "source": "predict_primitive"
  },
  {
    "id": "de_007",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is BootstrapFewShot in DSPy?",
    "ground_truth": "BootstrapFewShot is an optimizer that automatically generates few-shot examples by running the model on training data and selecting high-quality demonstrations to improve performance.",
    "source": "bootstrap_fewshot"
  },
  {
    "id": "de_008",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "How do you configure a language model in DSPy?",
    "ground_truth": "You configure a language model in DSPy using dspy.configure() or by setting the language model directly with dspy.settings.configure(lm=...), specifying the model provider and parameters.",
    "source": "configuration"
  },
  {
    "id": "de_009",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is the MultiChainComparison module used for?",
    "ground_truth": "MultiChainComparison is a DSPy module that generates multiple reasoning chains and compares them to select the best answer, combining ensemble and reasoning strategies.",
    "source": "multichain_comparison"
  },
  {
    "id": "de_010",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does dspy.evaluate() function do?",
    "ground_truth": "dspy.evaluate() assesses the performance of a DSPy program on a test set using specified metrics, returning accuracy or other performance indicators.",
    "source": "evaluation_function"
  },
  {
    "id": "de_011",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is the purpose of DSPy's Program class?",
    "ground_truth": "The Program class in DSPy is a base class for defining reusable, composable modules that encapsulate language model tasks with forward() methods that specify execution logic.",
    "source": "program_class"
  },
  {
    "id": "de_012",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does the MultiHop module enable in DSPy?",
    "ground_truth": "The MultiHop module enables multi-step reasoning by chaining multiple retrieval and reasoning operations, allowing models to answer complex questions requiring multiple reasoning steps.",
    "source": "multihop_module"
  },
  {
    "id": "de_013",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is dspy.settings in DSPy?",
    "ground_truth": "dspy.settings is a configuration object that stores global settings for DSPy including the active language model, retrieval model, and other parameters used across modules.",
    "source": "settings_object"
  },
  {
    "id": "de_014",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does the input/output specification in a Signature accomplish?",
    "ground_truth": "Input/output specifications in a Signature define the contract for a task, allowing DSPy to automatically generate prompts, validate data types, and optimize performance.",
    "source": "signature_specification"
  },
  {
    "id": "de_015",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is RAG in the context of DSPy?",
    "ground_truth": "RAG (Retrieval-Augmented Generation) in DSPy combines document retrieval with language generation, using the Retrieve module to fetch relevant context before model prediction.",
    "source": "rag_pattern"
  },
  {
    "id": "de_016",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "How do you define a custom DSPy module?",
    "ground_truth": "You define a custom DSPy module by creating a class that inherits from dspy.Module, implementing a forward() method that contains the module logic using DSPy primitives.",
    "source": "custom_modules"
  },
  {
    "id": "de_017",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is the function of the Assert in DSPy?",
    "ground_truth": "Assert in DSPy provides constraint validation on model outputs, ensuring generated text meets specified conditions and helping maintain output quality and consistency.",
    "source": "assert_constraint"
  },
  {
    "id": "de_018",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What does the Retry mechanism do in DSPy?",
    "ground_truth": "The Retry mechanism in DSPy automatically re-invokes a failed operation with modified inputs or prompts when assertions fail, improving robustness of generated outputs.",
    "source": "retry_mechanism"
  },
  {
    "id": "de_019",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What is the purpose of few-shot examples in DSPy?",
    "ground_truth": "Few-shot examples in DSPy are sample input-output pairs provided in prompts to guide the language model toward desired behavior and improve task performance.",
    "source": "fewshot_examples"
  },
  {
    "id": "de_020",
    "domain": "dspy_expert",
    "difficulty": "easy",
    "question": "What supported language model providers can be used with DSPy?",
    "ground_truth": "DSPy supports various language model providers including OpenAI, Cohere, Claude (Anthropic), Ollama, and open-source models through adapters and configurations.",
    "source": "lm_providers"
  },
  {
    "id": "de_021",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy's ChainOfThought module differ from basic prompting in terms of reasoning transparency?",
    "ground_truth": "ChainOfThought enables step-by-step reasoning visibility by instructing the LM to produce intermediate reasoning steps before the final answer, whereas basic prompting only returns a direct answer without exposing the reasoning process.",
    "source": "chain_of_thought"
  },
  {
    "id": "de_022",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What is the primary purpose of DSPy's teleprompter modules and how do they improve program performance?",
    "ground_truth": "Teleprompters automatically optimize DSPy programs by learning better prompts through few-shot examples and feedback, improving model predictions without requiring manual prompt engineering.",
    "source": "teleprompters"
  },
  {
    "id": "de_023",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "Explain the relationship between DSPy Signatures and the LM calls they produce.",
    "ground_truth": "Signatures define input/output schema and task instructions as high-level declarations; DSPy compiles these into actual LM prompts through various optimization strategies, enabling separation of task logic from implementation details.",
    "source": "signatures"
  },
  {
    "id": "de_024",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What distinguishes BootstrapFewShot from manual few-shot prompt engineering in DSPy?",
    "ground_truth": "BootstrapFewShot automatically generates few-shot examples by running the program on a training set and selecting high-quality examples based on correctness, eliminating manual example curation.",
    "source": "bootstrap_few_shot"
  },
  {
    "id": "de_025",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy's multi-hop reasoning pattern enable complex question-answering tasks?",
    "ground_truth": "Multi-hop reasoning chains multiple retrieval and reasoning steps together, where outputs from one step become inputs to the next, enabling the system to answer questions requiring information synthesis from multiple sources.",
    "source": "multi_hop_reasoning"
  },
  {
    "id": "de_026",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What are the trade-offs between using DSPy with GPT-4 versus smaller open-source models?",
    "ground_truth": "GPT-4 provides superior reasoning and fewer-shot requirements but has higher latency and costs; smaller models are cheaper and faster but require more optimization, longer prompts, or more examples to achieve comparable performance.",
    "source": "model_selection"
  },
  {
    "id": "de_027",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does the compile() method in DSPy differ across different optimizer strategies?",
    "ground_truth": "Different optimizers (e.g., BootstrapFewShot vs. MIPRO) compile programs differently: some generate few-shot examples, others optimize prompts, and some search hyperparameters; all aim to maximize task performance on validation data.",
    "source": "compilation"
  },
  {
    "id": "de_028",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What is the purpose of DSPy's Retrieve module and how does it integrate with LM predictions?",
    "ground_truth": "Retrieve fetches relevant context documents from a knowledge base based on a query; it integrates with LM modules by providing retrieved passages as context to improve answer generation accuracy in retrieval-augmented systems.",
    "source": "retrieval"
  },
  {
    "id": "de_029",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy's metric-based optimization improve upon traditional fine-tuning approaches?",
    "ground_truth": "DSPy optimizes programs using custom metrics and validation data without modifying model weights, allowing rapid iteration and task-specific optimization with less data overhead than traditional fine-tuning.",
    "source": "metrics_optimization"
  },
  {
    "id": "de_030",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What configuration decisions must be made when setting up a DSPy program with multiple LM calls?",
    "ground_truth": "Key decisions include: which LM provider/model for each module, whether to cache outputs, what signatures/instructions to use, retry logic for failures, and which teleprompter strategy to optimize the entire pipeline.",
    "source": "configuration"
  },
  {
    "id": "de_031",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy handle in-context learning differently than traditional prompt engineering frameworks?",
    "ground_truth": "DSPy treats in-context learning as a first-class optimization problem with automatic example selection and prompt optimization via teleprompters, rather than manual template writing; it abstracts away implementation details.",
    "source": "in_context_learning"
  },
  {
    "id": "de_032",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What are the key considerations when designing Signatures for complex multi-step reasoning tasks?",
    "ground_truth": "Signatures should clearly define input/output fields with descriptive names and optional instructions; for multi-step tasks, intermediate fields must be carefully designed to represent information flow between reasoning steps.",
    "source": "signature_design"
  },
  {
    "id": "de_033",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy's Program abstraction enable modularity and reusability in LM-based systems?",
    "ground_truth": "Programs encapsulate logic, signatures, and sub-modules into composable units that can be reused and combined; this allows complex systems to be built from simpler, independently testable components with clear interfaces.",
    "source": "program_abstraction"
  },
  {
    "id": "de_034",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What is the role of DSPy's assertions and backtracking in improving program reliability?",
    "ground_truth": "Assertions define constraints on LM outputs; when assertions fail, DSPy backtracks and retries the LM call with modified prompts or examples, increasing the probability of generating valid outputs without manual error handling.",
    "source": "assertions_backtracking"
  },
  {
    "id": "de_035",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does MIPRO differ from BootstrapFewShot in terms of optimization scope and complexity?",
    "ground_truth": "MIPRO jointly optimizes prompts, few-shot examples, and model hyperparameters across the entire program using advanced search techniques, whereas BootstrapFewShot focuses primarily on example selection, making MIPRO more comprehensive but computationally expensive.",
    "source": "mipro_optimizer"
  },
  {
    "id": "de_036",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What validation strategies should be employed when optimizing a DSPy program to avoid overfitting?",
    "ground_truth": "Use separate training/validation/test splits, monitor metrics across all splits during compilation, apply regularization in teleprompter selection, and validate on diverse examples to ensure generalization beyond the optimization dataset.",
    "source": "validation_strategies"
  },
  {
    "id": "de_037",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How does DSPy's caching mechanism impact development workflow and production performance?",
    "ground_truth": "Caching stores LM outputs to avoid redundant calls during development, accelerating iteration; in production, it reduces latency and costs for repeated queries but requires cache invalidation strategies when behavior should change.",
    "source": "caching"
  },
  {
    "id": "de_038",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What are the challenges of using DSPy with closed-source versus open-source language models?",
    "ground_truth": "Closed-source models (e.g., GPT-4) have unpredictable updates affecting reproducibility; open-source models require local deployment infrastructure but allow full control and fine-tuning; each affects optimization strategy and cost-performance trade-offs.",
    "source": "model_comparison"
  },
  {
    "id": "de_039",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "How should error handling and recovery mechanisms be structured in production DSPy applications?",
    "ground_truth": "Implement retry logic with exponential backoff, assertion-based validation to trigger controlled backtracking, fallback LM models for failures, and structured logging to track optimization effectiveness and runtime performance issues.",
    "source": "production_deployment"
  },
  {
    "id": "de_040",
    "domain": "dspy_expert",
    "difficulty": "medium",
    "question": "What factors should influence the choice between using DSPy's Predict versus ChainOfThought modules for a given task?",
    "ground_truth": "Use Predict for simple mappings where reasoning is unnecessary; use ChainOfThought for complex tasks requiring multi-step reasoning, interpretability, or when the task benefits from intermediate reasoning steps for accuracy.",
    "source": "predict_vs_cot"
  },
  {
    "id": "de_041",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "When implementing a custom ChainOfThought module in DSPy, how does the internal state management differ between using Predict vs. ChainOfThought for handling intermediate reasoning steps, and what are the performance implications?",
    "ground_truth": "ChainOfThought maintains explicit intermediate reasoning traces through its internal scratchpad mechanism, allowing for better error recovery and backtracking, whereas Predict operates as a black box. This results in higher token consumption for ChainOfThought but enables more sophisticated optimization strategies like bootstrap few-shot learning on reasoning chains.",
    "source": "chainofthought_internals"
  },
  {
    "id": "de_042",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "How does DSPy's teleprompter optimization handle conflicting optimization objectives when applying BootstrapFewShot and Signature constraints simultaneously, and what happens at the boundary conditions?",
    "ground_truth": "DSPy resolves conflicts through a prioritized constraint satisfaction model where Signature-level type constraints are enforced first, then BootstrapFewShot examples are validated against these constraints. At boundaries, the teleprompter falls back to greedy selection if no fully-compliant few-shot examples exist, potentially degrading optimization quality.",
    "source": "teleprompter_constraints"
  },
  {
    "id": "de_043",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "In DSPy, when using multi-hop retrieval with MIPS (Maximum Inner Product Search), what are the security implications of exposing embedding dimensions in production, and how does this interact with adversarial prompt injection?",
    "ground_truth": "Exposed embedding dimensions allow attackers to craft adversarial inputs that manipulate MIPS ranking by targeting high-dimension components. DSPy mitigates this through embedding normalization and dimension reduction in production deployments, but requires explicit configuration to prevent information leakage through retrieval rank ordering.",
    "source": "mips_security"
  },
  {
    "id": "de_044",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "What is the exact mechanism by which DSPy's assertion-based optimization differs from traditional constraint satisfaction, particularly regarding how failed assertions affect the backpropagation signal in the LM optimization loop?",
    "ground_truth": "DSPy's assertions generate hard constraints that propagate negative signals through the optimization graph, causing the optimizer to heavily penalize LM weights that produce assertion violations. Unlike soft constraints, failed assertions block gradient flow for that exemplar, effectively removing it from the training distribution until corrected.",
    "source": "assertion_optimization"
  },
  {
    "id": "de_045",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "When implementing DSPy with function-calling LMs (like GPT-4 Turbo), what edge cases arise in the parsing layer when tool_choice constraints conflict with Signature output specifications, and how does context window pressure affect this?",
    "ground_truth": "Tool_choice constraints can force function calls that violate Signature type requirements, creating parsing deadlocks where the LM cannot satisfy both constraints. Under context pressure, DSPy's parser falls back to lossy extraction, potentially corrupting structured outputs. Resolution requires explicit disambiguation through modified system prompts or constraint relaxation.",
    "source": "function_calling_edge_cases"
  },
  {
    "id": "de_046",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "How does DSPy's caching layer interact with non-deterministic LM sampling (temperature > 0), and what are the correctness implications for optimization pipelines that rely on cache hits across multiple epochs?",
    "ground_truth": "DSPy caches based on input hash, not sampling parameters, so identical inputs cached at different temperatures return the same output regardless of current temperature setting. This causes systematic bias in optimization: early epochs' high-temperature cache hits suppress exploration in later epochs. Correct usage requires cache invalidation when temperature changes or using temperature-aware cache keys.",
    "source": "caching_nondeterminism"
  },
  {
    "id": "de_047",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "In DSPy's retrieve-then-rank architecture, explain how the ranking loss function handles the case where the correct context rank is beyond the top-k retrieval window, and what happens to gradient computation in this scenario.",
    "ground_truth": "When the correct context is outside top-k, DSPy assigns maximum loss to the entire result set, but gradient computation is undefined for unranked items. Most implementations use truncated loss on the top-k window, effectively ignoring the missing ground truth. This introduces a systematic bias favoring higher k values and poor generalization to retrieval failures.",
    "source": "retrieve_rank_loss"
  },
  {
    "id": "de_048",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "What is the computational complexity of DSPy's signature-based type checking across deeply nested structures with recursive schema definitions, and how does this scale with prompt length?",
    "ground_truth": "Type checking complexity is O(n*d) where n is schema depth and d is definition cardinality, but with recursive schemas it becomes NP-hard due to circular reference resolution. DSPy mitigates this with memoization and depth limits (default 5 levels), causing valid nested structures beyond the limit to fail validation silently.",
    "source": "type_checking_complexity"
  },
  {
    "id": "de_049",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "When DSPy's BootstrapFewShot optimizer generates in-context examples, how does it handle the case where the training set contains near-duplicate examples with different labels, and what deduplication strategy is used?",
    "ground_truth": "DSPy uses semantic similarity (embedding-based) for deduplication with a configurable threshold (default 0.95 cosine similarity). Near-duplicates with conflicting labels are kept if below threshold, creating potential label noise in few-shot examples. The optimizer does not detect or resolve this contradiction, potentially degrading performance on similar test inputs.",
    "source": "bootstrap_deduplication"
  },
  {
    "id": "de_050",
    "domain": "dspy_expert",
    "difficulty": "hard",
    "question": "Explain the interplay between DSPy's traced execution model and LM API rate limiting: how does the tracer handle rate limit exceptions, and what guarantees exist for idempotency across retry attempts with different LM sampling?",
    "ground_truth": "DSPy's tracer catches rate limit exceptions and retries with exponential backoff, but does not guarantee idempotent sampling (temperature may vary across retries). For deterministic traces, users must explicitly set temperature=0, but this disables trace diversity needed for robust optimization. No built-in retry idempotency mechanism exists; users must implement external determinism controls.",
    "source": "rate_limiting_idempotency"
  }
]
