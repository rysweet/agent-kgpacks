{"id": "va_001", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the primary purpose of the generateText function in the Vercel AI SDK?", "ground_truth": "generateText is used to generate text completions from AI models and returns the result as a promise that resolves to a single text output, suitable for non-streaming scenarios.", "source": "generateText"}
{"id": "va_002", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "How does streamText differ from generateText in terms of response delivery?", "ground_truth": "streamText returns a stream that progressively sends text chunks to the client in real-time, while generateText waits for the complete response before returning it.", "source": "streamText"}
{"id": "va_003", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What function should you use when you need to generate structured JSON data from an AI model?", "ground_truth": "The generateObject function is used to generate structured data outputs that conform to a specified Zod schema, ensuring type-safe JSON responses.", "source": "generateObject"}
{"id": "va_004", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "Name one provider that is integrated with the Vercel AI SDK for model access.", "ground_truth": "OpenAI, Anthropic, Google, AWS Bedrock, and Azure are all supported provider integrations in the Vercel AI SDK.", "source": "provider_integrations"}
{"id": "va_005", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the AI SDK UI primarily designed to help developers build?", "ground_truth": "AI SDK UI provides pre-built components and hooks for quickly building chatbots and AI-powered completion interfaces with streaming support.", "source": "ai_sdk_ui"}
{"id": "va_006", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What does tool calling allow an AI model to do in the Vercel AI SDK?", "ground_truth": "Tool calling enables AI models to request the execution of specific functions or tools defined by the developer to accomplish tasks or retrieve information.", "source": "tool_calling"}
{"id": "va_007", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What parameter primarily controls the maximum number of steps in multi-step tool calling in the Vercel AI SDK, and what does `stopWhen` add?", "ground_truth": "The `maxSteps` parameter (a number) is the primary way to limit multi-step tool calling — it sets the maximum number of model-tool interaction rounds. `stopWhen` is a complementary option that accepts a custom predicate function to halt the loop based on the current state (e.g., tool results or step count), enabling dynamic termination before `maxSteps` is reached.", "source": "tool_calling_multistep"}
{"id": "va_008", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is RAG in the context of the Vercel AI SDK?", "ground_truth": "RAG (Retrieval-Augmented Generation) is a pattern where the AI SDK retrieves relevant documents or data from external sources and uses them to augment the AI model's context for generating better answers.", "source": "rag_patterns"}
{"id": "va_009", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the role of middleware in the Vercel AI SDK?", "ground_truth": "Middleware allows developers to intercept and process AI requests and responses, enabling features like logging, caching, authentication, and request modification.", "source": "middleware"}
{"id": "va_010", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What does Generative UI with AI RSC enable developers to do?", "ground_truth": "Generative UI with AI RSC (React Server Components) allows developers to generate entire UI components dynamically from AI models, combining server-side rendering with AI generation.", "source": "generative_ui"}
{"id": "va_011", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the primary purpose of lifecycle callbacks in the Vercel AI SDK?", "ground_truth": "Lifecycle callbacks provide observability hooks that fire at key points in the AI request lifecycle (like onStart, onFinish), enabling monitoring, logging, and tracing.", "source": "lifecycle_callbacks"}
{"id": "va_012", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "Which function should you use to get a streaming response of structured JSON data?", "ground_truth": "The streamObject function returns a stream of structured data chunks that conform to a Zod schema, allowing progressive delivery of generated objects.", "source": "streamObject"}
{"id": "va_013", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the Vercel AI SDK primarily written in?", "ground_truth": "The Vercel AI SDK is a TypeScript toolkit, providing full type safety and strong typing support for building AI applications.", "source": "overview"}
{"id": "va_014", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What do testing utilities in the Vercel AI SDK help developers accomplish?", "ground_truth": "Testing utilities provide mock models, helpers, and utilities for writing unit and integration tests for AI-powered applications without relying on external APIs.", "source": "testing_utilities"}
{"id": "va_015", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "In the context of agents, what does the Vercel AI SDK enable?", "ground_truth": "The Vercel AI SDK enables the creation of autonomous agents that can use tools, make decisions, and take actions iteratively to accomplish multi-step tasks.", "source": "agents"}
{"id": "va_016", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is the difference between single-step and multi-step tool calling?", "ground_truth": "Single-step tool calling executes one tool call per request, while multi-step tool calling allows the model to iteratively call multiple tools based on previous results until a goal is reached.", "source": "tool_calling"}
{"id": "va_017", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What streaming protocols does the Vercel AI SDK use for delivering AI responses?", "ground_truth": "The Vercel AI SDK uses two HTTP-based streaming protocols: Server-Sent Events (SSE) for broad compatibility, and the AI Data Stream Protocol (a custom chunked transfer encoding format for structured streaming of text, tool calls, and metadata). WebSocket-based streaming is not natively supported by the SDK.", "source": "streaming_protocols"}
{"id": "va_018", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What is prompt engineering in the context of the Vercel AI SDK?", "ground_truth": "Prompt engineering is the practice of crafting and optimizing input prompts to guide AI models toward desired outputs, improving response quality and relevance.", "source": "prompt_engineering"}
{"id": "va_019", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "Can the Vercel AI SDK work with multiple AI providers simultaneously?", "ground_truth": "Yes, the Vercel AI SDK's abstraction layer allows developers to work with multiple providers (OpenAI, Anthropic, Google, AWS Bedrock, Azure) and switch between them.", "source": "provider_integrations"}
{"id": "va_020", "domain": "vercel_ai_sdk", "difficulty": "easy", "question": "What type of applications is the Vercel AI SDK best suited for building?", "ground_truth": "The Vercel AI SDK is designed for building AI-powered applications including chatbots, completions interfaces, content generators, autonomous agents, and other LLM-based solutions.", "source": "overview"}
{"id": "va_021", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What is the primary difference between generateText and streamText in the Vercel AI SDK, and when would you choose one over the other?", "ground_truth": "generateText returns the complete response as a Promise, blocking until the entire text is generated, while streamText returns a StreamTextResult object that allows reading text progressively. Use generateText for simple use cases where you don't need incremental updates, and streamText for real-time applications like chatbots where users expect immediate feedback.", "source": "text_generation_functions"}
{"id": "va_022", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Explain the difference between generateObject and streamObject. What are the trade-offs of using each approach?", "ground_truth": "generateObject waits for the complete structured object before returning, ensuring full validity but with higher latency, while streamObject progressively streams partial objects. streamObject provides better UX with incremental updates but requires handling partial/incomplete objects, making it better for large schemas where latency matters.", "source": "structured_data_generation"}
{"id": "va_023", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "How does tool calling work in the Vercel AI SDK, and what is the difference between single-step and multi-step tool calling?", "ground_truth": "Tool calling allows AI models to request execution of specific functions defined by the developer. Single-step tool calling executes one tool call per request and returns the result in one interaction. Multi-step tool calling uses `maxSteps` (a number) to allow the model to iteratively call multiple tools across several rounds — the model calls a tool, receives the result, then can call another tool, continuing until `maxSteps` is reached or the model produces a final text response. The optional `stopWhen` predicate can terminate the loop earlier based on custom logic.", "source": "tool_calling_multistep"}
{"id": "va_024", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What is the stopWhen parameter in multi-step tool calling, and how does it control agent execution flow?", "ground_truth": "stopWhen is a callback function that determines when to halt the tool-calling loop by returning true/false based on the current state. It evaluates tool results and decides whether the agent has achieved its goal or should continue iterating, enabling custom termination logic for complex agent workflows.", "source": "tool_calling_multi_step"}
{"id": "va_025", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Describe the relationship between agents and tool calling in the Vercel AI SDK. How do agents use tools to accomplish tasks?", "ground_truth": "Agents are autonomous systems that use tool calling to reason about problems and execute actions iteratively. They receive a goal, call appropriate tools based on the current state, evaluate results, and decide next steps\u2014effectively combining language models with tool access and decision-making logic.", "source": "agents"}
{"id": "va_026", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What are the main components provided by AI SDK UI, and how do they simplify building conversational applications?", "ground_truth": "AI SDK UI provides components like useChat and useCompletion hooks that handle state management, streaming, message history, and form handling automatically. They abstract away boilerplate for real-time chat and completion interfaces, reducing development time and ensuring consistent behavior across applications.", "source": "ai_sdk_ui"}
{"id": "va_027", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "How does the Vercel AI SDK support multiple provider integrations, and what are the implications of switching between OpenAI, Anthropic, and Google models?", "ground_truth": "The SDK uses a unified interface where you pass a language model instance from different providers (e.g., openai(), anthropic(), google()). Switching providers requires changing the model parameter, but behavior may differ due to varying model capabilities, pricing, rate limits, and feature support like vision or structured outputs.", "source": "provider_integrations"}
{"id": "va_028", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What role do middleware functions play in the Vercel AI SDK, and what are common use cases for implementing custom middleware?", "ground_truth": "Middleware intercepts requests and responses in the AI generation pipeline, enabling cross-cutting concerns. Common use cases include request validation, response filtering, token counting, caching, logging, rate limiting, and cost tracking\u2014allowing you to augment SDK behavior without modifying core logic.", "source": "middleware"}
{"id": "va_029", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Explain how Retrieval-Augmented Generation (RAG) patterns are typically implemented using the Vercel AI SDK.", "ground_truth": "RAG patterns combine document retrieval with generation: retrieve relevant documents from a knowledge base based on user queries, include them as context in the system prompt or messages, then pass this augmented context to generateText/generateObject. This grounds responses in specific data while leveraging the model's reasoning abilities.", "source": "rag_patterns"}
{"id": "va_030", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What testing utilities does the Vercel AI SDK provide, and how would you use them to ensure your AI applications behave correctly?", "ground_truth": "The SDK provides mock providers and testing utilities that simulate model responses deterministically without real API calls. You use these to test your application logic, prompt handling, tool integrations, and error scenarios in a controlled, reproducible manner during development and CI/CD pipelines.", "source": "testing_utilities"}
{"id": "va_031", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What is Generative UI in the context of the Vercel AI SDK, and how does it differ from traditional chatbot interfaces?", "ground_truth": "Generative UI allows AI models to generate React components and UI elements dynamically, rather than just text responses. Using AI RSC (React Server Components), the model can output executable JSX that renders rich, interactive interfaces on-demand, enabling more sophisticated and contextual user experiences.", "source": "generative_ui_rsc"}
{"id": "va_032", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "How do lifecycle callbacks in the Vercel AI SDK support observability, and what metrics or data would you typically capture?", "ground_truth": "Lifecycle callbacks like onFinish, onToken, and middleware hooks fire at specific points in the generation pipeline. You capture metrics like tokens generated, latency, cost, tool calls, errors, and model decisions. This enables monitoring, debugging, analytics, and compliance tracking for production AI applications.", "source": "lifecycle_callbacks"}
{"id": "va_033", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Describe how prompt engineering principles apply when building applications with the Vercel AI SDK. What techniques would you use to improve output quality?", "ground_truth": "Prompt engineering in the SDK involves crafting system prompts, context, and examples passed to generateText/generateObject. Techniques include clear instructions, few-shot examples, role-playing, output formatting specifications, and iterative refinement. For structured data, use Zod schemas to enforce format constraints at the SDK level.", "source": "prompt_engineering"}
{"id": "va_034", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What streaming protocols does the Vercel AI SDK support, and how do they affect real-time data delivery to clients?", "ground_truth": "The SDK supports two HTTP-based streaming protocols: Server-Sent Events (SSE), which is widely compatible and works over standard HTTP connections, and the AI Data Stream Protocol, a custom chunked transfer format that carries structured payloads (text deltas, tool call events, metadata). WebSocket streaming is not natively supported; both protocols are unidirectional, meaning only the server streams to the client. SSE is simpler to implement; the AI Data Stream Protocol is preferred when rich structured streaming events are needed.", "source": "streaming_protocols"}
{"id": "va_035", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "How would you implement error handling and retry logic when using the Vercel AI SDK with external APIs?", "ground_truth": "Implement error handling by catching exceptions from generateText/generateObject, checking response.error, or using middleware to intercept failures. For retries, use exponential backoff with jitter, respect rate limits, and consider circuit breakers. Middleware can automatically handle retries before reaching application code.", "source": "error_handling"}
{"id": "va_036", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Explain how you would use the Vercel AI SDK to build an agent that can both retrieve information and perform actions on behalf of a user.", "ground_truth": "Create an agent by defining tools for retrieval (e.g., database queries, API calls) and action (e.g., creating records, sending messages). Use multi-step tool calling with stopWhen logic to orchestrate: the model decides which tool to use based on the user's intent, processes results, and repeats until the goal is achieved.", "source": "agents_with_tools"}
{"id": "va_037", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What considerations should you keep in mind when choosing between AWS Bedrock and native provider integrations in the Vercel AI SDK?", "ground_truth": "AWS Bedrock provides managed model access with enterprise features, IAM integration, and compliance benefits, but may have higher latency and less frequent model updates. Native providers (OpenAI, Anthropic) offer faster updates and direct model control. Choose Bedrock for enterprise deployments with strict compliance requirements; choose native for cutting-edge features.", "source": "provider_integrations_aws"}
{"id": "va_038", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "How would you implement cost tracking and budget limits for AI API calls in an application using the Vercel AI SDK?", "ground_truth": "Use middleware and lifecycle callbacks to track token counts and costs from provider responses. Store metrics in a database indexed by user/session. Implement guards in middleware that reject requests exceeding budget thresholds before making expensive API calls, and alert on approaching limits.", "source": "cost_management"}
{"id": "va_039", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "Describe how you would combine streamText with useChat in a Next.js application to build a responsive chatbot interface.", "ground_truth": "streamText on the server endpoint progressively streams text generation results. useChat on the client automatically consumes the stream, updates message state incrementally, and re-renders the UI as tokens arrive. This creates a responsive interface where users see text appearing in real-time without waiting for complete responses.", "source": "ui_streaming_integration"}
{"id": "va_040", "domain": "vercel_ai_sdk", "difficulty": "medium", "question": "What are the key differences between using generateObject with a Zod schema versus using generateText with JSON prompt instructions, and when would you choose each approach?", "ground_truth": "generateObject with Zod schemas enforces structure at the SDK level, ensuring type safety and guaranteed valid JSON, with automatic parsing. Generating JSON via text prompts is more flexible but error-prone and requires manual parsing. Use generateObject for reliability and type safety when structure is fixed; use generateText when flexibility or streaming partial objects matters.", "source": "structured_data_comparison"}
{"id": "va_041", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "When implementing multi-step tool calling with stopWhen, how does the SDK handle premature termination if a tool's execution throws an error before the stopWhen condition is evaluated, and what are the implications for agent state management?", "ground_truth": "The SDK propagates tool execution errors through the agentic loop; if stopWhen depends on tool results, the error prevents condition evaluation and typically halts the agent. The agent state remains at the last successful step, requiring explicit error recovery logic in your stopWhen predicate or outer error handling to prevent incomplete state.", "source": "tool_calling_multi_step_stopWhen"}
{"id": "va_042", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "Explain the performance and memory implications of using streamText versus generateText when handling large context windows (100k+ tokens) in a RAG system with multiple provider integrations, particularly regarding buffering and backpressure.", "ground_truth": "streamText streams tokens progressively, reducing peak memory and enabling early cancellation in RAG pipelines, whereas generateText buffers the entire response before returning, causing higher memory overhead with large contexts. streamText respects backpressure through Node.js stream mechanics, while generateText waits for complete generation; for RAG with multiple document chunks, streaming is preferred to avoid timeout and memory spikes.", "source": "generateText_streamText_performance"}
{"id": "va_043", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "When using generateObject or streamObject with strict schema validation and an LLM that repeatedly returns invalid JSON, what built-in retry mechanisms exist within the SDK, and how do they interact with provider-level retry policies?", "ground_truth": "The SDK does not automatically retry failed object generation internally; invalid JSON causes immediate failure. Retry logic must be implemented in user code or via provider middleware. Provider-specific retry policies (e.g., OpenAI's rate-limit handling) operate independently and don't guarantee schema compliance; you should wrap object generation in a retry loop with exponential backoff that re-prompts with schema feedback.", "source": "generateObject_streamObject_validation"}
{"id": "va_044", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "Describe how Generative UI with AI RSC handles streaming component updates when a single AI call generates multiple React Server Components with interdependent state. What are the ordering guarantees and potential race conditions?", "ground_truth": "AI RSC streams UI updates sequentially based on generation order; interdependent components may render before dependencies resolve, causing hydration mismatches. The SDK provides no automatic ordering guarantees\u2014you must manage dependency resolution in the prompt or use UI boundaries (Suspense) to handle incomplete state. Race conditions occur if client-side logic assumes server-side component ordering.", "source": "generative_ui_ai_rsc"}
{"id": "va_045", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "In a middleware chain with multiple observability callbacks (onStart, onFinish, onError), how should you order middleware to ensure accurate token counting for RAG systems when a generateText call is wrapped by both a caching middleware and a logging middleware?", "ground_truth": "Middleware execution order matters: place token-counting middleware after caching (so cache hits report zero new tokens) and before logging (so logs reflect final counts). If caching is applied first, it may short-circuit downstream middleware. Use lifecycle callbacks (onFinish) to access final token usage from the response, as middleware layers don't inherit intermediate counts unless explicitly threaded through context.", "source": "middleware_lifecycle_callbacks"}
{"id": "va_046", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "When switching between multiple LLM providers (OpenAI, Anthropic, Google) using provider integrations, what differences in streaming protocol implementation could cause message loss or duplication in a chatbot built with AI SDK UI, and how do you guard against this?", "ground_truth": "Providers implement streaming differently: OpenAI uses delta tokens, Anthropic uses content blocks, Google Vertex uses stream events. Message loss occurs if your UI assumes provider-agnostic deltas without normalization. Guard against this by using AI SDK's unified streaming interface, which abstracts provider differences, and validate stream completeness via onFinish callbacks that confirm all chunks were received.", "source": "provider_integrations_streaming"}
{"id": "va_047", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "Explain the security implications of using tool calling without proper input validation in a multi-tenant environment. What does the SDK provide natively, and what must be implemented application-side?", "ground_truth": "The SDK does not enforce input validation or sandbox tool execution; it passes tool parameters directly to your handler. You must implement application-side validation (e.g., Zod schemas in tool definitions, allowlisting), request signing, and isolation per tenant. Malicious prompts can inject arbitrary tool calls; use strict type schemas and guard against prompt injection attacks through prompt escaping and input bounds.", "source": "tool_calling_security"}
{"id": "va_048", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "In AWS Bedrock provider integration, how do cross-region model availability and latency affect a multi-region RAG deployment using the SDK, and what patterns mitigate this?", "ground_truth": "Bedrock models vary by region; a model available in us-east-1 may not exist in eu-west-1, causing deployment failures. Latency increases with geographic distance. Mitigation patterns: (1) route requests to the nearest region with the model, (2) use provider fallbacks (Bedrock \u2192 OpenAI), (3) cache embeddings and generations region-locally, (4) abstract region selection in middleware based on model and tenant location.", "source": "provider_integrations_aws_bedrock"}
{"id": "va_049", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "When building an agent that uses tool calling and RAG together, what are the performance bottlenecks if you fetch documents for every tool invocation, and how would you architect a caching strategy using the SDK's middleware and lifecycle callbacks?", "ground_truth": "Fetching documents per tool call causes N+1 query problems and latency spikes; agents with 5+ steps suffer linear slowdown. Architecture: (1) use middleware to cache document queries keyed by embedding, (2) implement onStart callbacks to prefetch documents based on the initial query, (3) use stopWhen to avoid unnecessary tool calls, (4) deduplicate tool requests via request fingerprinting in middleware before invoking the retriever.", "source": "agents_rag_optimization"}
{"id": "va_050", "domain": "vercel_ai_sdk", "difficulty": "hard", "question": "Describe a scenario where prompt engineering techniques (few-shot, CoT) interact adversely with streaming in generateText, causing incomplete or malformed outputs, and how would you diagnose and fix this issue?", "ground_truth": "Few-shot examples with complex formatting (tables, code blocks) can confuse tokenization in streaming mode, causing the LLM to truncate output mid-pattern or generate incomplete tokens. Diagnosis: check token counts via onFinish callbacks; if finish_reason is 'length', increase max_tokens. Fix: simplify few-shot examples, use explicit delimiters (e.g., <example>...</example>), test with generateText first (non-streaming) to isolate streaming issues.", "source": "prompt_engineering_streaming_interaction"}
