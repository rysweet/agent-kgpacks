[
  {
    "id": "pe_001",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is the primary purpose of a system prompt?",
    "ground_truth": "A system prompt defines the AI model's behavior, personality, constraints, and role for the entire conversation. It sets expectations for how the model should respond to all subsequent user inputs.",
    "source": "system_prompts"
  },
  {
    "id": "pe_002",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What does Chain-of-Thought (CoT) prompting encourage the model to do?",
    "ground_truth": "Chain-of-Thought prompting encourages the model to show its reasoning process step-by-step before arriving at a final answer, improving accuracy on complex reasoning tasks.",
    "source": "chain_of_thought"
  },
  {
    "id": "pe_003",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is few-shot prompting?",
    "ground_truth": "Few-shot prompting provides the model with a small number of example input-output pairs to demonstrate the desired task format or behavior before asking it to solve a similar problem.",
    "source": "few_shot_prompting"
  },
  {
    "id": "pe_004",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "Name one advantage of using XML tags for structuring prompts.",
    "ground_truth": "XML tags improve prompt clarity and parsing by explicitly delimiting sections, making it easier for both humans and models to identify instructions, examples, and context boundaries.",
    "source": "xml_tag_structuring"
  },
  {
    "id": "pe_005",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is JSON mode in language models?",
    "ground_truth": "JSON mode is a structured output format that constrains the model to respond with valid JSON, ensuring predictable machine-readable output that adheres to specified schemas.",
    "source": "structured_outputs"
  },
  {
    "id": "pe_006",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is prompt chaining?",
    "ground_truth": "Prompt chaining is a technique that breaks complex tasks into a sequence of smaller, connected prompts where the output of one prompt serves as input to the next.",
    "source": "prompt_chaining"
  },
  {
    "id": "pe_007",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "Define role assignment in prompt engineering.",
    "ground_truth": "Role assignment is the practice of instructing the model to adopt a specific persona or expertise (e.g., 'You are a senior software architect') to improve response quality and relevance.",
    "source": "role_assignment"
  },
  {
    "id": "pe_008",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is function calling in the context of language models?",
    "ground_truth": "Function calling allows language models to request specific tools or functions be executed by returning structured function calls, enabling integration with external systems and APIs.",
    "source": "tool_use_function_calling"
  },
  {
    "id": "pe_009",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What does Constitutional AI (CAI) emphasize?",
    "ground_truth": "Constitutional AI uses a set of principles or 'constitution' to guide model behavior, ensuring responses adhere to specified ethical guidelines and safety constraints.",
    "source": "constitutional_ai"
  },
  {
    "id": "pe_010",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is a prompt template?",
    "ground_truth": "A prompt template is a reusable prompt structure with placeholder variables that can be dynamically filled with different inputs for consistent task execution.",
    "source": "prompt_templates_variables"
  },
  {
    "id": "pe_011",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is A/B testing in prompt evaluation?",
    "ground_truth": "A/B testing compares the outputs of two different prompts or model configurations on the same set of inputs to determine which performs better on specified metrics.",
    "source": "evaluation_techniques"
  },
  {
    "id": "pe_012",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is context engineering for agents?",
    "ground_truth": "Context engineering for agents involves structuring and providing relevant background information, tools, and constraints that enable an AI agent to reason and act effectively.",
    "source": "context_engineering_agents"
  },
  {
    "id": "pe_013",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What does extended thinking in reasoning models allow?",
    "ground_truth": "Extended thinking allows reasoning models to spend more computational effort on complex problems through internal reasoning steps before producing a final response.",
    "source": "extended_thinking_reasoning"
  },
  {
    "id": "pe_014",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "Name one safety best practice when writing prompts.",
    "ground_truth": "One safety best practice is explicitly instructing the model to refuse harmful requests, decline sensitive queries, or avoid generating content that violates usage policies.",
    "source": "safety_best_practices"
  },
  {
    "id": "pe_015",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is schema adherence in structured outputs?",
    "ground_truth": "Schema adherence ensures that model outputs conform to a predefined data structure or JSON schema, guaranteeing specific fields, data types, and relationships are present.",
    "source": "structured_outputs"
  },
  {
    "id": "pe_016",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is the difference between few-shot and multishot prompting?",
    "ground_truth": "Few-shot prompting uses a small number of examples (typically 2-5), while multishot prompting uses many examples (10+) to teach the model a task or pattern.",
    "source": "few_shot_multishot_prompting"
  },
  {
    "id": "pe_017",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is a benchmark in prompt evaluation?",
    "ground_truth": "A benchmark is a standardized test dataset with known correct answers used to measure and compare prompt or model performance across consistent metrics.",
    "source": "evaluation_techniques"
  },
  {
    "id": "pe_018",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "Name one model-specific optimization for Claude.",
    "ground_truth": "One optimization is using XML tags with Claude, which the model handles well for structuring complex prompts and improving instruction clarity.",
    "source": "model_specific_optimizations"
  },
  {
    "id": "pe_019",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What is the purpose of evals in prompt engineering?",
    "ground_truth": "Evals (evaluations) systematically test prompts against defined criteria and metrics to measure quality, accuracy, safety, and consistency of model outputs.",
    "source": "evaluation_techniques"
  },
  {
    "id": "pe_020",
    "domain": "prompt_engineering",
    "difficulty": "easy",
    "question": "What should be included in tool use documentation for function calling?",
    "ground_truth": "Tool use documentation should include the function name, parameter descriptions, expected input/output formats, and usage examples to enable accurate function calls.",
    "source": "tool_use_function_calling"
  },
  {
    "id": "pe_021",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "When using chain-of-thought (CoT) prompting with Claude, what is the key difference between explicit CoT (asking the model to 'think step by step') versus leveraging extended thinking mode, and when should you use each?",
    "ground_truth": "Explicit CoT uses standard tokens and shows reasoning in the output, suitable for most tasks and transparent reasoning chains. Extended thinking mode uses dedicated thinking tokens, handles more complex multi-step reasoning internally, and is better for problems requiring deep analysis or when you want hidden reasoning. Use CoT for clarity and efficiency; use extended thinking for genuinely complex problems.",
    "source": "chain_of_thought_and_extended_thinking"
  },
  {
    "id": "pe_022",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In prompt chaining for complex tasks, what is the primary risk of passing unfiltered output from one stage directly to the next stage, and how can you mitigate it?",
    "ground_truth": "The primary risk is error accumulation and context pollution, where mistakes or unwanted content from the first stage corrupt downstream reasoning. Mitigate by parsing/validating outputs using schema enforcement, extracting only relevant fields, using structured outputs (JSON mode), and including validation checks between chain stages.",
    "source": "prompt_chaining_validation"
  },
  {
    "id": "pe_023",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How does the function calling pattern differ between Claude's tool_use block and GPT's function calling in terms of response structure, and what implications does this have for prompt design?",
    "ground_truth": "Claude uses explicit tool_use content blocks within the message with clear input JSON, while GPT embeds function calls in a specialized format. Claude's approach requires parsing the content array for tool blocks; GPT requires checking the tool_calls field. Design prompts with model-specific instructions on tool invocation format and validate responses accordingly for each platform.",
    "source": "tool_use_function_calling_patterns"
  },
  {
    "id": "pe_024",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "When designing a system prompt for a role-assigned agent, why is it important to define both capabilities AND explicit constraints, and what happens if you only define one?",
    "ground_truth": "Defining only capabilities risks the model overstepping safe boundaries; defining only constraints without clear purpose creates an unguided agent. Both are necessary because capabilities guide what the role should do, while constraints prevent misuse. A well-designed system prompt balances enabling desired behaviors with clear guardrails for safety and scope.",
    "source": "system_prompts_role_assignment"
  },
  {
    "id": "pe_025",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In few-shot prompting, what is the relationship between example diversity and model generalization, and how many examples typically optimize this trade-off?",
    "ground_truth": "More diverse examples improve generalization to unseen cases but with diminishing returns and increased context use. Typically 2-5 well-chosen examples from different input distributions achieve good results; beyond 5-8 examples, the benefit plateaus while consuming more tokens. The key is example quality and coverage of edge cases, not raw quantity.",
    "source": "few_shot_multishot_prompting"
  },
  {
    "id": "pe_026",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How should you structure XML tags in prompts to maximize model parsing accuracy, and what common mistakes reduce effectiveness?",
    "ground_truth": "Use well-nested, consistently formatted tags (e.g., <instruction>, <context>, <example>) with clear semantic meaning. Avoid deeply nested structures, inconsistent naming, and mixing XML with plain text instructions in the same region. Models parse shallow, logically organized XML better; use tags to delineate sections rather than inline markup.",
    "source": "xml_tag_structuring"
  },
  {
    "id": "pe_027",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "When implementing constitutional AI principles in a prompt, what is the relationship between constraint density (number of principles) and model performance, and how do you balance multiple competing values?",
    "ground_truth": "High constraint density (too many principles) increases token overhead and can create conflicting directives that confuse the model. Typically 3-5 core principles work best; order them by priority and allow the model to reason about trade-offs when values conflict. Test empirically, as too many constraints degrade performance while too few leave unsafe gaps.",
    "source": "constitutional_ai_principles"
  },
  {
    "id": "pe_028",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In structured output mode (JSON mode or schema adherence), why might a model fail to produce valid output even with explicit formatting instructions, and what design patterns prevent this?",
    "ground_truth": "Failures occur when instructions conflict with JSON schema, schemas are ambiguous, or the task itself is incompatible with the structure. Prevent this by: testing schema against diverse inputs, using unambiguous field names, including schema in the system prompt not just user message, and validating outputs with a parser that gives feedback on errors.",
    "source": "structured_outputs"
  },
  {
    "id": "pe_029",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How do you design context engineering for multi-turn agents to prevent context window exhaustion while maintaining task continuity across turns?",
    "ground_truth": "Use sliding window summarization of older turns, keep only recent high-relevance exchanges, store long-term facts in structured memory/tools rather than context, and periodically summarize the conversation state into a compact recap. Trade off verbatim history for semantic continuity by storing key facts and decisions separately from conversation logs.",
    "source": "context_engineering_agents"
  },
  {
    "id": "pe_030",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "What are the key differences between A/B testing and benchmark evaluation for prompt improvements, and when should you use each method?",
    "ground_truth": "Benchmark evaluation uses fixed test sets with metrics (accuracy, latency) to measure objective improvement; A/B testing compares real user outcomes (satisfaction, task completion). Use benchmarks for rapid iteration during development and regression testing; use A/B testing for production validation and measuring real-world impact. Benchmarks are faster; A/B tests are more reliable for user value.",
    "source": "evaluation_techniques_evals_benchmarks"
  },
  {
    "id": "pe_031",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "When designing prompt templates with variables, what escaping and injection risks must you consider, and how do you safely substitute user-provided values?",
    "ground_truth": "Risks include prompt injection (user input containing adversarial instructions) and variable expansion errors. Mitigate by: quoting/escaping variable values, using delimiters to separate user input from instructions, validating input length and format, and placing user variables at the end of prompts after all critical instructions. Never trust raw user input in template substitution.",
    "source": "prompt_templates_variables_safety"
  },
  {
    "id": "pe_032",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How does the token efficiency of zero-shot prompting compare to few-shot prompting, and under what conditions should you sacrifice efficiency for accuracy?",
    "ground_truth": "Zero-shot uses fewer tokens but often produces less accurate results; few-shot adds examples but improves accuracy significantly. Use zero-shot for simple tasks and when latency/cost is critical; use few-shot when accuracy matters or the task is complex/specialized. The efficiency trade-off is usually worth it: 2-5 good examples typically improve accuracy by 10-40% with modest token overhead.",
    "source": "few_shot_multishot_efficiency"
  },
  {
    "id": "pe_033",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In Claude-specific optimization, how does the use of caching (prompt caching) affect your prompt engineering strategy, and what structural changes improve cache hit rates?",
    "ground_truth": "Prompt caching reuses repeated context blocks, reducing latency and cost for requests with large shared preambles. Optimize by: structuring system prompts and large examples as reusable blocks that stay constant across requests, minimizing changes to cached sections, and using longer stable prefixes. Design prompts with cacheable boilerplate separate from dynamic content.",
    "source": "model_specific_optimizations_claude"
  },
  {
    "id": "pe_034",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "What is the relationship between prompt length and reasoning quality in models, and how should you balance comprehensive instructions with conciseness?",
    "ground_truth": "Very long prompts can degrade performance due to attention dilution and increased noise; very short prompts lack necessary context. Optimal range is typically 500-2000 tokens for instructions depending on task complexity. Focus on information density: include necessary detail and examples, but remove redundancy and verbose explanations. Measure empirically with your specific task.",
    "source": "prompt_optimization_length_quality"
  },
  {
    "id": "pe_035",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How should you structure multi-step reasoning for tasks that require both analysis and decision-making, and why is the order of CoT steps important?",
    "ground_truth": "Structure as: gather context \u2192 analyze alternatives \u2192 reason about trade-offs \u2192 make decision. Order matters because each step builds on prior understanding; starting with decision before analysis leads to biased reasoning. Use explicit markers (Step 1, 2, etc.) and require the model to show work before conclusions to ensure sound reasoning rather than confabulation.",
    "source": "chain_of_thought_reasoning_structure"
  },
  {
    "id": "pe_036",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In GPT-specific optimization, how does the distinction between system, user, and assistant messages affect prompt engineering strategy compared to Claude's approach?",
    "ground_truth": "GPT places higher weight on system messages and respects message role boundaries more strictly; Claude is more flexible. In GPT, reserve system for core instructions and constraints; use user for requests; use assistant messages to exemplify desired style. Claude allows more blending. Design GPT prompts with role-aware message structure; Claude prompts can be more flexible in message composition.",
    "source": "model_specific_optimizations_gpt"
  },
  {
    "id": "pe_037",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "What trade-offs exist between detailed explicit instructions versus implicit instruction through examples in few-shot prompting, and how do you decide between them?",
    "ground_truth": "Explicit instructions are reliable but verbose; examples are efficient and often sufficient for clear patterns. Use explicit instructions for complex rules or safety constraints; use examples for format/style patterns and nuanced behavior. Combining both is often best: explicit rules + examples demonstrating those rules. For ambiguous tasks, explicit instructions are safer.",
    "source": "instruction_vs_example_based_prompting"
  },
  {
    "id": "pe_038",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "How do you design evaluation metrics that accurately measure whether a prompt is achieving its intended goal, and what are common pitfalls in prompt evaluation?",
    "ground_truth": "Good metrics align with actual user goals (e.g., task completion, not token count). Common pitfalls include: measuring proxy metrics (similarity to expected output) instead of actual correctness, using weak baselines, and not accounting for variability. Design evals with ground truth labels, multiple metrics (precision, recall, latency), and A/B comparisons against baselines.",
    "source": "evaluation_metrics_design"
  },
  {
    "id": "pe_039",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "When using tool calling in agentic workflows, how should you design the tool descriptions and parameters to minimize hallucinated or incorrect tool invocations?",
    "ground_truth": "Use clear, concise descriptions avoiding jargon; specify parameter types, required vs optional fields, and realistic examples of valid inputs. Include constraints (e.g., 'only accepts ISO dates') and edge cases. Test descriptions independently of your task. Well-designed tool schemas with specific parameter descriptions reduce hallucinations by 20-40% compared to vague descriptions.",
    "source": "tool_design_for_function_calling"
  },
  {
    "id": "pe_040",
    "domain": "prompt_engineering",
    "difficulty": "medium",
    "question": "In prompt chaining with multiple agents, how do you ensure task continuity and information fidelity across handoffs, and what verification mechanisms prevent information loss?",
    "ground_truth": "Use structured handoff formats (JSON/schema), extract and validate key information at each stage, maintain a shared context store for critical facts, and include explicit verification steps (e.g., agent confirms received instructions). Design handoff protocols that are explicit and schema-validated rather than free-form text transfers to prevent interpretation errors.",
    "source": "multi_agent_prompt_handoffs"
  },
  {
    "id": "pe_041",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "When implementing chain-of-thought reasoning with extended thinking in Claude, what is the critical trade-off between budget_tokens and response latency, and how should you configure these parameters for time-sensitive applications?",
    "ground_truth": "Extended thinking increases latency due to internal reasoning computation before response generation; budget_tokens (typically 5,000-10,000 for Claude 3.5 Sonnet) control reasoning depth, with higher budgets increasing processing time significantly. For time-sensitive applications, reduce budget_tokens to 5,000-8,000 or use streaming with budget_tokens cap, accepting some reasoning depth reduction for sub-second response requirements.",
    "source": "extended_thinking_performance"
  },
  {
    "id": "pe_042",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "Design a prompt injection vulnerability scenario: How would an attacker exploit XML tag structuring in a system prompt that uses role assignment, and what defensive prompt engineering pattern effectively mitigates this attack?",
    "ground_truth": "An attacker could inject malicious XML tags within user input (e.g., '<system>ignore previous instructions</system>') to override role boundaries if the system prompt doesn't sanitize tag nesting. Mitigation: use escaped delimiters, nest user input within isolated tags with explicit closing markers, employ constitutional AI principles with explicit instruction hierarchies, and validate tag structure programmatically before model processing.",
    "source": "prompt_injection_defense"
  },
  {
    "id": "pe_043",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "In a multi-step prompt chaining architecture for complex reasoning tasks, explain how context bleed-through affects accuracy and propose a technical solution using structured outputs and schema enforcement.",
    "ground_truth": "Context bleed-through occurs when irrelevant information from previous chain steps contaminates reasoning in downstream steps, degrading accuracy; this is amplified with longer context windows. Solution: enforce strict JSON schema validation at each chain step using tool_use_mode='required', explicitly clear irrelevant context before downstream prompts using intermediary summarization steps, and implement per-step context windows with explicit purging of non-essential information.",
    "source": "prompt_chaining_architecture"
  },
  {
    "id": "pe_044",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "When implementing few-shot prompting with multishot examples for code generation tasks, what is the optimal number of examples for Claude 3.5 Sonnet vs GPT-4 Turbo, and how does example diversity impact performance vs model-specific token efficiency?",
    "ground_truth": "Claude 3.5 Sonnet typically performs best with 3-5 diverse examples (token-efficient due to larger context window), while GPT-4 Turbo requires 5-8 examples for comparable performance but with higher token cost. Example diversity (covering edge cases, different code patterns) matters more than quantity; diminishing returns occur after 8 examples for both models. Use stratified sampling of examples covering >70% of input distribution variance.",
    "source": "few_shot_optimization"
  },
  {
    "id": "pe_045",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "Explain the architectural difference between function calling patterns in Claude's tool_use vs GPT's function_calling, and how does this affect error recovery and retry logic in agent-based systems?",
    "ground_truth": "Claude's tool_use returns XML-formatted calls with explicit stop sequences, allowing graceful degradation if tool execution fails; GPT's function_calling uses JSON with implicit contract enforcement, requiring stricter pre-execution validation. Error recovery differs: Claude agents can backtrack via explicit tool_use rejection in conversation history, while GPT agents need explicit function result re-prompting. Claude's pattern is more resilient to malformed tool calls within reasoning chains.",
    "source": "tool_use_architecture"
  },
  {
    "id": "pe_046",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "Design an evaluation framework for comparing two prompt variants using A/B testing, including statistical rigor requirements, when sample size becomes problematic, and how to handle non-deterministic model outputs.",
    "ground_truth": "Use stratified random assignment with minimum 100-200 samples per variant for statistical significance (\u03b1=0.05); employ Cohen's d or t-tests for continuous metrics, chi-square for categorical. Handle non-determinism by fixing temperature=0 for deterministic evaluation or computing confidence intervals across 3-5 runs per prompt. When sample size is limited (<50), use Bayesian methods with informative priors or bootstrapping; validate results with qualitative analysis of failure modes.",
    "source": "evaluation_ab_testing"
  },
  {
    "id": "pe_047",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "In constitutional AI implementation, how do you resolve conflicts when explicit guardrails contradict learned model behavior, and what is the proper ordering of constraint layers in prompt hierarchy?",
    "ground_truth": "Conflicts arise when guardrails constrain legitimate use cases; resolution requires explicit priority declaration in system prompt with constraint hierarchy: [legal/safety boundaries] > [organizational policies] > [style guidelines]. Use nested conditional logic: 'If request involves X (high-risk), apply constraint Y, else if lower-risk apply constraint Z.' Test conflict scenarios during evals; when unresolvable, escalate to human review rather than allowing model discretion on boundary cases.",
    "source": "constitutional_ai_conflicts"
  },
  {
    "id": "pe_048",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "For an agent system with dynamic context engineering, explain how to implement context window budget allocation across multiple parallel function calls, and what happens when total context exceeds model limits mid-execution.",
    "ground_truth": "Implement context budgeting using token counters before each function call; allocate context proportionally based on task criticality (e.g., 40% for primary goal, 30% for tool results, 30% reserved). If mid-execution overflow occurs, prioritize active tool results over historical context, then cascade to tool summarization. For Claude, use explicit context windowing with `max_tokens` in system prompt; for GPT, implement token reservation patterns with fallback to context compression (e.g., summarization functions).",
    "source": "context_engineering_agents"
  },
  {
    "id": "pe_049",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "Compare prompt template variable injection security between f-string interpolation, Jinja2 templating, and specialized prompt frameworks (e.g., LangChain, Promptfoo). What is the attack surface for each approach?",
    "ground_truth": "f-string interpolation: vulnerable to indirect code execution if variables contain Python expressions; Jinja2: safer with sandboxing but vulnerable to template injection ({{ variable.__class__ }}); specialized frameworks provide escaping by default but require careful variable sourcing. Attack surface: all approaches are vulnerable to data poisoning at the variable source. Mitigation: use specialized frameworks with explicit escaping, validate variable origin, employ schema validation on injected values, and use allowlisting for template variable names.",
    "source": "prompt_template_security"
  },
  {
    "id": "pe_050",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "When optimizing prompts for Claude's vision capabilities combined with structured output requirements, how do you balance image token cost against reasoning depth, and what trade-offs exist in JSON schema strictness for multimodal inputs?",
    "ground_truth": "Vision tokens (approximately 1 token per 85 pixels for low-res, 1 per 170 for high-res) add significant overhead; compress images to 512x512 max unless fine details are critical. Trade-off: strict JSON schema (tool_use with explicit schema) reduces reasoning flexibility but ensures valid outputs; loose schema allows model reasoning over image interpretation at cost of unpredictable JSON. Optimal approach: use loose schema for image analysis step, then structured output step for final JSON with model reasoning guidance, not raw image analysis.",
    "source": "vision_structured_outputs"
  },
  {
    "id": "pe_051",
    "domain": "prompt_engineering",
    "difficulty": "hard",
    "question": "In a production system, how do you implement continuous prompt optimization where evals reveal performance degradation in specific failure modes, and what guardrails prevent prompt drift that optimizes for evals rather than real-world performance?",
    "ground_truth": "Implement dual-track evaluation: benchmark evals (fixed) + production metrics (online); when drift detected, rollback to previous version and investigate root cause before re-optimization. Prevent overfitting to evals using holdout eval sets (30% unseen at optimization time) and qualitative review of changes. Use constitutional AI constraints in prompt optimizer itself to prevent edge-case exploitation; monitor prompt version control and enforce human sign-off for changes exceeding 10% performance delta on production metrics.",
    "source": "prompt_optimization_guardrails"
  }
]
